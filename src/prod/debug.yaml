---
# Source: harness-prod/charts/ci/charts/ci-manager/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ci-manager
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: ci-manager
---
# Source: harness-prod/charts/platform/charts/access-control/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: access-control
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: access-control
---
# Source: harness-prod/charts/platform/charts/cv-nextgen/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cv-nextgen
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: cv-nextgen
---
# Source: harness-prod/charts/platform/charts/delegate-proxy/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: delegate-proxy
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: delegate-proxy
---
# Source: harness-prod/charts/platform/charts/gateway/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: gateway
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: gateway
---
# Source: harness-prod/charts/platform/charts/harness-manager/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: harness-manager
  annotations: {}
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: harness-manager
---
# Source: harness-prod/charts/platform/charts/le-nextgen/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: le-nextgen
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: le-nextgen
---
# Source: harness-prod/charts/platform/charts/next-gen-ui/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: next-gen-ui
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: next-gen-ui
---
# Source: harness-prod/charts/platform/charts/ng-auth-ui/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ng-auth-ui
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: ng-auth-ui
---
# Source: harness-prod/charts/platform/charts/ng-manager/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ng-manager
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: ng-manager
---
# Source: harness-prod/charts/platform/charts/pipeline-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: pipeline-service
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: pipeline-service
---
# Source: harness-prod/charts/platform/charts/platform-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: platform-service
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: platform-service
---
# Source: harness-prod/charts/platform/charts/scm-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: scm-service
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: scm-service
---
# Source: harness-prod/charts/platform/charts/template-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: template-service
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: template-service
---
# Source: harness-prod/charts/platform/charts/ti-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ti-service
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: ti-service
---
# Source: harness-prod/charts/platform/charts/harness-manager/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: harness-default
  namespace: harness-1
  annotations: {}
---
# Source: harness-prod/charts/platform/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: minio
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
secrets:
  - name: minio
---
# Source: harness-prod/charts/platform/charts/mongodb/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mongodb-replicaset-chart
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
secrets:
  - name: mongodb-replicaset-chart
automountServiceAccountToken: true
---
# Source: harness-prod/charts/platform/charts/harness-secrets/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: harness-secrets
  namespace: harness-1
  labels:
    app.kubernetes.io/name: harness-secrets
    helm.sh/chart: harness-secrets-0.2.0
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
  annotations: {}
type: Opaque
data:
    mongodbUsername: YWRtaW4=
    mongodbPassword: "RTY2YUxKeWV4MHB1aGt4dA=="
    postgresdbAdminPassword: "QnV1MEE5UVd4c3pQbnNEaA=="
    stoAppHarnessToken:  "bFl3bnZKUlpUOUtvZXlUNw=="
    stoAppAuditJWTSecret:  "bDAxU1UyejZ0U1d6R1doVQ=="
    timescaledbAdminPassword: "M0VJTmlGU1VHMWxNN1Vlcw=="
    timescaledbPostgresPassword: "ZmxydFJONTlGUmEyWlF0eQ=="
    timescaledbStandbyPassword:  "Q3JHc05sUUJNRkxTa1ZpYg=="
---
# Source: harness-prod/charts/platform/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: minio
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  root-user: "YWRtaW4="
  root-password: "MzV5R0VGaHNERw=="
  key.json: ""
---
# Source: harness-prod/charts/platform/charts/mongodb/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-replicaset-chart
  namespace: harness-1
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
type: Opaque
data:
  mongodb-root-password: "QVNyNXFlRlBrTA=="
  mongodb-replica-set-key: "S25IYmlydHd1Ng=="
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/secrets.yaml
apiVersion: v1
kind: Secret
type: kubernetes.io/tls
metadata:
  name: timescaledb-single-chart-certificate
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
    cluster-name: timescaledb-single-chart
  annotations: {}
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lSQU1qSkV5YjVzcWl3N0paWUk0eGpLcFF3RFFZSktvWklodmNOQVFFTEJRQXcKSXpFaE1COEdBMVVFQXhNWWRHbHRaWE5qWVd4bFpHSXRjMmx1WjJ4bExXTm9ZWEowTUI0WERUSXhNRFF3T1RBMQpOVFUwT0ZvWERUSTJNRFF3T1RBMU5UVTBPRm93SXpFaE1COEdBMVVFQXhNWWRHbHRaWE5qWVd4bFpHSXRjMmx1CloyeGxMV05vWVhKME1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBdVZvM0tuMS8KTzV3aUxWdkF1MWFOeWVIcXRjQU80RXJGOUkxOUh3NkZFR1MrbzdNQmZrU2U5UGFUQnJQeGV3cTNvM1kxaVdYUApGMDJiQ3RSUEMzRmxGQUZPMTRmYmJra1YxNVpYdmdxUlpuaU1Zb0Q0SmRwWDJPSm81Vm1iaVFnOVJBSkJJbVR5CkdGRnowb0lMVTI0Z05jRUZzZEtPSUFhR0NCRG9iRFpDM1MwbWxPQ3JZOE9GU2VpMnVTVGhvWkE5RFpCbG1WQkcKMkMyeDFQYkNGWmtYT2U3MlRtQ1IzcW9nM0JvOVRhSngxMUtjWE1sNzRrbUFmWlZob2Y2dE14cll1dnJTWFdaVQpKY25XdjlaZmxReWZXZ0QvOUljcWpOWXFmdkpkaTc5Z05aS05YME1lSnZYOG5OdjJzR29QcFloOGNpWmxRR29hCnk4eWw2TVc2WXVDM2h3SURBUUFCbzBJd1FEQU9CZ05WSFE4QkFmOEVCQU1DQXFRd0hRWURWUjBsQkJZd0ZBWUkKS3dZQkJRVUhBd0VHQ0NzR0FRVUZCd01DTUE4R0ExVWRFd0VCL3dRRk1BTUJBZjh3RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFGZ285MWtMd2xDeExqYkhqdXdTNnFOSWJxOVRicWdDb2R4WW9Jb25OS0huRDdwYVZMM2JuMjdFCmUxQmZLTjVJcGd4ODEyeTVNbW5sNG9wS2NuMmpSWW0zNDFjeHd3L1gweFVqclNTMnF3VTZpNXdjRjBDS0M1Q3gKM3FHZU9aZ3FwekovcHJ1Y2NnNnVZVlBHSFYwUkJ1ZkQzYXZ6RlFsM3FjVUJESE9PUUVwUW5wR25uSU1qbHM0NQpzOURKdWkzYnBCYWRGaVFqeXdnNXA4UnlZSUMvcTVvMzRVb1JXNGI0RnM3Z1BNWnNtRmFTaU9xS1dmRGxVOU92Ck51MUFDNFNtRkgyVXYrZVc0UEhWUklMR0xoeDI3M0JUcVJnMzFIbE8rVmpMNzBrd2tWVDkrS04zclFVUlR5cGgKdC9hYWdDQ2tCWDVPVmxBTnhpRk9mTmM3ekJpb05QYz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  tls.key: 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
---
# Source: harness-prod/charts/sto/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app: postgres
type: Opaque
data:
  postgres-password: "cUZXRmNZUDlESA=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: harness-prod/charts/sto/charts/sto-manager/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: service-accounts-sto-manager
  namespace: harness-1
data:
  cloud-data-store: test
  redis-labs-ca-truststore: test
---
# Source: harness-prod/charts/sto/charts/sto-manager/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: sto-manager
stringData:
  ##DO MANUALLY
  # MONGO_URI: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc/harness?authSource=admin'
  # STOMANAGER_MONGO_URI: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc/harness-sto?authSource=admin'
  # PMS_MONGO_URI: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
  # NOTIFICATION_MONGO_URI: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/notifications?replicaSet=rs0&authSource=admin'
  # TIMESCALE_URI: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
  # TIMESCALE_PASSWORD:
  # TIMESCALEDB_USERNAME: postgres
  # STO_SERVICE_GLOBAL_TOKEN: token
#  NEXT_GEN_MANAGER_SECRET:
#  MANAGER_SECRET:
#  ACCESS_CONTROL_SECRET:
#  JWT_AUTH_SECRET:
#  JWT_IDENTITY_SERVICE_SECRET:
#  EVENTS_FRAMEWORK_REDIS_USERNAME:
#  EVENTS_FRAMEWORK_REDIS_PASSWORD:
#  EVENTS_FRAMEWORK_REDIS_SSL_CA_TRUST_STORE_PASSWORD:
type: Opaque
---
# Source: harness-prod/charts/ci/charts/ci-manager/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ci-manager
  namespace: harness-1
data:
  CACHE_BACKEND: "REDIS"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_USE_SENTINEL: "true"
  DEPLOY_MODE: KUBERNETES_ONPREM
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  GRPC_SERVER_PORT: "9979"
  SCM_SERVICE_URI: "scm:8091"
  ADDON_IMAGE: docker.io/harness/ci-addon:1.14.7
  LE_IMAGE: docker.io/harness/ci-lite-engine:1.14.7
  GIT_CLONE_IMAGE: docker.io/harness/drone-git:1.2.0-rootless
  DOCKER_PUSH_IMAGE: docker.io/plugins/kaniko:1.6.0
  ECR_PUSH_IMAGE:  docker.io/plugins/kaniko-ecr:1.6.0
  GCR_PUSH_IMAGE: docker.io/plugins/kaniko-gcr:1.6.0
  GCS_UPLOAD_IMAGE: docker.io/plugins/gcs:1.3.0
  S3_UPLOAD_IMAGE: docker.io/plugins/s3:1.1.0
  ARTIFACTORY_UPLOAD_IMAGE: docker.io/plugins/artifactory:1.1.0
  GCS_CACHE_IMAGE: docker.io/plugins/cache:1.4.0
  S3_CACHE_IMAGE: docker.io/plugins/cache:1.4.0
  PMS_TARGET:  pipeline-service:12011
  PMS_AUTHORITY:  pipeline-service:12011
  LOGGING_LEVEL: INFO
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  SHOULD_CONFIGURE_WITH_PMS: "true"
  ENABLE_DASHBOARD_TIMESCALE: "true"
  MEMORY: "4096"
  USE_REDIS_FOR_SDK_RESPONSE_EVENTS: "true"
  DEFAULT_INTERNAL_IMAGE_CONNECTOR: "account.harnessImage"
  NG_MANAGER_URL: 'https://helm-test.qa.harness.io/ng/api/'
  MANAGER_URL: 'https://helm-test.qa.harness.io/api/'
  LOG_SERVICE_ENDPOINT: 'https://helm-test.qa.harness.io/gateway/log-service/'
  TI_SERVICE_ENDPOINT: 'https://helm-test.qa.harness.io/ti-service/'
  API_URL: 'https://helm-test.qa.harness.io/ng/#/'
  STO_SERVICE_ENDPOINT: 'https://helm-test.qa.harness.io/sto/'
  SECURITY_IMAGE: docker.io/harness/sto-plugin:latest
  LOG_SERVICE_GLOBAL_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  TI_SERVICE_GLOBAL_TOKEN: 78d16b66-4b4c-11eb-8377-acde48001122
---
# Source: harness-prod/charts/platform/charts/access-control/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: access-control
  namespace: harness-1
data:
  IDENTITY_SERVICE_SECRET: HVSKUYqD4e5Rxu12hFDdCJKGM64sxgEynvdDhaOHaTHhwwn0K4Ttr0uoOxSsEVYNrUU=

  DEPLOY_MODE: KUBERNETES_ONPREM
  LOGGING_LEVEL: "INFO"
  EVENTS_CONFIG_USE_SENTINEL: 'true'
  EVENTS_CONFIG_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'

  # lockRedisConfig
  LOCK_CONFIG_USE_SENTINEL: 'true'
  LOCK_CONFIG_SENTINEL_MASTER_NAME: 'harness-redis'
  LOCK_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'

  # iteratorsConfig
  RESOURCE_GROUP_ITERATOR_ENABLED: 'true'
  RESOURCE_GROUP_ITERATOR_INTERVAL: '3600'
  USER_GROUP_ITERATOR_ENABLED: 'true'
  USER_GROUP_ITERATOR_INTERVAL: '3600'
  USER_ITERATOR_ENABLED: 'true'
  USER_ITERATOR_INTERVAL: '3600'
  SERVICEACCOUNT_ITERATOR_ENABLED: 'true'
  SERVICEACCOUNT_ITERATOR_INTERVAL: '3600'
  SUPPORTPREFERENCE_ITERATOR_ENABLED: 'true'
  SUPPORTPREFERENCE_ITERATOR_INTERVAL: '600'
  SCOPE_ITERATOR_ENABLED: 'true'
  SCOPE_ITERATOR_INTERVAL: '3600'

  # resourceGroupClient:
  RESOURCE_GROUP_CLIENT_BASE_URL: 'http://platform-service.harness-1.svc.cluster.local:9005/api/'

  # userClient:
  USER_CLIENT_BASE_URL: 'https://helm-test.qa.harness.io/ng/api/'

  # userGroupClient
  USER_GROUP_CLIENT_BASE_URL: 'https://helm-test.qa.harness.io/ng/api/'

  # serviceAccountClient
  SERVICEACCOUNT_CLIENT_BASE_URL : 'https://helm-test.qa.harness.io/ng/api/'

  #accountClient
  ACCOUNT_CLIENT_BASE_URL: 'https://helm-test.qa.harness.io/api/'
  FEATURE_FLAG_CLIENT_BASE_URL: 'https://helm-test.qa.harness.io/api/'

  #projectClient
  PROJECT_CLIENT_BASE_URL: 'https://helm-test.qa.harness.io/ng/api/'

  #organizationClient
  ORGANIZATION_CLIENT_BASE_URL: 'https://helm-test.qa.harness.io/ng/api/'

  # aggreatorModuleConfig
  OFFSET_FLUSH_INTERVAL_MS: '10000'
  MONGODB_USER: admin
  MONGODB_SSL_ENABLED: 'false'
  AGGREGATOR_ENABLED: 'true'

  # auth
  ENABLE_AUTH: 'true'

  # preference
  ACCESS_CONTROL_PREFERENCE_ENABLED: 'true'

  #for notification
  NOTIFICATION_SLACK_WEBHOOK_URL: ""
  NOTIFICATION_ENVIRONMENT: ONPREM


  # for client
  ENABLE_ACCESS_CONTROL: 'false'
  ACCESS_CONTROL_SERVICE_BASE_URL: 'http://access-control.harness-1.svc.cluster.local:9006/api/'
  ENABLE_AUDIT: 'true'
  AUDIT_CLIENT_BASE_URL: 'http://platform-service.harness-1.svc.cluster.local:9005/api/'
  DISTRIBUTED_LOCK_IMPLEMENTATION: REDIS
  GOOGLE_APPLICATION_CREDENTIALS: /opt/harness/monitoring/stackdriver.json
  MEMORY: "512m"
---
# Source: harness-prod/charts/platform/charts/change-data-capture/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: change-data-capture
  namespace: harness-1
data:
  DEPLOY_MODE: "KUBERNETES"
  MEMORY: "2048"
  MONGO_TAG_NAME: "none"
  MONGO_TAG_VALUE: "none"
---
# Source: harness-prod/charts/platform/charts/cv-nextgen/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cv-nextgen
  namespace: harness-1
data:
  DEPLOY_MODE: KUBERNETES_ONPREM
  ENV: KUBERNETES_ONPREM
  LOGGING_LEVEL: INFO
  MANAGER_URL: http://harness-manager.harness-1.svc.cluster.local:9090/api/
  NG_MANAGER_URL: http://ng-manager.harness-1.svc.cluster.local:7090/api/
  MEMORY: "4096"
  STACK_DRIVER_LOGGING_ENABLED: "false"
  VERIFICATION_PORT: "6060"
  VERIFICATION_SERVICE_SECRET: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8
  NOTIFICATION_BASE_URL: http://platform-service.harness-1.svc.cluster.local:9005/api/
  SHOULD_CONFIGURE_WITH_NOTIFICATION: "true"
  PORTAL_URL: http://harness-manager.harness-1.svc.cluster.local:9090/api/
  MANAGER_CLIENT_BASEURL: http://harness-manager.harness-1.svc.cluster.local:9090/api/
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  SHOULD_CONFIGURE_WITH_PMS: "true"
  PMS_TARGET: pipeline-service:12011
  PMS_AUTHORITY: pipeline-service:12011
  GRPC_SERVER_PORT: "9979"
  CACHE_CONFIG_REDIS_URL: 'redis://localhost:6379'
  CACHE_BACKEND: "REDIS"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_USE_SENTINEL: "true"
  MOCK_ACCESS_CONTROL_SERVICE: "false"
  ACCESS_CONTROL_BASE_URL:  http://access-control.harness-1.svc.cluster.local:9006/api/
  ACCESS_CONTROL_ENABLED: "true"
---
# Source: harness-prod/charts/platform/charts/delegate-proxy/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: delegate-proxy
  namespace: harness-1
data:
  proxy.conf: "server { root /www/data;proxy_http_version 1.1;\n}"
---
# Source: harness-prod/charts/platform/charts/gateway/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway
  namespace: harness-1
data:
  MANAGER_URL: "https://helm-test.qa.harness.io"
  MANAGER_PUBLIC_URL: "https://helm-test.qa.harness.io"
  MEMORY: "512"
  DEPLOY_MODE: KUBERNETES_ONPREM
  API_VERSION: 'release-gateway:182'
  LOG_SVC_GLOBAL_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  TI_SVC_GLOBAL_TOKEN: 78d16b66-4b4c-11eb-8377-acde48001122
  CACHE_TYPE: REDIS
  TOKEN_CACHE_TTL: '300'
  USE_SENTINEL: 'true'
  SENTINEL_MASTER_NAME: 'harness-redis'
  REDIS_PORT: '26379'
  REDIS_SENTINELS: 'redis-sentinel-harness-announce-0.harness-1,redis-sentinel-harness-announce-1.harness-1,redis-sentinel-harness-announce-2.harness-1'
---
# Source: harness-prod/charts/platform/charts/harness-manager/templates/config.yaml
apiVersion: v1
data:
  ALLOWED_ORIGINS: 'https://helm-test.qa.harness.io'
  API_URL: 'https://helm-test.qa.harness.io'
  DELEGATE_METADATA_URL: 'https://helm-test.qa.harness.io/storage/wingsdelegates/delegateprod.txt'
  UI_SERVER_URL: 'https://helm-test.qa.harness.io'
  WATCHER_METADATA_URL: 'https://helm-test.qa.harness.io/storage/wingswatchers/watcherprod.txt'
  LOG_STREAMING_SERVICE_BASEURL: 'https://helm-test.qa.harness.io/gateway/log-service/'
  ATMOSPHERE_BACKEND: REDIS
  BACKGROUND_SCHEDULER_CLUSTERED: "true"
  CACHE_BACKEND: REDIS
  CAPSULE_JAR: rest-capsule.jar
  DELEGATE_DOCKER_IMAGE: docker.io/harness/delegate:latest
  DELEGATE_SERVICE_TARGET: harness-manager:9879
  DELEGATE_SERVICE_AUTHORITY: harness-manager:9879
  DISTRIBUTED_LOCK_IMPLEMENTATION: REDIS
  DEPLOY_MODE: KUBERNETES_ONPREM
  DISABLE_NEW_RELIC: "true"
  ENABLE_G1GC: "true"
  EXTERNAL_GRAPHQL_RATE_LIMIT: '500'
  FEATURES: 'SECURITY,SECURITY_STAGE,STO_CI_PIPELINE_SECURITY,STO_API_V2,LDAP_SSO_PROVIDER,ASYNC_ARTIFACT_COLLECTION,JIRA_INTEGRATION,AUDIT_TRAIL_UI,GDS_TIME_SERIES_SAVE_PER_MINUTE,STACKDRIVER_SERVICEGUARD,BATCH_SECRET_DECRYPTION,TIME_SERIES_SERVICEGUARD_V2,TIME_SERIES_WORKFLOW_V2,CUSTOM_DASHBOARD,GRAPHQL,CV_FEEDBACKS,LOGS_V2_247,UPGRADE_JRE,CDNG_ENABLED,NEXT_GEN_ENABLED,LOG_STREAMING_INTEGRATION,CING_ENABLED,NG_HARNESS_APPROVAL,GIT_SYNC_NG,NG_SHOW_DELEGATE,NG_CG_TASK_ASSIGNMENT_ISOLATION,CI_OVERVIEW_PAGE,AZURE_CLOUD_PROVIDER_VALIDATION_ON_DELEGATE,TERRAFORM_AWS_CP_AUTHENTICATION,NG_TEMPLATES,NEW_DEPLOYMENT_FREEZE,HELM_CHART_AS_ARTIFACT,RESOLVE_DEPLOYMENT_TAGS_BEFORE_EXECUTION,WEBHOOK_TRIGGER_AUTHORIZATION,GITHUB_WEBHOOK_AUTHENTICATION,CUSTOM_MANIFEST,GIT_ACCOUNT_SUPPORT,AZURE_WEBAPP,PRUNE_KUBERNETES_RESOURCES,LDAP_GROUP_SYNC_JOB_ITERATOR,POLLING_INTERVAL_CONFIGURABLE,APPLICATION_DROPDOWN_MULTISELECT,USER_GROUP_AS_EXPRESSION,RESOURCE_CONSTRAINT_SCOPE_PIPELINE_ENABLED,ENABLE_DEFAULT_NG_EXPERIENCE_FOR_ONPREM'
  HAZELCAST_NAMESPACE: 'harness-1'
  HAZELCAST_SERVICE: harness-manager
  HZ_CLUSTER_NAME: harness-manager
  LOGGING_LEVEL: 'INFO'
  MEMORY: "2048"
  REDIS_SENTINEL: "true"
  REDIS_URL: 'redis://localhost:6379'
  REDIS_MASTER_NAME: 'harness-redis'
  REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  SERVER_PORT: "9090"
  SERVICE_ACC: /opt/harness/svc/service_acc.json
  VERSION: 1.0.76019
  LOG_STREAMING_SERVICE_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  NG_MANAGER_BASE_URL: 'https://helm-test.qa.harness.io/ng/api/'
  ACCESS_CONTROL_ENABLED: "true"
  ACCESS_CONTROL_BASE_URL: 'https://helm-test.qa.harness.io/authz/api/'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_AVAILABLE_IN_ONPREM: "true"
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  VERIFICATION_SERVICE_SECRET: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8

kind: ConfigMap
metadata:
  name: harness-manager-config
  namespace: harness-1
  annotations: {}
---
# Source: harness-prod/charts/platform/charts/le-nextgen/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: le-nextgen
  namespace: harness-1
data:
  https_port: "10800"
  learning_env: "on_prem"
  server_url: "http://cv-nextgen:6060"
  service_secret: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8
---
# Source: harness-prod/charts/platform/charts/log-service/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-service
  namespace: harness-1
data:
  LOG_SERVICE_S3_BUCKET: logs
  LOG_SERVICE_S3_REGION: us-east-1
  LOG_SERVICE_S3_ENDPOINT: https://helm-test.qa.harness.io
  LOG_SERVICE_S3_PATH_STYLE: "true"
  LOG_SERVICE_DISABLE_AUTH: "true"
  LOG_SERVICE_GLOBAL_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  LOG_SERVICE_SECRET: IC04LYMBf1lDP5oeY4hupxd4HJhLmN6azUku3xEbeE3SUx5G3ZYzhbiwVtK4i7AmqyU9OZkwB4v8E9qM
---
# Source: harness-prod/charts/platform/charts/mongodb/templates/common-scripts-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-replicaset-chart-common-scripts
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
data:
  startup-probe.sh: |
    #!/bin/bash
    mongosh  $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval 'db.hello().isWritablePrimary || db.hello().secondary' | grep -q 'true$'
  readiness-probe.sh: |
    #!/bin/bash
    # Run the proper check depending on the version
    [[ $(mongod -version | grep "db version") =~ ([0-9]+\.[0-9]+\.[0-9]+) ]] && VERSION=${BASH_REMATCH[1]}
    . /opt/bitnami/scripts/libversion.sh
    VERSION_MAJOR="$(get_sematic_version "$VERSION" 1)"
    VERSION_MINOR="$(get_sematic_version "$VERSION" 2)"
    VERSION_PATCH="$(get_sematic_version "$VERSION" 3)"
    if [[ ( "$VERSION_MAJOR" -ge 5 ) || ( "$VERSION_MAJOR" -ge 4 && "$VERSION_MINOR" -ge 4 && "$VERSION_PATCH" -ge 2 ) ]]; then
        mongosh $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval 'db.hello().isWritablePrimary || db.hello().secondary' | grep -q 'true$'
    else
        mongosh  $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval 'db.isMaster().ismaster || db.isMaster().secondary' | grep -q 'true$'
    fi
  ping-mongodb.sh: |
    #!/bin/bash
    mongosh  $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval "db.adminCommand('ping')"
---
# Source: harness-prod/charts/platform/charts/mongodb/templates/replicaset/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-replicaset-chart-scripts
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
data:
  setup.sh: |-
    #!/bin/bash

    . /opt/bitnami/scripts/mongodb-env.sh
    . /opt/bitnami/scripts/libfs.sh
    . /opt/bitnami/scripts/liblog.sh
    . /opt/bitnami/scripts/libvalidations.sh

    if is_empty_value "$MONGODB_ADVERTISED_PORT_NUMBER"; then
      export MONGODB_ADVERTISED_PORT_NUMBER="$MONGODB_PORT_NUMBER"
    fi

    info "Advertised Hostname: $MONGODB_ADVERTISED_HOSTNAME"
    info "Advertised Port: $MONGODB_ADVERTISED_PORT_NUMBER"

    # Check for existing replica set in case there is no data in the PVC
    # This is for cases where the PVC is lost or for MongoDB caches without
    # persistence
    current_primary=""
    if is_dir_empty "${MONGODB_DATA_DIR}/db"; then
      info "Data dir empty, checking if the replica set already exists"
      current_primary=$(mongosh admin --host "mongodb-replicaset-chart-0.mongodb-replicaset-chart-headless.harness-1.svc.cluster.local:27017,mongodb-replicaset-chart-1.mongodb-replicaset-chart-headless.harness-1.svc.cluster.local:27017,mongodb-replicaset-chart-2.mongodb-replicaset-chart-headless.harness-1.svc.cluster.local:27017" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD --eval 'db.runCommand("ismaster")' | awk -F\' '/primary/ {print $2}')

      if ! is_empty_value "$current_primary"; then
        info "Detected existing primary: ${current_primary}"
      fi
    fi

    if ! is_empty_value "$current_primary" && [[ "$MONGODB_ADVERTISED_HOSTNAME:$MONGODB_ADVERTISED_PORT_NUMBER" == "$current_primary" ]]; then
        info "Advertised name matches current primary, configuring node as a primary"
        export MONGODB_REPLICA_SET_MODE="primary"
    elif ! is_empty_value "$current_primary" && [[ "$MONGODB_ADVERTISED_HOSTNAME:$MONGODB_ADVERTISED_PORT_NUMBER" != "$current_primary" ]]; then
        info "Current primary is different from this node. Configuring the node as replica of ${current_primary}"
        export MONGODB_REPLICA_SET_MODE="secondary"
        export MONGODB_INITIAL_PRIMARY_HOST="${current_primary%:*}"
        export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="${current_primary#*:}"
        export MONGODB_SET_SECONDARY_OK="yes"
    elif [[ "$MY_POD_NAME" = "mongodb-replicaset-chart-0" ]]; then
        info "Pod name matches initial primary pod name, configuring node as a primary"
        export MONGODB_REPLICA_SET_MODE="primary"
    else
        info "Pod name doesn't match initial primary pod name, configuring node as a secondary"
        export MONGODB_REPLICA_SET_MODE="secondary"
        export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="$MONGODB_PORT_NUMBER"
    fi

    if [[ "$MONGODB_REPLICA_SET_MODE" == "secondary" ]]; then
        export MONGODB_INITIAL_PRIMARY_ROOT_USER="$MONGODB_ROOT_USER"
        export MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD="$MONGODB_ROOT_PASSWORD"
        export MONGODB_ROOT_PASSWORD=""
        export MONGODB_EXTRA_USERNAMES=""
        export MONGODB_EXTRA_DATABASES=""
        export MONGODB_EXTRA_PASSWORDS=""
        export MONGODB_ROOT_PASSWORD_FILE=""
        export MONGODB_EXTRA_USERNAMES_FILE=""
        export MONGODB_EXTRA_DATABASES_FILE=""
        export MONGODB_EXTRA_PASSWORDS_FILE=""
    fi

    exec /opt/bitnami/scripts/mongodb/entrypoint.sh /opt/bitnami/scripts/mongodb/run.sh
  setup-hidden.sh: |-
    #!/bin/bash

    . /opt/bitnami/scripts/mongodb-env.sh

    echo "Advertised Hostname: $MONGODB_ADVERTISED_HOSTNAME"
    echo "Advertised Port: $MONGODB_ADVERTISED_PORT_NUMBER"
    echo "Configuring node as a hidden node"
    export MONGODB_REPLICA_SET_MODE="hidden"
    export MONGODB_INITIAL_PRIMARY_ROOT_USER="$MONGODB_ROOT_USER"
    export MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD="$MONGODB_ROOT_PASSWORD"
    export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="$MONGODB_PORT_NUMBER"
    export MONGODB_ROOT_PASSWORD=""
    export MONGODB_EXTRA_USERNAMES=""
    export MONGODB_EXTRA_DATABASES=""
    export MONGODB_EXTRA_PASSWORDS=""
    export MONGODB_ROOT_PASSWORD_FILE=""
    export MONGODB_EXTRA_USERNAMES_FILE=""
    export MONGODB_EXTRA_DATABASES_FILE=""
    export MONGODB_EXTRA_PASSWORDS_FILE=""
    exec /opt/bitnami/scripts/mongodb/entrypoint.sh /opt/bitnami/scripts/mongodb/run.sh
---
# Source: harness-prod/charts/platform/charts/next-gen-ui/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: next-gen-ui
  namespace: harness-1
data:
  API_URL: 'https://helm-test.qa.harness.io/gateway'
  DEPLOYMENT_TYPE: ON_PREM
  HARNESS_ENABLE_NG_AUTH_UI_PLACEHOLDER: "true"
---
# Source: harness-prod/charts/platform/charts/ng-auth-ui/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ng-auth-ui
  namespace: harness-1
data:
  DEPLOYMENT_TYPE: ON_PREM
  API_URL: /gateway
---
# Source: harness-prod/charts/platform/charts/ng-manager/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ng-manager
  namespace: harness-1
data:
  CACHE_BACKEND: "REDIS"
  DEPLOY_MODE: KUBERNETES_ONPREM
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  NG_MANAGER_TARGET: ng-manager:13002
  NG_MANAGER_AUTHORITY: ng-manager:13002
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  GRPC_SERVER_PORT: "9979"
  SHOULD_CONFIGURE_WITH_PMS: "true"
  PMS_GITSYNC_TARGET:  pipeline-service:14002
  PMS_GITSYNC_AUTHORITY:  pipeline-service:14002
  TEMPLATE_GITSYNC_TARGET:  template-service:16002
  TEMPLATE_GITSYNC_AUTHORITY:  template-service:16002
  PMS_TARGET:  pipeline-service:12011
  PMS_AUTHORITY:  pipeline-service:12011
  MEMORY: "4096m"
  LOGGING_LEVEL: INFO
  LOCK_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  LOCK_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  LOCK_CONFIG_USE_SENTINEL: "true"
  LOG_STREAMING_SERVICE_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  USE_REDIS_FOR_SDK_RESPONSE_EVENTS: "true"
  MOCK_ACCESS_CONTROL_SERVICE: "false"
  ACCESS_CONTROL_ENABLED: "true"
  ENABLE_DEFAULT_RESOURCE_GROUP_CREATION: "true"
  ENABLE_DASHBOARD_TIMESCALE: "true"
  AUDIT_ENABLED: "true"
  SCM_SERVICE_URI: "scm:8091"
  MANAGER_CLIENT_BASEURL: http://harness-manager.harness-1.svc.cluster.local:9090/api/
  NG_MANAGER_CLIENT_BASEURL: 'https://helm-test.qa.harness.io/ng/api/'
  MANAGER_UI_URL:  'https://helm-test.qa.harness.io'
  NG_MANAGER_API_URL: 'https://helm-test.qa.harness.io/ng/api/'
  NG_MANAGER_UI_URL: 'https://helm-test.qa.harness.io/ng/#/'
  LOG_STREAMING_SERVICE_BASEURL: 'http://log-service.harness-1.svc.cluster.local:8079/'
  ACCESS_CONTROL_BASE_URL: 'http://access-control.harness-1.svc.cluster.local:9006/api/'
  RESOURCE_GROUP_BASE_URL: 'http://platform-service.harness-1.svc.cluster.local:9005/api/'
  AUDIT_CLIENT_BASEURL: 'http://platform-service.harness-1.svc.cluster.local:9005/api/'
  CURRENT_GEN_UI_URL: 'https://helm-test.qa.harness.io/#/'
---
# Source: harness-prod/charts/platform/charts/pipeline-service/templates/configmap.yaml
apiVersion: v1
data:
  CACHE_BACKEND: "REDIS"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_USE_SENTINEL: "true"
  DEPLOY_MODE: KUBERNETES_ONPREM
  LOGGING_LEVEL: INFO
  LOCK_CONFIG_REDIS_SENTINELS: redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379
  LOCK_CONFIG_SENTINEL_MASTER_NAME: harness-redis
  LOCK_CONFIG_USE_SENTINEL: "true"
  MEMORY: "4096m"
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  GRPC_SERVER_PORT: "12011"
  NG_MANAGER_TARGET: ng-manager:9979
  NG_MANAGER_AUTHORITY: ng-manager:9979
  NG_MANAGER_GITSYNC_TARGET: ng-manager:13002
  NG_MANAGER_GITSYNC_AUTHORITY: ng-manager:13002
  CI_MANAGER_TARGET: ci-manager:9979
  CI_MANAGER_AUTHORITY: ci-manager:9979
  SCM_SERVICE_URI: "scm:8091"
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  PIPELINE_SERVICE_BASE_URL: 'https://helm-test.qa.harness.io/ng/#'
  PMS_API_BASE_URL: 'https://helm-test.qa.harness.io/pipeline/api/'
  LOG_STREAMING_SERVICE_BASEURL: 'https://helm-test.qa.harness.io/gateway/log-service/'
  MANAGER_BASE_URL: 'https://helm-test.qa.harness.io/api/'
  NG_MANAGER_BASE_URL: 'https://helm-test.qa.harness.io/ng/api/'
  WEBHOOK_TRIGGER_BASEURL: 'https://helm-test.qa.harness.io/ng/api/'
  CUSTOM_TRIGGER_BASEURL: 'https://helm-test.qa.harness.io/pipeline/api/'
  ACCESS_CONTROL_BASE_URL: 'https://helm-test.qa.harness.io/authz/api/'
  NOTIFICATION_BASE_URL: 'https://helm-test.qa.harness.io/notifications/api/'
  TEMPLATE_SERVICE_ENDPOINT: 'https://helm-test.qa.harness.io/template/api/'
  CI_MANAGER_BASE_URL: 'https://helm-test.qa.harness.io/ci/'
  MANAGER_CLIENT_BASEURL: 'https://helm-test.qa.harness.io/api/'
  AUTH_ENABLED: "true"
  USE_REDIS_FOR_INTERRUPTS: "true"
  USE_REDIS_FOR_ORCHESTRATION_EVENTS: "true"
  USE_REDIS_FOR_SDK_RESPONSE_EVENTS: "true"
  MOCK_ACCESS_CONTROL_SERVICE: "false"
  ACCESS_CONTROL_ENABLED: "true"
  ENABLE_DASHBOARD_TIMESCALE: "true"
  SHOULD_USE_INSTANCE_CACHE: "false"
  STO_MANAGER_BASE_URL: 'https://helm-test.qa.harness.io/sto-manager/'
  STO_MANAGER_AUTHORITY: sto-manager:9979
  STO_MANAGER_TARGET: sto-manager:9979
  CV_MANAGER_BASE_URL: 'https://helm-test.qa.harness.io/cv/api/'
  CVNG_MANAGER_AUTHORITY: cv-nextgen:9979
  CVNG_MANAGER_TARGET: cv-nextgen:9979

kind: ConfigMap
metadata:
  name: pipeline-service
  namespace: harness-1
---
# Source: harness-prod/charts/platform/charts/platform-service/templates/configmap.yaml
apiVersion: v1
data:
  DEPLOY_MODE: KUBERNETES_ONPREM
  GRPC_MANAGER_TARGET: harness-manager:9879
  GRPC_MANAGER_AUTHORITY: harness-manager:9879
  SMTP_HOST: ""
  SMTP_PORT: ""
  SMTP_PASSWORD: ""
  SMTP_USERNAME: ""
  SMTP_USE_SSL: "true"
  ENABLE_AUDIT_SERVICE: 'true'
  MOCK_ACCESS_CONTROL_SERVICE: 'false'
  AUDIT_ENABLED: 'true'
  ENABLE_RESOURCE_GROUP: 'true'
  EVENTS_FRAMEWORK_USE_SENTINEL: 'true'
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  LOCK_CONFIG_USE_SENTINEL: 'true'
  LOCK_CONFIG_SENTINEL_MASTER_NAME: 'harness-redis'
  LOCK_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  ACCESS_CONTROL_ENABLED: 'true'
  LOGGING_LEVEL: INFO
  DISTRIBUTED_LOCK_IMPLEMENTATION: REDIS
  MANAGER_CLIENT_BASEURL: 'https://helm-test.qa.harness.io/api/'
  RBAC_URL: 'https://helm-test.qa.harness.io/ng/api/'
  ACCESS_CONTROL_BASE_URL: 'http://access-control.harness-1.svc.cluster.local:9006/api/'
  RESOURCE_GROUP_CLIENT_BASE_URL: 'http://platform-service.harness-1.svc.cluster.local:9005/api/'
  NG_MANAGER_CLIENT_BASEURL: 'https://helm-test.qa.harness.io/ng/api/'
  PIPELINE_SERVICE_CLIENT_BASEURL: 'https://helm-test.qa.harness.io/pipeline/api/'
  TEMPLATE_SERVICE_CLIENT_BASEURL: 'https://helm-test.qa.harness.io/template/api/'
  AUDIT_CLIENT_BASEURL: 'http://platform-service.harness-1.svc.cluster.local:9005/api/'
  TEMPLATE_SERVICE_BASE_URL: http://template-service.harness-1.svc.cluster.local:15002/api/
  MEMORY: "3072m"

kind: ConfigMap
metadata:
  name: platform-service
  namespace: harness-1
---
# Source: harness-prod/charts/platform/charts/redis/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-sentinel-harness-configmap
  namespace: harness-1
  labels:
    app: redis-sentinel-harness
    helm.sh/chart: redis-0.2.0
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |
    dir "/data"
    port 6379
    active-defrag-cycle-max 25
    active-defrag-ignore-bytes 1mb
    activedefrag yes
    maxmemory 0
    maxmemory-policy volatile-lru
    min-replicas-max-lag 10
    min-replicas-to-write 1
    rdbchecksum yes
    rdbcompression yes
    repl-diskless-sync yes
    save 60 1
    maxclients 30000
    timeout 10

  sentinel.conf: |
    dir "/data"
        sentinel down-after-milliseconds harness-redis 10000
        sentinel failover-timeout harness-redis 180000
        maxclients 30000
        sentinel parallel-syncs harness-redis 5

  init.sh: |
    HOSTNAME="$(hostname)"
    INDEX="${HOSTNAME##*-}"
    MASTER="$(redis-cli -h redis-sentinel-harness -p 26379 sentinel get-master-addr-by-name harness-redis | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
    MASTER_GROUP="harness-redis"
    QUORUM="2"
    REDIS_CONF=/data/conf/redis.conf
    REDIS_PORT=6379
    SENTINEL_CONF=/data/conf/sentinel.conf
    SENTINEL_PORT=26379
    SERVICE=redis-sentinel-harness
    set -eu

    sentinel_update() {
        echo "Updating sentinel config with master $MASTER"
        eval MY_SENTINEL_ID="\${SENTINEL_ID_$INDEX}"
        sed -i "1s/^/sentinel myid $MY_SENTINEL_ID\\n/" "$SENTINEL_CONF"
        sed -i "2s/^/sentinel monitor $MASTER_GROUP $1 $REDIS_PORT $QUORUM \\n/" "$SENTINEL_CONF"
        echo "sentinel announce-ip $ANNOUNCE_IP" >> $SENTINEL_CONF
        echo "sentinel announce-port $SENTINEL_PORT" >> $SENTINEL_CONF
    }

    redis_update() {
        echo "Updating redis config"
        echo "slaveof $1 $REDIS_PORT" >> "$REDIS_CONF"
        echo "slave-announce-ip $ANNOUNCE_IP" >> $REDIS_CONF
        echo "slave-announce-port $REDIS_PORT" >> $REDIS_CONF
    }

    copy_config() {
        cp /readonly-config/redis.conf "$REDIS_CONF"
        cp /readonly-config/sentinel.conf "$SENTINEL_CONF"
    }

    setup_defaults() {
        echo "Setting up defaults"
        if [ "$INDEX" = "0" ]; then
            echo "Setting this pod as the default master"
            redis_update "$ANNOUNCE_IP"
            sentinel_update "$ANNOUNCE_IP"
            sed -i "s/^.*slaveof.*//" "$REDIS_CONF"
        else
            DEFAULT_MASTER="$(getent hosts "$SERVICE-announce-0" | awk '{ print $1 }')"
            if [ -z "$DEFAULT_MASTER" ]; then
                echo "Unable to resolve host"
                exit 1
            fi
            echo "Setting default slave config.."
            redis_update "$DEFAULT_MASTER"
            sentinel_update "$DEFAULT_MASTER"
        fi
    }

    find_master() {
        echo "Attempting to find master"
        if [ "$(redis-cli -h "$MASTER" ping)" != "PONG" ]; then
           echo "Can't ping master, attempting to force failover"
           if redis-cli -h "$SERVICE" -p "$SENTINEL_PORT" sentinel failover "$MASTER_GROUP" | grep -q 'NOGOODSLAVE' ; then
               setup_defaults
               return 0
           fi
           sleep 10
           MASTER="$(redis-cli -h $SERVICE -p $SENTINEL_PORT sentinel get-master-addr-by-name $MASTER_GROUP | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
           if [ "$MASTER" ]; then
               sentinel_update "$MASTER"
               redis_update "$MASTER"
           else
              echo "Could not failover, exiting..."
              exit 1
           fi
        else
            echo "Found reachable master, updating config"
            sentinel_update "$MASTER"
            redis_update "$MASTER"
        fi
    }

    mkdir -p /data/conf/

    echo "Initializing config.."
    copy_config

    ANNOUNCE_IP=$(getent hosts "$SERVICE-announce-$INDEX" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP" ]; then
        "Could not resolve the announce ip for this pod"
        exit 1
    elif [ "$MASTER" ]; then
        find_master
    else
        setup_defaults
    fi

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/replace-default-auth/${ESCAPED_AUTH}/" "$REDIS_CONF" "$SENTINEL_CONF"
    fi

    echo "Ready..."

  haproxy_init.sh: |
    HAPROXY_CONF=/data/haproxy.cfg
    cp /readonly/haproxy.cfg "$HAPROXY_CONF"
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-0 && break
      echo "Waiting for service redis-sentinel-harness-announce-0 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP0=$(getent hosts "redis-sentinel-harness-announce-0" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP0" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-0"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE0/$ANNOUNCE_IP0/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-1 && break
      echo "Waiting for service redis-sentinel-harness-announce-1 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP1=$(getent hosts "redis-sentinel-harness-announce-1" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP1" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-1"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE1/$ANNOUNCE_IP1/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-2 && break
      echo "Waiting for service redis-sentinel-harness-announce-2 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP2=$(getent hosts "redis-sentinel-harness-announce-2" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP2" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-2"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE2/$ANNOUNCE_IP2/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
---
# Source: harness-prod/charts/platform/charts/template-service/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: template-service
  namespace: harness-1
data:
  CACHE_BACKEND: "REDIS"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_USE_SENTINEL: "true"
  DEPLOY_MODE: KUBERNETES_ONPREM
  LOGGING_LEVEL: INFO
  MEMORY: "1024m"
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  NG_MANAGER_GITSYNC_TARGET: ng-manager:13002
  NG_MANAGER_GITSYNC_AUTHORITY: ng-manager:13002
  SCM_SERVICE_URI: "scm:8091"
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  ENABLE_AUTH: "true"
  ENABLE_GIT_SYNC: "true"
  ENABLE_AUDIT: "true"
  ACCESS_CONTROL_ENABLED: "true"
  ENFORCEMENT_CHECK_ENABLED: "true"
  NG_MANAGER_BASE_URL: 'https://helm-test.qa.harness.io/ng/api/'
  ACCESS_CONTROL_BASE_URL: 'http://access-control.harness-1.svc.cluster.local:9006/api/'
  AUDIT_SERVICE_BASE_URL: 'http://platform-service.harness-1.svc.cluster.local:9005/api/'
  SERVER_PORT: "15002"
  PMS_GRPC_AUTHORITY: pipeline-service:12011
  PMS_GRPC_TARGET: pipeline-service:12011
  MANAGER_CLIENT_BASEURL: https://helm-test.qa.harness.io/api/
---
# Source: harness-prod/charts/platform/charts/ti-service/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ti-service
  namespace: harness-1
data:
  TI_SERVICE_TIMESCALE_HOST: timescaledb-single-chart.harness-1
  TI_SERVICE_TIMESCALE_PORT: "5432"
  TI_SERVICE_DB_NAME: harnessti
  TI_SERVICE_MONGODB_DB_NAME: ti-harness
  TI_SERVICE_HYPER_TABLE: evaluation
  TI_SERVICE_DISABLE_AUTH: "true"
  TI_SERVICE_SELECTION_HYPER_TABLE: selection
  TI_SERVICE_COVERAGE_HYPER_TABLE: coverage
  TI_SERVICE_TIMESCALE_SSL_MODE: "require"
  EVENTS_FRAMEWORK_REDIS_URL: default
  EVENTS_FRAMEWORK_REDIS_USE_SENTINEL: 'true'
  EVENTS_FRAMEWORK_REDIS_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINEL_URLS: 'redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  TI_SERVICE_GLOBAL_TOKEN: 78d16b66-4b4c-11eb-8377-acde48001122
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescaledb-init
  namespace: harness-1
  annotations: {}
data:
  on_start: |
    #!/bin/bash

    # This script should only run on the master instance, Patroni
    # passes on the role in the second parameter
    echo "Running timescaledb-init"
    [ "$2" != "master" ] && exit 0

    echo "SELECT 'CREATE DATABASE harness' WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'harness')\gexec" | psql

    echo "SELECT 'CREATE DATABASE harnessti' WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'harnessti')\gexec" | psql
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescaledb-single-chart-patroni
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
    cluster-name: timescaledb-single-chart
data:
  patroni.yaml: |
    bootstrap:
      dcs:
        loop_wait: 10
        maximum_lag_on_failover: 33554432
        postgresql:
          parameters:
            archive_command: /etc/timescaledb/scripts/pgbackrest_archive.sh %p
            archive_mode: "on"
            archive_timeout: 1800s
            autovacuum_analyze_scale_factor: 0.02
            autovacuum_max_workers: 10
            autovacuum_vacuum_scale_factor: 0.05
            hot_standby: "on"
            log_autovacuum_min_duration: 0
            log_checkpoints: "on"
            log_connections: "on"
            log_disconnections: "on"
            log_line_prefix: '%t [%p]: [%c-%l] %u@%d,app=%a [%e] '
            log_lock_waits: "on"
            log_min_duration_statement: 1s
            log_statement: ddl
            max_connections: 100
            max_prepared_transactions: 150
            shared_preload_libraries: timescaledb,pg_stat_statements
            ssl: "on"
            ssl_cert_file: /etc/certificate/tls.crt
            ssl_key_file: /etc/certificate/tls.key
            tcp_keepalives_idle: 900
            tcp_keepalives_interval: 100
            temp_file_limit: 1GB
            timescaledb.passfile: ../.pgpass
            unix_socket_directories: /var/run/postgresql
            unix_socket_permissions: "0750"
            wal_level: hot_standby
            wal_log_hints: "on"
          use_pg_rewind: true
          use_slots: true
        retry_timeout: 10
        ttl: 30
      method: restore_or_initdb
      post_init: /etc/timescaledb/scripts/post_init.sh
      restore_or_initdb:
        command: |
          /etc/timescaledb/scripts/restore_or_initdb.sh --encoding=UTF8 --locale=C.UTF-8
        keep_existing_recovery_conf: true
    kubernetes:
      ports:
      - name: postgresql
        port: 5432
        targetPort: 5432
      role_label: role
      scope_label: cluster-name
      use_endpoints: true
    log:
      level: WARNING
    postgresql:
      authentication:
        replication:
          username: standby
        superuser:
          username: postgres
      basebackup:
      - waldir: /var/lib/postgresql/wal/pg_wal
      callbacks:
        on_reload: /etc/timescaledb/scripts/patroni_callback.sh
        on_restart: /etc/timescaledb/scripts/patroni_callback.sh
        on_role_change: /etc/timescaledb/scripts/patroni_callback.sh
        on_start: /etc/timescaledb/scripts/patroni_callback.sh
        on_stop: /etc/timescaledb/scripts/patroni_callback.sh
      create_replica_methods:
      - pgbackrest
      - basebackup
      listen: 0.0.0.0:5432
      pg_hba:
      - hostnossl all,replication all                all                reject
      - local     all             all                                   peer
      - hostssl   all             all                127.0.0.1/32       md5
      - hostssl   all             all                ::1/128            md5
      - hostssl   replication     standby            all                md5
      - hostssl   all             all                all                md5
      pgbackrest:
        command: /etc/timescaledb/scripts/pgbackrest_restore.sh
        keep_data: true
        no_master: 1
        no_params: true
      recovery_conf:
        restore_command: /etc/timescaledb/scripts/pgbackrest_archive_get.sh %f "%p"
      use_unix_socket: true
    restapi:
      listen: 0.0.0.0:8008
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescaledb-single-chart-scripts
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
    cluster-name: timescaledb-single-chart
data:
  # If no backup is configured, archive_command would normally fail. A failing archive_command on a cluster
  # is going to cause WAL to be kept around forever, meaning we'll fill up Volumes we have quite quickly.
  #
  # Therefore, if the backup is disabled, we always return exitcode 0 when archiving
  pgbackrest_archive.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 0

    source "${HOME}/.pgbackrest_environment"
    exec pgbackrest --stanza=poddb archive-push $1
  pgbackrest_archive_get.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 1

    source "${HOME}/.pgbackrest_environment"
    exec pgbackrest --stanza=poddb archive-get ${1} "${2}"
  pgbackrest_bootstrap.sh: |
    #!/bin/bash
    set -e

    function log {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }

    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done

    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" == "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
        }
    fi

    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
  pgbackrest_restore.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0
    [ "${PGBACKREST_BACKUP_ENABLED}" == "0" ] && exit 1

    source "${HOME}/.pod_environment"

    PGDATA="/var/lib/postgresql/data"
    WALDIR="/var/lib/postgresql/wal/pg_wal"

    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"

    pgbackrest --stanza=poddb --force --delta --log-level-console=detail restore
  restore_or_initdb.sh: |
    #!/bin/bash

    source "${HOME}/.pod_environment"

    function log {
      echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }

    PGDATA="/var/lib/postgresql/data"
    WALDIR="/var/lib/postgresql/wal/pg_wal"

    # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
    # initdb has no business with these parameters
    initdb_args=""
    for value in "$@"
    do
      [[ $value == --scope* ]] || [[ $value == --datadir* ]] || initdb_args="${initdb_args} $value"
    done

    log "Invoking initdb"
    initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    echo "include_if_exists = '/var/run/postgresql/timescaledb.conf'" >> "${PGDATA}/postgresql.conf"

  post_init.sh: |
    #!/bin/bash
    PGBACKREST_BACKUP_ENABLED=0

    source "${HOME}/.pod_environment"

    function log {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }

    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
      \connect template1
      -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;

      \connect postgres
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;
    __SQL__

    TABLESPACES=""
    for tablespace in $TABLESPACES
    do
      log "Creating tablespace ${tablespace}"
      tablespacedir="/var/lib/postgresql/tablespaces/${tablespace}/data"
      psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done

    if [ "${PGBACKREST_BACKUP_ENABLED}" == "1" ]; then
      log "Waiting for pgBackRest API to become responsive"
      while sleep 1; do
          if [ $SECONDS -gt 10 ]; then
              log "pgBackRest API did not respond within $SECONDS seconds, will not trigger a backup"
              exit 0
          fi
          timeout 1 bash -c "echo > /dev/tcp/localhost/8081" 2>/dev/null && break
      done

      log "Triggering pgBackRest backup"
      curl -i -X POST http://localhost:8081/backups
    fi

    # We always exit 0 this script, otherwise the database initialization fails.
    exit 0
  patroni_callback.sh: |
    #!/bin/bash
    set -e

    source "${HOME}/.pod_environment"

    for suffix in "$1" all
    do
      CALLBACK="/etc/timescaledb/callbacks/${suffix}"
      if [ -f "${CALLBACK}" ]
      then
        "${CALLBACK}" $@
      fi
    done

  lifecycle_preStop.psql: |
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''

    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '/var/run/postgresql', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset

    \timing on
    \set ECHO queries

    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif

    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;

    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
---
# Source: harness-prod/charts/sto/charts/sto-core/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: sto-core
  namespace: harness-1
data:
  APP_ENABLE_UI: "true"
  APP_ACL_URL: 'https://helm-test.qa.harness.io/authz/api'
  APP_TOKEN_JWT_SECRET: "HVSKUYqD4e5Rxu12hFDdCJKGM64sxgEynvdDhaOHaTHhwwn0K4Ttr0uoOxSsEVYNrUU="
  APP_INTERNAL_TOKEN_JWT_SECRET: "dOkdsVqdRPPRJG31XU0qY4MPqmBBMk0PTAGIKM6O7TGqhjyxScIdJe80mwh5Yb5zF3KxYBHw6B3Lfzlq"
  APP_HARNESS_TOKEN: "token"
  APP_NG_URL: 'https://helm-test.qa.harness.io'
  APP_AUDIT_JWT_SECRET: test
---
# Source: harness-prod/charts/sto/charts/sto-manager/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: sto-manager
  namespace: harness-1
data:
  DEPLOY_MODE: KUBERNETES_ONPREM
  MEMORY: "2500"
  STO_SERVICE_ENDPOINT:  'https://helm-test.qa.harness.io/sto/'
  STO_SERVICE_GLOBAL_TOKEN: "token"
  NG_MANAGER_URL: 'http://ng-manager.harness-1.svc.cluster.local:7090/api/'
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  MANAGER_URL: 'http://harness-manager.harness-1.svc.cluster.local:9090/api/'
  VERIFICATION_SERVICE_SECRET: 67d9b94d9856665afc21acd3aa745401
  SCM_SERVICE_URI: "scm:8091"
  LOG_SERVICE_ENDPOINT: 'https://helm-test.qa.harness.io/gateway/log-service/'
  TI_SERVICE_ENDPOINT: 'http://ti-service.harness-1.svc.cluster.local:8078/'
  ENABLE_APPDYNAMICS: "false"
  DISABLE_NEW_RELIC: "true"
  SHOULD_CONFIGURE_WITH_PMS: "true"
  PMS_TARGET: pipeline-service:12011
  PMS_AUTHORITY: pipeline-service:12011
  GRPC_SERVER_PORT: "9979"
  DEFAULT_INTERNAL_IMAGE_CONNECTOR: test
  API_URL: 'https://helm-test.qa.harness.io/ng/#'
  DEFAULT_CPU_LIMIT: "500"
  DEFAULT_MEMORY_LIMIT: "600"
  MONGO_INDEX_MANAGER_MODE: AUTO
  STACK_DRIVER_LOGGING_ENABLED: "false"
  GOOGLE_APPLICATION_CREDENTIALS: /opt/harness/svc/cloud_stackdriver.json
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  EVENTS_FRAMEWORK_REDIS_SSL_ENABLED: "true"
  EVENTS_FRAMEWORK_REDIS_SSL_CA_TRUST_STORE_PATH: /opt/harness/svc/redis_labs_ca_truststore
  USE_REDIS_FOR_SDK_RESPONSE_EVENTS: "true"
  ENABLE_DASHBOARD_TIMESCALE: "true"
  TIMESCALE_URI: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
  CACHE_BACKEND: REDIS
  CACHE_CONFIG_USE_SENTINEL: "true"
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.harness-1:26379,redis://redis-sentinel-harness-announce-1.harness-1:26379,redis://redis-sentinel-harness-announce-2.harness-1:26379'
  #CACHE_NAMESPACE:
  ENFORCEMENT_CHECK_ENABLED: "false"
  PMS_SDK_ORCHESTRATION_EVENT_POOL_CORE_SIZE: "15"
#  VERSION: 1.0.<+regex.extract("^[0-9]+", <+artifact.tag>)>
  S3_UPLOAD_IMAGE: docker.io/bewithaman/s3:latest
  USE_DMS: "false"
  SECURITY_IMAGE: docker.io/harness/sto-plugin:latest
---
# Source: harness-prod/charts/platform/charts/minio/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: minio
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "8Gi"
---
# Source: harness-prod/charts/platform/charts/harness-manager/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: harness-manager-role
  namespace: harness-1
  annotations: {}
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: timescaledb-single-chart
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
  annotations: {}
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources: ["services"]
  verbs:
  - create
- apiGroups: [""]
  resources:
  - endpoints
  - endpoints/restricted
  verbs:
  - create
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources: ["pods"]
  verbs:
  - get
  - list
  - patch
  - update
  - watch
---
# Source: harness-prod/charts/platform/charts/harness-manager/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: harness-manager-role-binding
  namespace: harness-1
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: harness-manager-role
subjects:
  - kind: ServiceAccount
    name: harness-default
    namespace: harness-1
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: timescaledb-single-chart
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
  annotations: {}
subjects:
  - kind: ServiceAccount
    name: harness-default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: timescaledb-single-chart
---
# Source: harness-prod/charts/ci/charts/ci-manager/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ci-manager
  namespace: harness-1
  labels:
    helm.sh/chart: ci-manager-0.2.7
    app.kubernetes.io/name: ci-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: ci-manager
    port: 7090
    protocol: TCP
    targetPort: 7090
  - name: grpc-ci-manager
    port: 9979
    protocol: TCP
    targetPort: 9979
  selector:
    app.kubernetes.io/name: ci-manager
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/platform/charts/access-control/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name:  access-control
  namespace: harness-1
  labels:
    helm.sh/chart: access-control-0.2.4
    app.kubernetes.io/name: access-control
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 9006
    protocol: TCP
    targetPort: 9006
  selector:
    app.kubernetes.io/name: access-control
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/platform/charts/cv-nextgen/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cv-nextgen
  namespace: harness-1
  labels:
    helm.sh/chart: cv-nextgen-0.2.8
    app.kubernetes.io/name: cv-nextgen
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: cv
      port: 6060
      protocol: TCP
      targetPort: 6060
    - name: grpc-cv-nextgen
      port: 9979
      protocol: TCP
      targetPort: 9979
  selector:
    app.kubernetes.io/name: cv-nextgen
    app.kubernetes.io/instance: chart
  type: ClusterIP
---
# Source: harness-prod/charts/platform/charts/delegate-proxy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: delegate-proxy
  namespace: harness-1
  labels:
    helm.sh/chart: delegate-proxy-0.2.0
    app.kubernetes.io/name: delegate-proxy
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: delegate-proxy
    app.kubernetes.io/instance: chart
  ports:
    - port: 80
      targetPort: 8080
---
# Source: harness-prod/charts/platform/charts/gateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gateway
  namespace: harness-1
  labels:
    helm.sh/chart: gateway-0.2.5
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort:  8080
  selector:
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/platform/charts/harness-manager/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: harness-manager-0.2.9
    app.kubernetes.io/name: harness-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: harness-manager
  namespace: harness-1
  annotations: {}
spec:
  ports:
    - name: http-manager
      port: 9090
      protocol: TCP
      targetPort: 9090
    - name: grpc-manager
      port: 9879
      protocol: TCP
      targetPort: 9879
  selector:
    app.kubernetes.io/name: harness-manager
    app.kubernetes.io/instance: chart
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness-prod/charts/platform/charts/log-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: log-service
  namespace: harness-1
  labels:
    helm.sh/chart: log-service-0.2.7
    app.kubernetes.io/name: log-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: log-service
    port: 8079
    protocol: TCP
    targetPort: 8079
  selector:
    app.kubernetes.io/name: log-service
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/platform/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: minio-api
      port: 9000
      targetPort: minio-api
      nodePort: null
    - name: minio-console
      port: 9001
      targetPort: minio-console
      nodePort: null
  selector:
    app.kubernetes.io/name: minio
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/platform/charts/mongodb/templates/replicaset/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongodb-replicaset-chart
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: "mongodb"
      port: 27017
      targetPort: mongodb
  selector:
    app.kubernetes.io/name: mongodb
    app.kubernetes.io/instance: chart
    app.kubernetes.io/component: mongodb
---
# Source: harness-prod/charts/platform/charts/next-gen-ui/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: next-gen-ui
  namespace: harness-1
  labels:
    helm.sh/chart: next-gen-ui-0.2.1
    app.kubernetes.io/name: next-gen-ui
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app.kubernetes.io/name: next-gen-ui
    app.kubernetes.io/instance: chart
  sessionAffinity: None
---
# Source: harness-prod/charts/platform/charts/ng-auth-ui/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ng-auth-ui
  namespace: harness-1
  labels:
    helm.sh/chart: ng-auth-ui-0.2.0
    app.kubernetes.io/name: ng-auth-ui
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app.kubernetes.io/name: ng-auth-ui
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/platform/charts/ng-manager/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ng-manager
  namespace: harness-1
  labels:
    helm.sh/chart: ng-manager-0.2.12
    app.kubernetes.io/name: ng-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http-ng-manager
    port: 7090
    protocol: TCP
    targetPort: 7090
  - name: grpc-ng-manager
    port: 9979
    protocol: TCP
    targetPort: 9979
  - name: grpc-git-sync
    port: 13002
    protocol: TCP
    targetPort: 13002
  selector:
    app.kubernetes.io/name: ng-manager
    app.kubernetes.io/instance: chart
  sessionAffinity: None
---
# Source: harness-prod/charts/platform/charts/pipeline-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: pipeline-service-0.2.7
    app.kubernetes.io/name: pipeline-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: pipeline-service
  namespace: harness-1
spec:
  ports:
  - name: grpc-pms
    port: 12011
    protocol: TCP
    targetPort: 12011
  - name: http-pms
    port: 12001
    protocol: TCP
    targetPort: 12001
  - name: grpc-gitsync
    port: 14002
    protocol: TCP
    targetPort: 14002
  selector:
    app.kubernetes.io/name: pipeline-service
    app.kubernetes.io/instance: chart
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness-prod/charts/platform/charts/platform-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: platform-service-0.2.6
    app.kubernetes.io/name: platform-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: platform-service
  namespace: harness-1
spec:
  ports:
    - name: http
      port: 9005
      protocol: TCP
      targetPort: 9005
  selector:
    app.kubernetes.io/name: platform-service
    app.kubernetes.io/instance: chart
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness-prod/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness
  namespace: harness-1
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.0
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
---
# Source: harness-prod/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness-announce-0
  namespace: harness-1
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.0
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
    "statefulset.kubernetes.io/pod-name": redis-sentinel-harness-server-0
---
# Source: harness-prod/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness-announce-1
  namespace: harness-1
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.0
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
    "statefulset.kubernetes.io/pod-name": redis-sentinel-harness-server-1
---
# Source: harness-prod/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness-announce-2
  namespace: harness-1
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.0
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
    "statefulset.kubernetes.io/pod-name": redis-sentinel-harness-server-2
---
# Source: harness-prod/charts/platform/charts/scm-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: scm-service-0.2.0
    app.kubernetes.io/name: scm-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: scm-service
  namespace: harness-1
spec:
  ports:
    - name: scm
      port: 8091
      protocol: TCP
      targetPort: 8091
  selector:
    app.kubernetes.io/name: scm-service
    app.kubernetes.io/instance: chart
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness-prod/charts/platform/charts/template-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: template-service-0.2.8
    app.kubernetes.io/name: template-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: template-service
  namespace: harness-1
spec:
  ports:
  - name: grpc-template
    port: 15011
    protocol: TCP
    targetPort: 15011
  - name: http-template
    port: 15002
    protocol: TCP
    targetPort: 15002
  - name: grpc-gitsync
    port: 16002
    protocol: TCP
    targetPort: 16002
  selector:
    app.kubernetes.io/name: template-service
    app.kubernetes.io/instance: chart
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness-prod/charts/platform/charts/ti-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ti-service
  namespace: harness-1
  labels:
    helm.sh/chart: ti-service-0.2.3
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: ti-service
    port: 8078
    protocol: TCP
    targetPort: 8078
  selector:
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: timescaledb-single-chart-config
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
    cluster-name: timescaledb-single-chart
  annotations: {}
spec:
  selector:
    app: timescaledb-single-chart
    cluster-name: timescaledb-single-chart
  type: ClusterIP
  clusterIP: None
  ports:
  - name: patroni
    port: 8008
    protocol: TCP
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: timescaledb-single-chart-replica
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
    cluster-name: timescaledb-single-chart
    role: replica
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "4000"
spec:
  selector:
    app: timescaledb-single-chart
    cluster-name: timescaledb-single-chart
    role: replica
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432, even if `!replicaLoadBalancer.enabled`.
    port: 5432
    protocol: TCP
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: timescaledb-single-chart
  namespace: harness-1
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
    cluster-name: timescaledb-single-chart
    role: master
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "4000"
spec:
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432, even if `!loadBalancer.enabled`.
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: harness-prod/charts/sto/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-hl
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app: postgres
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: chart
    app.kubernetes.io/component: primary
---
# Source: harness-prod/charts/sto/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app: postgres
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: chart
    app.kubernetes.io/component: primary
---
# Source: harness-prod/charts/sto/charts/sto-core/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: sto-core
  namespace: harness-1
  labels:
    helm.sh/chart: sto-core-0.2.4
    app.kubernetes.io/name: sto-core
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 4000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: sto-core
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/sto/charts/sto-manager/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: sto-manager
  namespace: harness-1
  labels:
    helm.sh/chart: sto-manager-0.2.7
    app.kubernetes.io/name: sto-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 7090
      targetPort: http
      protocol: TCP
      name: http
    - port: 9979
      targetPort: sto-mgr-grpc
      name: sto-mgr-grpc
  selector:
    app.kubernetes.io/name: sto-manager
    app.kubernetes.io/instance: chart
---
# Source: harness-prod/charts/ci/charts/ci-manager/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ci-manager
  namespace: harness-1
  labels:
    helm.sh/chart: ci-manager-0.2.7
    app.kubernetes.io/name: ci-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 300
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ci-manager
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ci-manager
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-harness-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=harness-manager"
      containers:
        - name: ci-manager
          image: docker.io/harness/ci-manager-signed:76019
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          ports:
          - name: http-ci-manager
            containerPort: 7090
            protocol: "TCP"
          - name: grpc-ci-manager
            containerPort: 9979
            protocol: "TCP"
          resources:
            limits:
              cpu: 1
              memory: 8192Mi
            requests:
              cpu: 1
              memory: 1400Mi
          env:
            - name: STO_SERVICE_GLOBAL_TOKEN
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: stoAppHarnessToken
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/harness?replicaSet=rs0&authSource=admin'
            - name: CIMANAGER_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/harness-ci?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
            - name: TIMESCALE_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
          envFrom:
          - configMapRef:
              name: ci-manager
          readinessProbe:
            httpGet:
              path: /health
              port: http-ci-manager
            initialDelaySeconds: 60
            timeoutSeconds: 5
            periodSeconds: 5
            failureThreshold: 8
          livenessProbe:
            httpGet:
              path: /health
              port: http-ci-manager
            initialDelaySeconds: 40
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 20
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ci-manager
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/access-control/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  access-control
  namespace: harness-1
  labels:
    helm.sh/chart: access-control-0.2.4
    app.kubernetes.io/name: access-control
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 720
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: access-control
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: access-control
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
        - name: wait-for-mongo
          image: docker.io/harness/helm-init-container:latest
          imagePullPolicy: IfNotPresent
          args:
            - "pod"
            - "-lapp=mongodb-replicaset"
      containers:
        - name: access-control
          image: docker.io/harness/accesscontrol-service-signed:76000
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          readinessProbe:
            httpGet:
              path: /api/health
              port: 9006
            initialDelaySeconds: 200
            timeoutSeconds: 20
            periodSeconds: 10
            failureThreshold: 40
          livenessProbe:
            httpGet:
              path: /api/health
              port: 9006
            initialDelaySeconds: 200
            timeoutSeconds: 20
            periodSeconds: 10
            failureThreshold: 40
          resources:
            limits:
              cpu: 0.5
              memory: 8192Mi
            requests:
              cpu: 0.5
              memory: 512Mi
          ports:
          - name: http
            containerPort: 9006
            protocol: "TCP"
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGODB_HOSTS
              value: 'mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc:27017,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc:27017,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017'
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/accesscontrol?replicaSet=rs0&authSource=admin'
            - name: OFFSET_STORAGE_FILE_FILENAME
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/accesscontrol?replicaSet=rs0&authSource=admin'
            - name: IDENTITY_SERVICE_SECRET
              value: 'HVSKUYqD4e5Rxu12hFDdCJKGM64sxgEynvdDhaOHaTHhwwn0K4Ttr0uoOxSsEVYNrUU='
          envFrom:
          - configMapRef:
              name: access-control
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - access-control
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/change-data-capture/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: change-data-capture
  namespace: harness-1
  labels:
    helm.sh/chart: change-data-capture-0.2.7
    app.kubernetes.io/name: change-data-capture
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: change-data-capture
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: change-data-capture
        app.kubernetes.io/name: change-data-capture
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-ng-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=ng-manager"
      containers:
        - name: change-data-capture
          image: docker.io/harness/cdcdata-signed:75618
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALEDB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/harness?replicaSet=rs0&authSource=admin'
            - name: EVENTS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/events?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
            - name: CDC_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/change-data-capture?replicaSet=rs0&authSource=admin'
            - name: NG_HARNESS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/ng-harness?replicaSet=rs0&authSource=admin'
            - name: TIMESCALEDB_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
          envFrom:
          - configMapRef:
              name: change-data-capture
          resources:
            limits:
              cpu: 1
              memory: 2880Mi
            requests:
              cpu: 1
              memory: 2880Mi
          readinessProbe:
            httpGet:
              path: /health
              port: 8190
            initialDelaySeconds: 300
            timeoutSeconds: 10
            periodSeconds: 10
            failureThreshold: 20
          livenessProbe:
            httpGet:
              path: /health
              port: 8190
            initialDelaySeconds: 400
            timeoutSeconds: 10
            periodSeconds: 10
            failureThreshold: 10
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - change-data-capture
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/cv-nextgen/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cv-nextgen
  namespace: harness-1
  labels:
    helm.sh/chart: cv-nextgen-0.2.8
    app.kubernetes.io/name: cv-nextgen
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: cv-nextgen
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cv-nextgen
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      initContainers:
      - name: wait-for-harness-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=harness-manager"
      containers:
        - name: cv-nextgen
          image: docker.io/harness/cv-nextgen-signed:76019
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/cvng-harness?replicaSet=rs0&authSource=admin'
            - name : NOTIFICATION_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/notifications?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
          envFrom:
            - configMapRef:
                name: cv-nextgen
          resources:
            limits:
              cpu: 1
              memory: 6144Mi
            requests:
              cpu: 1
              memory: 6144Mi
          readinessProbe:
            httpGet:
              path: /cv/api/health
              port: cv
            initialDelaySeconds: 60
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /cv/api/health
              port: cv
            initialDelaySeconds: 300
            periodSeconds: 10
            failureThreshold: 2
          ports:
            - name: cv
              containerPort: 6060
              protocol: "TCP"
            - name: grpc-cv-ng
              containerPort: 9979
              protocol: "TCP"
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cv-nextgen
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/delegate-proxy/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: delegate-proxy
  namespace: harness-1
  labels:
    helm.sh/chart: delegate-proxy-0.2.0
    app.kubernetes.io/name: delegate-proxy
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: delegate-proxy
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: delegate-proxy
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        runAsUser: 101
      containers:
        - name: delegate-proxy
          image: docker.io/harness/delegate-proxy-signed:76018
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 200m
              memory: 100Mi
            requests:
              cpu: 200m
              memory: 100Mi
          volumeMounts:
            - mountPath: /etc/nginx/conf.d
              name: harness-nginx-conf
      volumes:
        - name: harness-nginx-conf
          configMap:
            name: delegate-proxy
            items:
              - key: proxy.conf
                path: proxy.conf
---
# Source: harness-prod/charts/platform/charts/gateway/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway
  namespace: harness-1
  labels:
    helm.sh/chart: gateway-0.2.5
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: gateway
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: gateway
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-ng-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=ng-manager"
      containers:
        - name: gateway
          image: docker.io/harness/gateway-signed:1000138
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          readinessProbe:
            httpGet:
              path: /actuator/health
              port: gateway-port
            initialDelaySeconds: 120
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /actuator/health
              port: gateway-port
            initialDelaySeconds: 180
            periodSeconds: 20
            failureThreshold: 2
          ports:
          - name: gateway-port
            containerPort: 8080
            protocol: "TCP"
          resources:
            limits:
              cpu: 0.4
              memory: 1024Mi
            requests:
              cpu: 0.2
              memory: 512Mi
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGO_DB_URL
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/gateway?replicaSet=rs0&authSource=admin'
          envFrom:
          - configMapRef:
              name: gateway
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - gateway
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/harness-manager/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: harness-manager
  namespace: harness-1
  labels:
    helm.sh/chart: harness-manager-0.2.9
    app.kubernetes.io/name: harness-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations: {}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: harness-manager
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: harness-manager
        app.kubernetes.io/name: harness-manager
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-redis
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=redis-sentinel"
      containers:
        - envFrom:
            - configMapRef:
                name: harness-manager-config
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALEDB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/harness?replicaSet=rs0&authSource=admin'
            - name: TIMESCALEDB_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
          image: docker.io/harness/manager-signed:76019
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          lifecycle:
            preStop:
              exec:
                command:
                  - touch
                  - shutdown
          livenessProbe:
            failureThreshold: 20
            initialDelaySeconds: 180
            httpGet:
              path: /api/version
              port: 9090
              scheme: HTTP
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
            - containerPort: 9879
              protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /api/health
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 90
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 2
              memory: 8192Mi
            requests:
              cpu: 2
              memory: 3000Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - harness-manager
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/le-nextgen/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: le-nextgen
  namespace: harness-1
  labels:
    helm.sh/chart: le-nextgen-0.2.0
    app.kubernetes.io/name: le-nextgen
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: le-nextgen
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: le-nextgen
        app.kubernetes.io/instance: chart
    spec:
      containers:
        - envFrom:
            - configMapRef:
                name: le-nextgen
          image: docker.io/harness/le-nextgen-signed:66501
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
          name: le-nextgen
          ports:
            - containerPort: 8108
              name: learning
              protocol: TCP
          resources:
            limits:
              cpu: 1
              memory: 6144Mi
            requests:
              cpu: 1
              memory: 6144Mi
      serviceAccountName: harness-default
      securityContext:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - le-nextgen
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/log-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: log-service
  namespace: harness-1
  labels:
    helm.sh/chart: log-service-0.2.7
    app.kubernetes.io/name: log-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 300
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: log-service
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: log-service
        app.kubernetes.io/instance: chart
    spec:
      terminationGracePeriodSeconds: 30
      serviceAccountName: harness-default
      securityContext:
        {}
      containers:
      - name: log-service
        envFrom:
        - configMapRef:
            name: log-service
        env:
          - name: LOG_SERVICE_S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: minio
                key: root-user
          - name: LOG_SERVICE_S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: minio
                key: root-password
        livenessProbe:
          httpGet:
            path: /healthz
            port: http-log-svc
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: http-log-svc
          initialDelaySeconds: 10
          periodSeconds: 10
        image: docker.io/harness/log-service-signed:release-18
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
        - name: http-log-svc
          containerPort: 8079
          protocol: "TCP"
        resources:
            limits:
              cpu: 1
              memory: 3072Mi
            requests:
              cpu: 1
              memory: 3072Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - log-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/minio/templates/standalone/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: minio
      app.kubernetes.io/instance: chart
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: minio
        helm.sh/chart: minio-11.9.1
        app.kubernetes.io/instance: chart
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/credentials-secret: 7ecef5f9994ecdf2167f0f129d7860db9e307aa34112a304e9af78d8af0e5869
    spec:

      serviceAccountName: minio
      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: minio
                    app.kubernetes.io/instance: chart
                namespaces:
                  - "harness-1"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2022.8.22-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_FORCE_NEW_KEYS
              value: "no"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: minio
                  key: root-password
            - name: MINIO_DEFAULT_BUCKETS
              value: logs
            - name: MINIO_BROWSER
              value: "on"
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
            - name: MINIO_CONSOLE_PORT_NUMBER
              value: "9001"
          envFrom:
          ports:
            - name: minio-api
              containerPort: 9000
              protocol: TCP
            - name: minio-console
              containerPort: 9001
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: minio-api
              scheme: "HTTP"
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            tcpSocket:
              port: minio-api
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: minio
---
# Source: harness-prod/charts/platform/charts/next-gen-ui/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: next-gen-ui
  namespace: harness-1
  labels:
    helm.sh/chart: next-gen-ui-0.2.1
    app.kubernetes.io/name: next-gen-ui
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: next-gen-ui
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: next-gen-ui
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-change-data-capture
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=change-data-capture"
      containers:
      - name: next-gen-ui
        image: docker.io/harness/nextgenui-signed:0.312.15
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 101
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 100
          periodSeconds: 20
          failureThreshold: 2
        ports:
        - name: ng-ui-port
          containerPort: 8080
          protocol: "TCP"
        resources:
            limits:
              cpu: 0.2
              memory: 200Mi
            requests:
              cpu: 0.2
              memory: 200Mi
        envFrom:
        - configMapRef:
            name: next-gen-ui
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - next-gen-ui
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/ng-auth-ui/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-auth-ui
  namespace: harness-1
  labels:
    helm.sh/chart: ng-auth-ui-0.2.0
    app.kubernetes.io/name: ng-auth-ui
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ng-auth-ui
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ng-auth-ui
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      containers:
      - name: ng-auth-ui
        image: docker.io/harness/ng-auth-ui-signed:0.41.0
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 101
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 90
          periodSeconds: 20
          failureThreshold: 2
        ports:
        - name: ng-auth-ui-port
          containerPort: 8080
          protocol: "TCP"
        envFrom:
        - configMapRef:
            name: ng-auth-ui
        resources:
            limits:
              cpu: 0.5
              memory: 512Mi
            requests:
              cpu: 0.5
              memory: 512Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ng-auth-ui
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/ng-manager/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-manager
  namespace: harness-1
  labels:
    helm.sh/chart: ng-manager-0.2.12
    app.kubernetes.io/name: ng-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 800
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ng-manager
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app: ng-manager
        app.kubernetes.io/name: ng-manager
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-harness-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=harness-manager"
      containers:
      - name: ng-manager
        image: docker.io/harness/ng-manager-signed:76019
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
        - name: http-ng-manager
          containerPort: 7090
          protocol: "TCP"
        - name: grpc-ng-manager
          containerPort: 9979
          protocol: "TCP"
        - name: grpc-git-sync
          containerPort: 13002
          protocol: "TCP"
        resources:
            limits:
              cpu: 2
              memory: 8192Mi
            requests:
              cpu: 2
              memory: 200Mi
        env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/ng-harness?replicaSet=rs0&authSource=admin'
            - name : NOTIFICATION_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/notifications?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: TIMESCALE_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
        envFrom:
        - configMapRef:
            name: ng-manager
        readinessProbe:
          httpGet:
            path: /health
            port: 7090
          initialDelaySeconds: 180
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 20
        livenessProbe:
          httpGet:
            path: /health
            port: 7090
          initialDelaySeconds: 180
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 10
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ng-manager
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/pipeline-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pipeline-service
  namespace: harness-1
  labels:
    helm.sh/chart: pipeline-service-0.2.7
    app.kubernetes.io/name: pipeline-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  progressDeadlineSeconds: 300
  selector:
    matchLabels:
      app.kubernetes.io/name: pipeline-service
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: pipeline-service
        app.kubernetes.io/name: pipeline-service
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      containers:
      - name: pipeline-service
        image: docker.io/harness/pipeline-service-signed:76019
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
          - name: grpc-pms
            containerPort: 12011
            protocol: "TCP"
          - name: http-pms
            containerPort: 12001
            protocol: "TCP"
          - name: grpc-gitsync
            containerPort: 14002
            protocol: "TCP"
        resources:
            limits:
              cpu: 1
              memory: 6144Mi
            requests:
              cpu: 1
              memory: 6144Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name : TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: timescaledbPostgresPassword
          - name: MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
          - name : NOTIFICATION_MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/notifications?replicaSet=rs0&authSource=admin'
          - name: TIMESCALEDB_USERNAME
            value: postgres
          - name: TIMESCALE_URI
            value: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
        envFrom:
        - configMapRef:
            name: pipeline-service
        readinessProbe:
          httpGet:
            path: /api/health
            port: 12001
          initialDelaySeconds: 60
          timeoutSeconds: 5
          periodSeconds: 5
          failureThreshold: 8
        livenessProbe:
          httpGet:
            path: /api/health
            port: 12001
          initialDelaySeconds: 40
          timeoutSeconds: 5
          periodSeconds: 10
          failureThreshold: 20
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - pipeline-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/platform-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: platform-service
  namespace: harness-1
  labels:
    helm.sh/chart: platform-service-0.2.6
    app.kubernetes.io/name: platform-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  progressDeadlineSeconds: 550
  selector:
    matchLabels:
      app.kubernetes.io/name: platform-service
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: platform-service
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      containers:
      - name: platform-service
        image: docker.io/harness/platform-service-signed:76201
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
        - name: http
          containerPort: 9005
          protocol: "TCP"
        resources:
            limits:
              cpu: 0.5
              memory: 8192Mi
            requests:
              cpu: 0.5
              memory: 512Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name: MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/notifications?replicaSet=rs0&authSource=admin'
          - name: AUDIT_MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/audits?replicaSet=rs0&authSource=admin'
          - name: RESOURCE_GROUP_MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/resource-groups?replicaSet=rs0&authSource=admin'
        envFrom:
        - configMapRef:
            name: platform-service

        readinessProbe:
          httpGet:
            path: /api/health
            port: 9005
          initialDelaySeconds: 100
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 20
        livenessProbe:
          httpGet:
            path: /api/health
            port: 9005
          initialDelaySeconds: 300
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 20
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - platform-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/scm-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scm
  namespace: harness-1
  labels:
    helm.sh/chart: scm-service-0.2.0
    app.kubernetes.io/name: scm-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: scm-service
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: scm-service
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      containers:
        - name: scm
          image: docker.io/harness/ci-scm-signed:release-65
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          ports:
          - name: scm
            containerPort: 8091
            protocol: TCP
          resources:
            limits:
              cpu: 0.1
              memory: 512Mi
            requests:
              cpu: 0.1
              memory: 512Mi
          readinessProbe:
            exec:
              command: ["/grpc_health_probe", "-addr=:8091"]
            initialDelaySeconds: 5
          livenessProbe:
            exec:
              command: ["/grpc_health_probe", "-addr=:8091"]
            initialDelaySeconds: 10
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - scm-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/template-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: template-service
  namespace: harness-1
  labels:
    helm.sh/chart: template-service-0.2.8
    app.kubernetes.io/name: template-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  progressDeadlineSeconds: 300
  selector:
    matchLabels:
      app.kubernetes.io/name: template-service
      app.kubernetes.io/instance: chart
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: template-service
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-pipeline-service
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=pipeline-service"
      containers:
      - name: template-service
        image:  docker.io/harness/template-service-signed:76019
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
          - name: grpc-template
            containerPort: 15011
            protocol: "TCP"
          - name: http-template
            containerPort: 15002
            protocol: "TCP"
          - name: grpc-gitsync
            containerPort: 16002
            protocol: "TCP"
        resources:
            limits:
              cpu: 1
              memory: 1400Mi
            requests:
              cpu: 1
              memory: 1400Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name : TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: timescaledbPostgresPassword
          - name: MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/template-harness?replicaSet=rs0&authSource=admin'
        envFrom:
        - configMapRef:
            name: template-service
        readinessProbe:
          httpGet:
            path: /api/health
            port: 15002
          initialDelaySeconds: 60
          timeoutSeconds: 5
          periodSeconds: 5
          failureThreshold: 8
        livenessProbe:
          httpGet:
            path: /api/health
            port: 15002
          initialDelaySeconds: 40
          timeoutSeconds: 5
          periodSeconds: 10
          failureThreshold: 20
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - template-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/platform/charts/ti-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ti-service
  namespace: harness-1
  labels:
    helm.sh/chart: ti-service-0.2.3
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ti-service
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ti-service
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      containers:
        - name: ti-service
          image: docker.io/harness/ti-service-signed:release-77
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          livenessProbe:
            httpGet:
              path: /healthz
              port: http-ti-service
            initialDelaySeconds: 10
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /healthz
              port: http-ti-service
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            limits:
              cpu: 1
              memory: 3072Mi
            requests:
              cpu: 1
              memory: 3072Mi
          ports:
          - name: http-ti-service
            containerPort: 8078
            protocol: "TCP"
          env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name: TI_SERVICE_MONGODB_CONN_STR
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/ti-harness?replicaSet=rs0&authSource=admin'
          - name: TI_SERVICE_TIMESCALE_USERNAME
            value: "postgres"
          - name: TI_SERVICE_TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: timescaledbPostgresPassword
          - name: TSDB_URL
            value: 'postgres://postgres:$(TI_SERVICE_TIMESCALE_PASSWORD)@timescaledb-single-chart.harness-1:5432/harnessti'
          envFrom:
          - configMapRef:
              name: ti-service
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ti-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/sto/charts/sto-core/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sto-core
  namespace: harness-1
  labels:
    helm.sh/chart: sto-core-0.2.4
    app.kubernetes.io/name: sto-core
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: sto-core
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: sto-core
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      initContainers:
      - name: wait-for-postgres
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=postgres"
      containers:
        - name: sto-core
          securityContext:
            {}
          image: docker.io/harness/stocore-signed:v2-alpha15
          imagePullPolicy: IfNotPresent
          env:
          - name: APP_HARNESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: stoAppHarnessToken
          - name: APP_AUDIT_JWT_SECRET
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: stoAppAuditJWTSecret
          - name: DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: postgres
                key: postgres-password
          - name: APP_DATABASE_DATASOURCE
            value: "postgres://postgres:$(DB_PASSWORD)@postgres:5432"
          - name: APP_DB_MIGRATION_DATASOURCE
            value: "postgres://postgres:$(DB_PASSWORD)@postgres:5432"
          envFrom:
          - configMapRef:
              name: sto-core
          ports:
            - name: http
              containerPort: 4000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /api/v2/system/health
              port: http
            initialDelaySeconds: 3
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/v2/system/health
              port: http
            initialDelaySeconds: 3
            periodSeconds: 30
          resources:
            requests:
              cpu: 500m
              memory: 500Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sto-core
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/sto/charts/sto-manager/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sto-manager
  namespace: harness-1
  labels:
    helm.sh/chart: sto-manager-0.2.7
    app.kubernetes.io/name: sto-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: sto-manager
      app.kubernetes.io/instance: chart
  template:
    metadata:
      labels:
        app.kubernetes.io/name: sto-manager
        app.kubernetes.io/instance: chart
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-pipeline-service
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=pipeline-service"
      containers:
        - name: sto-manager
          securityContext:
            {}
          image: docker.io/harness/stomanager-signed:76000-000
          imagePullPolicy: IfNotPresent
          envFrom:
            - configMapRef:
                name: sto-manager
          ports:
            - name: http
              containerPort: 7090
              protocol: TCP
            - name: sto-mgr-grpc
              containerPort: 9979
              protocol: TCP
          env:
            - name: STO_SERVICE_GLOBAL_TOKEN
              value: token
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALEDB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: TIMESCALE_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.harness-1:5432/harness'
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/harness?replicaSet=rs0&authSource=admin'
            - name: STOMANAGER_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/harness-sto?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
            - name: NOTIFICATION_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/notifications?replicaSet=rs0&authSource=admin'
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 60
            timeoutSeconds: 5
            periodSeconds: 5
            failureThreshold: 8
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 40
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 20
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sto-manager
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness-prod/charts/ci/charts/ci-manager/templates/hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: ci-manager
  namespace: harness-1
  labels:
    helm.sh/chart: ci-manager-0.2.7
    app.kubernetes.io/name: ci-manager
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ci-manager
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage : 80
---
# Source: harness-prod/charts/platform/charts/delegate-proxy/templates/hpa.yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: delegate-proxy
  namespace: harness-1
  labels:
    helm.sh/chart: delegate-proxy-0.2.0
    app.kubernetes.io/name: delegate-proxy
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: delegate-proxy
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage : 80
---
# Source: harness-prod/charts/platform/charts/mongodb/templates/replicaset/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb-replicaset-chart
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  serviceName: mongodb-replicaset-chart
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/instance: chart
      app.kubernetes.io/component: mongodb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mongodb
        helm.sh/chart: mongodb-13.1.2
        app.kubernetes.io/instance: chart
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: mongodb
        app: mongodb-replicaset
    spec:

      serviceAccountName: mongodb-replicaset-chart
      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: mongodb
                    app.kubernetes.io/instance: chart
                    app.kubernetes.io/component: mongodb
                namespaces:
                  - "harness-1"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
        sysctls: []

      containers:
        - name: mongodb
          image: docker.io/bitnami/mongodb:4.2.19
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MY_POD_HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: K8S_SERVICE_NAME
              value: "mongodb-replicaset-chart"
            - name: MONGODB_INITIAL_PRIMARY_HOST
              value: mongodb-replicaset-chart-0.$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local
            - name: MONGODB_REPLICA_SET_NAME
              value: "rs0"
            - name: MONGODB_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: MONGODB_ROOT_USER
              value: "admin"
            - name: MONGODB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGODB_REPLICA_SET_KEY
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-replica-set-key
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: MONGODB_SYSTEM_LOG_VERBOSITY
              value: "0"
            - name: MONGODB_DISABLE_SYSTEM_LOG
              value: "no"
            - name: MONGODB_DISABLE_JAVASCRIPT
              value: "no"
            - name: MONGODB_ENABLE_JOURNAL
              value: "yes"
            - name: MONGODB_PORT_NUMBER
              value: "27017"
            - name: MONGODB_ENABLE_IPV6
              value: "no"
            - name: MONGODB_ENABLE_DIRECTORY_PER_DB
              value: "no"
          ports:
            - name: mongodb
              containerPort: 27017
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10
            exec:
              command:
                - /bitnami/scripts/ping-mongodb.sh
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bitnami/scripts/readiness-probe.sh
          resources:
            limits:
              cpu: 4
              memory: 8192Mi
            requests:
              cpu: 4
              memory: 8192Mi
          volumeMounts:
            - name: datadir
              mountPath: /bitnami/mongodb
              subPath:
            - name: common-scripts
              mountPath: /bitnami/scripts
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh

      volumes:
        - name: common-scripts
          configMap:
            name: mongodb-replicaset-chart-common-scripts
            defaultMode: 0550
        - name: scripts
          configMap:
            name: mongodb-replicaset-chart-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: datadir
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "200Gi"
---
# Source: harness-prod/charts/platform/charts/redis/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-sentinel-harness-server
  namespace: harness-1
  labels:
    redis-sentinel-harness: replica
    app: redis-sentinel
    helm.sh/chart: redis-0.2.0
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      release: redis-ha
      app: redis-sentinel
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: chart
  serviceName: redis-sentinel-harness
  replicas: 3
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/init-config: 0fb17318c62ec6e7f89897284e4d3edf7b1a0fc156692f8b15db8fd976df2e48
      labels:
        release: redis-ha
        app: redis-sentinel
        redis-sentinel-harness: replica
        app.kubernetes.io/name: redis
        app.kubernetes.io/instance: chart
    spec:
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      serviceAccountName: harness-default
      initContainers:
      - name: config-init
        image: docker.io/harness/redis:6.2.5-alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        command:
        - sh
        args:
        - /readonly-config/init.sh
        env:
        - name: SENTINEL_ID_0
          value: ed89975e57ea5a6848fe664901b11b5e6b22b537

        - name: SENTINEL_ID_1
          value: 4abd57ef009b0a1595767af80ef815e3438ae7e9

        - name: SENTINEL_ID_2
          value: e21a3c2cf7abdfc6e8d921901addb1bf86fe32a0
        volumeMounts:
        - name: config
          mountPath: /readonly-config
          readOnly: true
        - name: data
          mountPath: /data
      containers:
      - name: redis
        image: docker.io/harness/redis:6.2.5-alpine
        imagePullPolicy: IfNotPresent
        command:
        - redis-server
        args:
        - /data/conf/redis.conf
        env:
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 15
        resources:
            limits:
              cpu: 0.1
              memory: 200Mi
            requests:
              cpu: 0.1
              memory: 200Mi
        ports:
        - name: redis
          containerPort: 6379
        volumeMounts:
        - mountPath: /data
          name: data
      - name: sentinel
        image: docker.io/harness/redis:6.2.5-alpine
        imagePullPolicy: IfNotPresent
        command:
          - redis-sentinel
        args:
          - /data/conf/sentinel.conf
        livenessProbe:
          tcpSocket:
            port: 26379
          initialDelaySeconds: 15
        resources:
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
        ports:
          - name: sentinel
            containerPort: 26379
        volumeMounts:
        - mountPath: /data
          name: data
      volumes:
      - name: config
        configMap:
          name: redis-sentinel-harness-configmap
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: redis-sentinel
                  release: redis-ha
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app:  redis-sentinel
                    release: redis-ha
                    redis-sentinel-harness: replica
                topologyKey: failure-domain.beta.kubernetes.io/zone
  volumeClaimTemplates:
  - metadata:
      name: data
      annotations:
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 10Gi
---
# Source: harness-prod/charts/platform/charts/timescaledb/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations: {}
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-single-0.5.5
    release: timescaledb-single-chart
    heritage: Tiller
    cluster-name: timescaledb-single-chart
  name: timescaledb-single-chart
  namespace: harness-1
spec:
  serviceName: timescaledb-single-chart
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: timescaledb-single-chart
      release: timescaledb-single-chart
  template:
    metadata:
      name: timescaledb-single-chart
      labels:
        app: timescaledb-single-chart
        release: timescaledb-single-chart
        cluster-name: timescaledb-single-chart
      annotations:
        backup.velero.io/backup-volumes: storage-volume
    spec:
      serviceAccountName: harness-default
      securityContext:
        # The postgres user inside the TimescaleDB image has uid=1000.
        # This configuration ensures the permissions of the mounts are suitable
        fsGroup: 1000
      initContainers:
      # Issuing the final checkpoints on a busy database may take considerable time.
      # Unfinished checkpoints will require more time during startup, so the tradeoff
      # here is time spent in shutdown/time spent in startup.
      # We choose shutdown here, especially as during the largest part of the shutdown
      # we can still serve clients.
      terminationGracePeriodSeconds: 600
      containers:
      - name: timescaledb
        image: docker.io/timescale/timescaledb-ha:pg13-ts2.6-oss-latest
        securityContext:
          runAsUser: 1000
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - psql
              - -X
              - --file
              - "/etc/timescaledb/scripts/lifecycle_preStop.psql"
        # When reusing an already existing volume it sometimes happens that the permissions
        # of the PGDATA and/or wal directory are incorrect. To guard against this, we always correctly
        # set the permissons of these directories before we hand over to Patroni.
        # We also create all the tablespaces that are defined, to ensure a smooth restore/recovery on a
        # pristine set of Volumes.
        # As PostgreSQL requires to have full control over the permissions of the tablespace directories,
        # we create a subdirectory "data" in every tablespace mountpoint. The full path of every tablespace
        # therefore always ends on "/data".
        # By creating a .pgpass file in the $HOME directory, we expose the superuser password
        # to processes that may not have it in their environment (like the preStop lifecycle hook).
        # To ensure Patroni will not mingle with this file, we give Patroni its own pgpass file.
        # As these files are in the $HOME directory, they are only available to *this* container,
        # and they are ephemeral.
        command:
          - /bin/bash
          - "-c"
          - |

            install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/data" "/var/lib/postgresql/wal/pg_wal" || exit 1
            TABLESPACES=""
            for tablespace in ; do
              install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/tablespaces/${tablespace}/data"
            done

            # Environment variables can be read by regular users of PostgreSQL. Especially in a Kubernetes
            # context it is likely that some secrets are part of those variables.
            # To ensure we expose as little as possible to the underlying PostgreSQL instance, we have a list
            # of allowed environment variable patterns to retain.
            #
            # We need the KUBERNETES_ environment variables for the native Kubernetes support of Patroni to work.
            #
            # NB: Patroni will remove all PATRONI_.* environment variables before starting PostgreSQL

            # We store the current environment, as initscripts, callbacks, archive_commands etc. may require
            # to have the environment available to them
            set -o posix
            export -p > "${HOME}/.pod_environment"
            export -p | grep PGBACKREST > "${HOME}/.pgbackrest_environment"

            for UNKNOWNVAR in $(env | awk -F '=' '!/^(PATRONI_.*|HOME|PGDATA|PGHOST|LC_.*|LANG|PATH|KUBERNETES_SERVICE_.*)=/ {print $1}')
            do
                unset "${UNKNOWNVAR}"
            done

            echo "*:*:*:postgres:${PATRONI_SUPERUSER_PASSWORD}" >> ${HOME}/.pgpass
            chmod 0600 ${HOME}/.pgpass

            export PATRONI_POSTGRESQL_PGPASS="${HOME}/.pgpass.patroni"

            exec patroni /etc/timescaledb/patroni.yaml
        env:
        # We use mixed case environment variables for Patroni User management,
        # as the variable themselves are documented to be PATRONI_<username>_OPTIONS.
        # Where possible, we want to have lowercase usernames in PostgreSQL as more complex postgres usernames
        # requiring quoting to be done in certain contexts, which many tools do not do correctly, or even at all.
        # https://patroni.readthedocs.io/en/latest/ENVIRONMENT.html#bootstrap-configuration
        - name: PATRONI_admin_OPTIONS
          value: createrole,createdb
        - name: PATRONI_admin_PASSWORD
          valueFrom:
            secretKeyRef:
              name: harness-secrets
              key: timescaledbAdminPassword
        - name: PATRONI_postgres_PASSWORD
          valueFrom:
            secretKeyRef:
              name: harness-secrets
              key: timescaledbPostgresPassword
        - name: PATRONI_standby_PASSWORD
          valueFrom:
            secretKeyRef:
              name: harness-secrets
              key: timescaledbStandbyPassword
        - name: PATRONI_REPLICATION_PASSWORD
          valueFrom:
            secretKeyRef:
              name: harness-secrets
              key: timescaledbStandbyPassword
        - name: PATRONI_SUPERUSER_PASSWORD
          valueFrom:
            secretKeyRef:
              name: harness-secrets
              key: timescaledbPostgresPassword
        - name: PATRONI_REPLICATION_USERNAME
          value: standby
        # To specify the PostgreSQL and Rest API connect addresses we need
        # the PATRONI_KUBERNETES_POD_IP to be available as a bash variable, so we can compose an
        # IP:PORT address later on
        - name: PATRONI_KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PATRONI_POSTGRESQL_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):5432"
        - name: PATRONI_RESTAPI_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):8008"
        - name: PATRONI_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PATRONI_POSTGRESQL_DATA_DIR
          value: "/var/lib/postgresql/data"
        - name: PATRONI_KUBERNETES_NAMESPACE
          value: 'harness-1'
        - name: PATRONI_KUBERNETES_LABELS
          value: "{app: timescaledb-single-chart, cluster-name: timescaledb-single-chart, release: timescaledb-single-chart}"
        - name: PATRONI_SCOPE
          value: timescaledb-single-chart
        - name: PGBACKREST_CONFIG
          value: /etc/pgbackrest/pgbackrest.conf
        # PGDATA and PGHOST are not required to let Patroni/PostgreSQL run correctly,
        # but for interactive sessions, callbacks and PostgreSQL tools they should be correct.
        - name: PGDATA
          value: "$(PATRONI_POSTGRESQL_DATA_DIR)"
        - name: PGHOST
          value: "/var/run/postgresql"
          # pgBackRest is also called using the archive_command if the backup is enabled.
          # this script will also need access to the environment variables specified for
          # the backup. This can be removed once we do not directly invoke pgBackRest
          # from inside the TimescaleDB container anymore
        ports:
        - containerPort: 8008
        - containerPort: 5432
        readinessProbe:
          exec:
            command:
              - pg_isready
              - -h
              - /var/run/postgresql
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        volumeMounts:
        - name: storage-volume
          mountPath: "/var/lib/postgresql"
          subPath: ""
        - mountPath: /etc/timescaledb/patroni.yaml
          subPath: patroni.yaml
          name: patroni-config
          readOnly: true
        - mountPath: /etc/timescaledb/scripts
          name: timescaledb-scripts
          readOnly: true
        - mountPath: /etc/certificate
          name: certificate
          readOnly: true
        - name: socket-directory
          mountPath: /var/run/postgresql
        - name: patroni-callbacks
          mountPath: /etc/timescaledb/callbacks
          readOnly: true
        resources:
          limits:
            cpu: "1"
            memory: "2048Mi"
          requests:
            cpu: "1"
            memory: "2048Mi"
# end comment
      volumes:
      - name: socket-directory
        emptyDir: {}
      - name: patroni-config
        configMap:
          name: timescaledb-single-chart-patroni
      - name: timescaledb-scripts
        configMap:
          name: timescaledb-single-chart-scripts
          defaultMode: 488 # 0750 permissions
      - name: patroni-callbacks
        configMap:
          name: timescaledb-init
          defaultMode: 488 # 0750 permissions
      - name: certificate
        secret:
          secretName: timescaledb-single-chart-certificate
          defaultMode: 416 # 0640 permissions
  volumeClaimTemplates:
    - metadata:
        name: storage-volume
        annotations:
        labels:
          app: timescaledb-single-chart
          release: timescaledb-single-chart
          heritage: Tiller
          cluster-name: timescaledb-single-chart
          purpose: data-directory
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
---
# Source: harness-prod/charts/sto/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: "harness-1"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: chart
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
    app: postgres
  annotations:
spec:
  replicas: 1
  serviceName: postgres-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: chart
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: postgres
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-11.6.16
        app.kubernetes.io/instance: chart
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
        app: postgres
      annotations:
    spec:
      serviceAccountName: default

      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/instance: chart
                    app.kubernetes.io/component: primary
                namespaces:
                  - "harness-1"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      initContainers:
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:14.4.0-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres
                  key: postgres-password
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e

                - |
                  exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: harness-prod/charts/platform/charts/ti-service/templates/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: a-ti-migration-release-77
  namespace: harness-1
  annotations:
  labels:
    helm.sh/chart: ti-service-0.2.3
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: chart
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: ti-service
        app.kubernetes.io/instance: chart
    spec:
      containers:
      - name: migrate
        image: docker.io/harness/ti-service-signed:release-77
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c", "sleep 300 && /opt/harness/migrate.sh" ]
        resources:
          limits:
            cpu: 1
            memory: 3072Mi
          requests:
            cpu: 1
            memory: 3072Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name: TI_SERVICE_MONGODB_CONN_STR
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.harness-1.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.harness-1.svc:27017/ti-harness?replicaSet=rs0&authSource=admin'
          - name: TI_SERVICE_TIMESCALE_USERNAME
            value: "postgres"
          - name: TI_SERVICE_TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: "timescaledbPostgresPassword"
          - name: TSDB_URL
            value: 'postgres://postgres:$(TI_SERVICE_TIMESCALE_PASSWORD)@timescaledb-single-chart.harness-1:5432/harnessti'
      restartPolicy: Never
---
# Source: harness-prod/templates/gateway.yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: public
  namespace: istio-system
spec:
  selector:
     istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - "*"
    tls:
      credentialName: harness-wildcard-istio
      minProtocolVersion: TLSV1_2
      mode: SIMPLE
---
# Source: harness-prod/templates/virtualservice.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: harness-vs
  namespace: harness-1
  labels:
    app.kubernetes.io/instance: chart
spec:
  gateways:
    - istio-system/public
  hosts:
    - helm-test.qa.harness.io
  tcp:
  - match:
    - port: 9000
    route:
    - destination:
        host: log-service-minio
        port:
          number: 9000
  - match:
    - port: 9879
    route:
    - destination:
        host: harness-manager
        port:
          number: 9879
  http:
  - name: ng-manager
    match:
    - uri:
        prefix: /ng/api/
    - uri:
        prefix: /ng/api
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 7090
        host: ng-manager
  - name: sto
    match:
    - uri:
        prefix: /sto/
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 4000
        host: sto-core
  - name: sto-manager
    match:
    - uri:
        prefix: /sto-manager/
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 7090
        host: sto-manager
  - name: next-gen-ui
    match:
    - uri:
        prefix: /ng/
    - uri:
        prefix: /ng
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 80
        host: next-gen-ui
  - name: ci-manager
    match:
    - uri:
        prefix: /ci/
    - uri:
        prefix: /ci
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 7090
        host: ci-manager

  - name: cv-nextgen
    match:
    - uri:
        prefix: /cv/
    - uri:
        prefix: /cv
    route:
    - destination:
        port:
          number: 6060
        host: cv-nextgen

  - name: minio-ui
    match:
    - uri:
        prefix: "/minio"
    route:
    - destination:
        port:
          number: 9000
        host: minio

  - name: log-serivce-minio
    match:
    - uri:
        prefix: "/logs/"
    route:
    - destination:
        port:
          number: 9000
        host: minio

  - name: log-service
    match:
    - uri:
        prefix: /log-service/
    - uri:
        prefix: /log-service
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 8079
        host: log-service

  # - name: gateway
  #   match:
  #   - uri:
  #       regex: "/gateway(/|$)(.*)"
  #   route:
  #   - destination:
  #       port:
  #         number: 80
  #       host: gateway

  - name: gateway
    match:
    - uri:
        prefix: /gateway/
    - uri:
        prefix: /gateway
    rewrite:
      uri: /
    route:
    - destination:
        host: gateway
        port:
          number: 80

  - name: access-control
    match:
    - uri:
        prefix: /authz/
    - uri:
        prefix: /authz
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 9006
        host: access-control

  - name: ti-service
    match:
    - uri:
        prefix: /ti-service/
    - uri:
        prefix: /ti-service
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 8078
        host: ti-service

  - name: template-service
    match:
    - uri:
        prefix: /template/
    - uri:
        prefix: /template
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 15002
        host: template-service

  - name: delegate-proxy
    match:
    - uri:
        prefix: "/storage"
    route:
    - destination:
        port:
          number: 80
        host: delegate-proxy


  - name: pipeline-service
    match:
    - uri:
        prefix: /pipeline/
    - uri:
        prefix: /pipeline
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 12001
        host: pipeline-service

  - name: notification-service
    match:
    - uri:
        prefix: /notifications/
    - uri:
        prefix: /notifications
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 9005
        host: platform-service

  - name: audit-service
    match:
    - uri:
        prefix: /audit/
    - uri:
        prefix: /audit
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 9005
        host: platform-service

  - name: resourcegroup-service
    match:
    - uri:
        prefix: /resourcegroup/
    - uri:
        prefix: /resourcegroup
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 9005
        host: platform-service

  - name: verification-svc
    match:
    - uri:
        #regex: "/verification(/|$)(.*)"
        prefix: "/verification"
    route:
    - destination:
        port:
          number: 7070
        host: verification-svc

  - name: harness-manager-api
    match:
    - uri:
        #regex: "/api(/|$)(.*)"
        prefix: "/api"
    route:
    - destination:
        port:
          number: 9090
        host: harness-manager

  - name: harness-manager-stream
    match:
    - uri:
        #regex: "/stream(/|$)(.*)"
        prefix: "/stream"
    route:
    - destination:
        port:
          number: 9090
        host: harness-manager

  - name: grpc-manager
    match:
    - uri:
        prefix: "/"
      port: 9879
    route:
    - destination:
        port:
          number: 9879
        host: harness-manager

  - name: ng-auth-ui
    match:
    - uri:
        prefix: "/auth"
    rewrite:
      uri: /
    route:
    - destination:
        port:
          number: 80
        host: ng-auth-ui

  - name: harness-ui
    match:
    - uri:
        prefix: "/"
    route:
    - destination:
        port:
          number: 80
        host: next-gen-ui

  - name: catchall-redirect
    match:
    - uri:
        regex: ".*"
    redirect:
      uri: "/"
