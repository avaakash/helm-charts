chaos:
  chaos-manager:
    replicaCount: 1
    resources:
      limits:
        memory: 256Mi
      requests:
        cpu: 128m
        memory: 256Mi
  chaos-web:
    replicaCount: 1
    resources:
      limits:
        memory: 128Mi
      requests:
        cpu: 64m
        memory: 128Mi 
  chaos-k8s-ifs:
    replicaCount: 1
    resources:
      limits:
        memory: 128Mi
      requests:
        cpu: 64m
        memory: 128Mi 
  chaos-linux-ifc:
    replicaCount: 1
    resources:
      limits:
        memory: 128Mi
      requests:
        cpu: 64m
        memory: 128Mi 
  chaos-linux-ifs:
    replicaCount: 1
    resources:
      limits:
        memory: 128Mi
      requests:
        cpu: 64m
        memory: 128Mi
ci:
  ci-manager:
    autoscaling:
      enabled: false
    java:
      memory: "512m"
    replicaCount: 1
    resources:
      limits:
        memory: 1280Mi
      requests:
        cpu: 256m
        memory: 1280Mi
  ti-service:
    autoscaling:
      enabled: false
    jobresources:
      limits:
        memory: 256Mi
      requests:
        cpu: 32m
        memory: 256Mi
    replicaCount: 1
    resources:
      limits:
        memory: 1400Mi
      requests:
        cpu: 0.5
        memory: 1400Mi
platform:
  access-control:
    autoscaling:
      enabled: false
    java:
      memory: 384m
    replicaCount: 1
    resources:
      limits:
        memory: 768Mi
      requests:
        cpu: 128m
        memory: 768Mi
  bootstrap:
    database:
      clickhouse:
        enabled: false
      minio:
        defaultBuckets: "logs"
        fullnameOverride: "minio"
        mode: standalone
        affinity: {}
        nodeSelector: {}
        tolerations: []
        podAnnotations: {}
        persistence:
          size: 10Gi
      mongodb:
        arbiter:
          affinity: {}
          nodeSelector: {}
          tolerations: []
          annotations: {}
          podAnnotations: {}
        extraFlags:
          - "--wiredTigerCacheSizeGB=0.5"
        persistence:
          size: 20Gi
        replicaCount: 1
        resources:
          limits:
            memory: 1024Mi
          requests:
            cpu: 500m
            memory: 1024Mi
      postgresql:
        auth:
          existingSecret: "postgres"
        primary:
          affinity: {}
          nodeSelector: {}
          tolerations: []
          podAnnotations: {}
          persistence:
            size: 8Gi
          resources:
            limits:
              memory: 4Gi
            requests:
              cpu: 2
              memory: 4Gi
      redis:
        affinity: {}
        nodeSelector: {}
        tolerations: []
        podAnnotations: {}
        redis:
          resources:
            limits:
              memory: 256Mi
            requests:
              cpu: 1
              memory: 256Mi
        replicaCount: 3
        sentinel:
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
        useAntiAffinity: false
        volumeClaimTemplate:
          resources:
            requests:
              storage: 10Gi
      timescaledb:
        autoscaling:
          enabled: false
        affinity: {}
        nodeSelector: {}
        tolerations: []
        podAnnotations: {}
        replicaCount: 1
        # Increasing persistentVolumes size while upgrading a helm deployment
        # will fail because these values are immutable for a statefulset.
        # To increase the size, it requires to delete the statefulset
        # and then upgrade. For more information follow this document
        # https://developer.harness.io/docs/self-managed-enterprise-edition/advanced-configurations/increase-pv-size-statefulsets
        persistentVolumes:
          data:
            size: 100Gi
          wal:
            size: 1Gi
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 0.3
            memory: 512Mi
    harness-secrets:
      enabled: true
    networking:
      defaultbackend:
        # -- Create will deploy a default backend into your cluster
        create: false
      nginx:
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 32m
            memory: 512Mi
        affinity: {}
        controller:
          # -- annotations to be addded to ingress Controller
          annotations: {}
        # -- Create Nginx Controller.  True will deploy a controller into your cluster
        create: false
        healthNodePort: ""
        healthPort: ""
        httpNodePort: ""
        httpsNodePort: ""
        loadBalancerEnabled: false
        loadBalancerIP: '0.0.0.0'
        nodeSelector: {}
        tolerations: []
  change-data-capture:
    appLogLevel: INFO
    autoscaling:
      enabled: false
    java:
      memory: 2048
    replicaCount: 1
    resources:
      limits:
        memory: 2880Mi
      requests:
        cpu: 1
        memory: 2880Mi
  delegate-proxy:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 50Mi
      requests:
        cpu: 50m
        memory: 50Mi
  gateway:
    autoscaling:
      enabled: false
    java:
      memory: 768
    replicaCount: 1
    resources:
      limits:
        memory: 1280Mi
      requests:
        cpu: 0.1
        memory: 1280Mi
  harness-manager:
    autoscaling:
      enabled: false
    external_graphql_rate_limit: "500"
    java:
      memory: "1024"
    replicaCount: 1
    resources:
      limits:
        memory: 2048Mi
      requests:
        cpu: 0.5
        memory: 2048Mi
  log-service:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 256Mi
      requests:
        cpu: 64m
        memory: 256Mi
  ng-dashboard-aggregator:
    autoscaling:
      enabled: false
    replicaCount: 1
    java:
      memory: "1024"
    resources:
      limits:
        memory: 2048Mi
      requests:
        cpu: 64m
        memory: 2048Mi      
  looker:
    config:
      # -- clickhouse database name
      clickhouseDatabase: 'ccm'
      # -- Leave empty if not deploying Clickhouse.
      # -- clickhouse hostname
      clickhouseHost: 'clickhouse'
      # -- clickhouse port
      clickhousePort: '8123'
      # -- clickhouse user
      clickhouseUser: 'default'
    ingress:
      hosts: []
      tls:
        secretName: ''
    resources:
      limits:
        memory: 8Gi
      requests:
        cpu: 2
        memory: 8Gi
    secrets:
      # -- Required: Looker license key
      lookerLicenseKey: ""
  migrator:
    autoscaling:
      enabled: false
    java:
      memory: "2048"
    replicaCount: 1
    resources:
      limits:
        memory: 3000Mi
      requests:
        cpu: 0.5
        memory: 3000Mi
  next-gen-ui:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 100Mi
      requests:
        cpu: 0.1
        memory: 100Mi
  ng-auth-ui:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 64Mi
      requests:
        cpu: 32m
        memory: 64Mi
  ng-custom-dashboards:
    config:
      # -- Required: domain name of your looker instance, this must be accessible by users in your organisation
      lookerPubDomain: ''
      # -- Required: HTTP scheme used, either http or https
      lookerPubScheme: 'https'
    memory: 384
    resources:
      limits:
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 512Mi
  ng-manager:
    autoscaling:
      enabled: false
    java:
      memory: "1024m"
    replicaCount: 1
    resources:
      limits:
        memory: 2048Mi
      requests:
        cpu: 256m
        memory: 2048Mi
  pipeline-service:
    autoscaling:
      enabled: false
    java:
      memory: "768m"
    replicaCount: 1
    resources:
      limits:
        memory: 2048Mi
      requests:
        cpu: 512m
        memory: 2048Mi
  platform-service:
    autoscaling:
      enabled: false
    java:
      memory: "1024m"
    replicaCount: 1
    resources:
      limits:
        memory: 1400Mi
      requests:
        cpu: 256m
        memory: 1400Mi
  scm-service:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 256Mi
      requests:
        cpu: 64m
        memory: 256Mi
  template-service:
    autoscaling:
      enabled: false
    java:
      memory: "1024m"
    replicaCount: 1
    resources:
      limits:
        memory: 1500Mi
      requests:
        cpu: 256m
        memory: 1500Mi
  ui:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 200Mi
      requests:
        cpu: 0.2
        memory: 200Mi
sto:
  sto-core:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 256Mi
      requests:
        cpu: 128m
        memory: 256Mi
  sto-manager:
    autoscaling:
      enabled: false
    replicaCount: 1
    resources:
      limits:
        memory: 1.5Gi
      requests:
        cpu: 256m
        memory: 1.5Gi
