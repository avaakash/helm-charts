---
# Source: harness/charts/ccm/charts/nextgen-ce/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: nextgen-ce
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: nextgen-ce
---
# Source: harness/charts/platform/charts/access-control/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: access-control
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: access-control
---
# Source: harness/charts/platform/charts/cv-nextgen/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cv-nextgen
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: cv-nextgen
---
# Source: harness/charts/platform/charts/delegate-proxy/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: delegate-proxy
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: delegate-proxy
---
# Source: harness/charts/platform/charts/gateway/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: gateway
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: gateway
---
# Source: harness/charts/platform/charts/harness-manager/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: harness-manager
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: harness-manager
---
# Source: harness/charts/platform/charts/le-nextgen/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: le-nextgen
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: le-nextgen
---
# Source: harness/charts/platform/charts/next-gen-ui/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: next-gen-ui
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: next-gen-ui
---
# Source: harness/charts/platform/charts/ng-auth-ui/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ng-auth-ui
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: ng-auth-ui
---
# Source: harness/charts/platform/charts/ng-manager/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ng-manager
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: ng-manager
---
# Source: harness/charts/platform/charts/pipeline-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: pipeline-service
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: pipeline-service
---
# Source: harness/charts/platform/charts/platform-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: platform-service
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: platform-service
---
# Source: harness/charts/platform/charts/scm-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: scm-service
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: scm-service
---
# Source: harness/charts/platform/charts/template-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: template-service
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: template-service
---
# Source: harness/charts/platform/charts/ti-service/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ti-service
  namespace: default
spec:
  minAvailable: "50%"
  selector:
   matchLabels:
    app: ti-service
---
# Source: harness/charts/platform/charts/harness-manager/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: harness-default
  namespace: default
  annotations: {}
---
# Source: harness/charts/platform/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: minio
  namespace: "default"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
secrets:
  - name: minio
---
# Source: harness/charts/platform/charts/mongodb/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mongodb-replicaset-chart
  namespace: "default"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
secrets:
  - name: mongodb-replicaset-chart
automountServiceAccountToken: true
---
# Source: harness/charts/platform/charts/timescaledb/templates/serviceaccount-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-timescaledb
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: rbac
---
# Source: harness/charts/infra/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app: postgres
type: Opaque
data:
  postgres-password: "TDBmRDVmQ29WYg=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: harness/charts/platform/charts/harness-secrets/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: harness-secrets
  namespace: default
  labels:
    app.kubernetes.io/name: harness-secrets
    helm.sh/chart: harness-secrets-0.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations: {}
type: Opaque
data:
    mongodbUsername: YWRtaW4=
    mongodbPassword: "MEE4VzB0MVo2bEJNMktkdQ=="
    postgresdbAdminPassword: "UDJtV1N6YUxrUkNDYWdJMQ=="
    stoAppHarnessToken:  "T2J2QXkzNTJPUm9NRWV6QQ=="
    stoAppAuditJWTSecret:  "Tk1aVmFXVWh3SDV6RjRIbA=="
    timescaledbAdminPassword: "V3I1V2lqVHh5anVHSzdnSg=="
    timescaledbPostgresPassword: "MUNIcFpvOUdpa0FrTzROMA=="
    timescaledbStandbyPassword:  "YkN0ajBDR0FHVEZEOEJ3dg=="
    PATRONI_SUPERUSER_PASSWORD: "MUNIcFpvOUdpa0FrTzROMA=="
    PATRONI_REPLICATION_PASSWORD: "YkN0ajBDR0FHVEZEOEJ3dg=="
    PATRONI_admin_PASSWORD: "V3I1V2lqVHh5anVHSzdnSg=="
---
# Source: harness/charts/platform/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: minio
  namespace: "default"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  root-user: "YWRtaW4="
  root-password: "SHpkaG5lQ2RPMg=="
  key.json: ""
---
# Source: harness/charts/platform/charts/mongodb/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-replicaset-chart
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
type: Opaque
data:
  mongodb-root-password: "cmJaM2Flc1lNSg=="
  mongodb-replica-set-key: "bFVyTWlpeUtLcA=="
---
# Source: harness/charts/platform/charts/timescaledb/templates/secret-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescaledb-single-chart-pgbackrest"
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: pgbackrest
type: Opaque
stringData:
  PGBACKREST_REPO1_S3_BUCKET: ""
  PGBACKREST_REPO1_S3_ENDPOINT: s3.amazonaws.com
  PGBACKREST_REPO1_S3_KEY: ""
  PGBACKREST_REPO1_S3_KEY_SECRET: ""
  PGBACKREST_REPO1_S3_REGION: ""
...
---
# Source: harness/charts/ccm/charts/nextgen-ce/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nextgen-ce
  namespace: default
data:
  LOGGING_LEVEL: "INFO"
  MANAGER_TARGET: "harness-manager:9879"
  MANAGER_AUTHORITY: "harness-manager:9879"
  MANAGER_CLIENT_BASEURL: "http://harness-manager.default.svc.cluster.local:9090/"
  NG_MANAGER_CLIENT_BASEURL: "http://ng-manager.default.svc.cluster.local:7090/"
  CE_AWS_TEMPLATE_URL: "dummy" ## TODO
  MANAGER_URL: "http://harness-manager.default.svc.cluster.local:9090/"
  DEPLOY_MODE: "KUBERNETES_ONPREM"
  CE_NEXTGEN_PORT: "6340"
  EVENTS_FRAMEWORK_REDIS_URL: "redis://localhost:6379"
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: "harness-redis"
  EVENTS_FRAMEWORK_REDIS_SENTINELS: "redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379"
  EVENTS_FRAMEWORK_ENV_NAMESPACE: default
  EVENTS_FRAMEWORK_REDIS_SSL_ENABLED: "false"
  GOOGLE_CREDENTIALS_PATH: "/opt/harness/svc/ceng_gcp_credentials.json" ##TODO
  GOOGLE_APPLICATION_CREDENTIALS: "/opt/harness/svc/ceng_gcp_credentials.json" #TODO
  CE_GCP_CREDENTIALS_PATH: "/opt/harness/svc/ceng_gcp_credentials.json" #TODO
  STACK_DRIVER_LOGGING_ENABLED: "false"
  EVENTS_MONGO_INDEX_MANAGER_MODE: "MANUAL" ##TODO
  GCP_PROJECT_ID: "ce-prod-274307" ##TODO
  AZURE_APP_CLIENT_ID: "0211763d-24fb-4d63-865d-92f86f77e908" ##TODO
  AZURE_ENABLE_FILE_CHECK_AT_SOURCE: "true"
  AWS_ACCOUNT_ID: "891928451355" #TODO
  AWS_TEMPLATE_LINK: "https://continuous-efficiency-prod.s3.us-east-2.amazonaws.com/setup/ngv1/HarnessAWSTemplate.yaml" #TODO
  AWS_DESTINATION_BUCKET: "ce-customer-billing-data-prod" #TODO
  AWS_GOV_CLOUD_ACCOUNT_ID: "147449478367" #TODO
  AWS_GOV_CLOUD_TEMPLATE_LINK: "https://continuous-efficiency.s3.us-east-2.amazonaws.com/setup/v1/ng/HarnessAWSTemplate.yaml" #TODO
  AWS_GOV_CLOUD_REGION_NAME: "us-gov-west-1" #TODO
  SEGMENT_ENABLED: "false"
  AUDIT_ENABLED: "true"
  AUDIT_CLIENT_BASEURL: "http://platform-service..svc.cluster.local:9005/api/"
  NOTIFICATION_BASE_URL: "http://platform-service..svc.cluster.local:9005/api/"
  ACCESS_CONTROL_BASE_URL: "http://accesscontrol-service..svc.cluster.local:9006/api/"
  ACCESS_CONTROL_ENABLED: "true"
  MOCK_ACCESS_CONTROL_SERVICE: "false"
  MEMORY: "2500"
  ENABLE_APPDYNAMICS: "false"
  JWT_AUTH_SECRET: IC04LYMBf1lDP5oeY4hupxd4HJhLmN6azUku3xEbeE3SUx5G3ZYzhbiwVtK4i7AmqyU9OZkwB4v8E9qM
  NEXT_GEN_MANAGER_SECRET: IC04LYMBf1lDP5oeY4hupxd4HJhLmN6azUku3xEbeE3SUx5G3ZYzhbiwVtK4i7AmqyU9OZkwB4v8E9qM
  JWT_IDENTITY_SERVICE_SECRET: HVSKUYqD4e5Rxu12hFDdCJKGM64sxgEynvdDhaOHaTHhwwn0K4Ttr0uoOxSsEVYNrUU=
  NOTIFICATION_CLIENT_SECRET: IC04LYMBf1lDP5oeY4hupxd4HJhLmN6azUku3xEbeE3SUx5G3ZYzhbiwVtK4i7AmqyU9OZkwB4v8E9qM
  ACCESS_CONTROL_SECRET: IC04LYMBf1lDP5oeY4hupxd4HJhLmN6azUku3xEbeE3SUx5G3ZYzhbiwVtK4i7AmqyU9OZkwB4v8E9qM
---
# Source: harness/charts/platform/charts/access-control/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: access-control
  namespace: default
data:
  IDENTITY_SERVICE_SECRET: HVSKUYqD4e5Rxu12hFDdCJKGM64sxgEynvdDhaOHaTHhwwn0K4Ttr0uoOxSsEVYNrUU=

  DEPLOY_MODE: KUBERNETES_ONPREM
  LOGGING_LEVEL: "INFO"
  EVENTS_CONFIG_USE_SENTINEL: 'true'
  EVENTS_CONFIG_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'

  # lockRedisConfig
  LOCK_CONFIG_USE_SENTINEL: 'true'
  LOCK_CONFIG_SENTINEL_MASTER_NAME: 'harness-redis'
  LOCK_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'

  # iteratorsConfig
  RESOURCE_GROUP_ITERATOR_ENABLED: 'true'
  RESOURCE_GROUP_ITERATOR_INTERVAL: '3600'
  USER_GROUP_ITERATOR_ENABLED: 'true'
  USER_GROUP_ITERATOR_INTERVAL: '3600'
  USER_ITERATOR_ENABLED: 'true'
  USER_ITERATOR_INTERVAL: '3600'
  SERVICEACCOUNT_ITERATOR_ENABLED: 'true'
  SERVICEACCOUNT_ITERATOR_INTERVAL: '3600'
  SUPPORTPREFERENCE_ITERATOR_ENABLED: 'true'
  SUPPORTPREFERENCE_ITERATOR_INTERVAL: '600'
  SCOPE_ITERATOR_ENABLED: 'true'
  SCOPE_ITERATOR_INTERVAL: '3600'

  # resourceGroupClient:
  RESOURCE_GROUP_CLIENT_BASE_URL: 'http://platform-service.default.svc.cluster.local:9005/api/'

  # userClient:
  USER_CLIENT_BASE_URL: '/ng/api/'

  # userGroupClient
  USER_GROUP_CLIENT_BASE_URL: '/ng/api/'

  # serviceAccountClient
  SERVICEACCOUNT_CLIENT_BASE_URL : '/ng/api/'

  #accountClient
  ACCOUNT_CLIENT_BASE_URL: '/api/'
  FEATURE_FLAG_CLIENT_BASE_URL: '/api/'

  #projectClient
  PROJECT_CLIENT_BASE_URL: '/ng/api/'

  #organizationClient
  ORGANIZATION_CLIENT_BASE_URL: '/ng/api/'

  # aggreatorModuleConfig
  OFFSET_FLUSH_INTERVAL_MS: '10000'
  MONGODB_USER: admin
  MONGODB_SSL_ENABLED: 'false'
  AGGREGATOR_ENABLED: 'true'

  # auth
  ENABLE_AUTH: 'true'

  # preference
  ACCESS_CONTROL_PREFERENCE_ENABLED: 'true'

  #for notification
  NOTIFICATION_SLACK_WEBHOOK_URL: ""
  NOTIFICATION_ENVIRONMENT: ONPREM


  # for client
  ENABLE_ACCESS_CONTROL: 'false'
  ACCESS_CONTROL_SERVICE_BASE_URL: 'http://access-control.default.svc.cluster.local:9006/api/'
  ENABLE_AUDIT: 'true'
  AUDIT_CLIENT_BASE_URL: 'http://platform-service.default.svc.cluster.local:9005/api/'
  DISTRIBUTED_LOCK_IMPLEMENTATION: REDIS
  GOOGLE_APPLICATION_CREDENTIALS: /opt/harness/monitoring/stackdriver.json
  MEMORY: "512m"
---
# Source: harness/charts/platform/charts/change-data-capture/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: change-data-capture
  namespace: default
data:
  DEPLOY_MODE: "KUBERNETES"
  MEMORY: "2048"
  MONGO_TAG_NAME: "none"
  MONGO_TAG_VALUE: "none"
---
# Source: harness/charts/platform/charts/cv-nextgen/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cv-nextgen
  namespace: default
data:
  DEPLOY_MODE: KUBERNETES_ONPREM
  ENV: KUBERNETES_ONPREM
  LOGGING_LEVEL: INFO
  MANAGER_URL: http://harness-manager.default.svc.cluster.local:9090/
  NG_MANAGER_URL: http://ng-manager.default.svc.cluster.local:7090/
  MEMORY: "4096"
  STACK_DRIVER_LOGGING_ENABLED: "false"
  VERIFICATION_PORT: "6060"
  VERIFICATION_SERVICE_SECRET: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8
  NOTIFICATION_BASE_URL: http://platform-service.default.svc.cluster.local:9005/api/
  SHOULD_CONFIGURE_WITH_NOTIFICATION: "true"
  PORTAL_URL: http://harness-manager.default.svc.cluster.local:9090/
  MANAGER_CLIENT_BASEURL: http://harness-manager.default.svc.cluster.local:9090/
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  SHOULD_CONFIGURE_WITH_PMS: "true"
  PMS_TARGET: pipeline-service:12011
  PMS_AUTHORITY: pipeline-service:12011
  GRPC_SERVER_PORT: "9979"
  CACHE_CONFIG_REDIS_URL: 'redis://localhost:6379'
  CACHE_BACKEND: "REDIS"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_USE_SENTINEL: "true"
  MOCK_ACCESS_CONTROL_SERVICE: "false"
  ACCESS_CONTROL_BASE_URL:  http://access-control.default.svc.cluster.local:9006/api/
  ACCESS_CONTROL_ENABLED: "true"
---
# Source: harness/charts/platform/charts/delegate-proxy/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: delegate-proxy
  namespace: default
data:
  proxy.conf: "server { root /www/data;proxy_http_version 1.1;\n}"
---
# Source: harness/charts/platform/charts/gateway/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway
  namespace: default
data:
  MANAGER_URL:
  MANAGER_PUBLIC_URL:
  MEMORY: "512"
  DEPLOY_MODE: KUBERNETES_ONPREM
  API_VERSION: 'release-gateway:182'
  LOG_SVC_GLOBAL_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  TI_SVC_GLOBAL_TOKEN: 78d16b66-4b4c-11eb-8377-acde48001122
  CACHE_TYPE: REDIS
  TOKEN_CACHE_TTL: '300'
  USE_SENTINEL: 'true'
  SENTINEL_MASTER_NAME: 'harness-redis'
  REDIS_PORT: '26379'
  REDIS_SENTINELS: 'redis-sentinel-harness-announce-0.default,redis-sentinel-harness-announce-1.default,redis-sentinel-harness-announce-2.default'
  HARNESS_ENABLE_NG_AUTH_UI : 'true'
---
# Source: harness/charts/platform/charts/harness-manager/templates/config.yaml
apiVersion: v1
data:
  ALLOWED_ORIGINS: ''
  API_URL: ''
  DELEGATE_METADATA_URL: '/storage/wingsdelegates/delegateprod.txt'
  UI_SERVER_URL: ''
  WATCHER_METADATA_URL: '/storage/wingswatchers/watcherprod.txt'
  LOG_STREAMING_SERVICE_BASEURL: '/gateway/log-service/'
  ATMOSPHERE_BACKEND: REDIS
  BACKGROUND_SCHEDULER_CLUSTERED: "true"
  CACHE_BACKEND: REDIS
  CAPSULE_JAR: rest-capsule.jar
  DELEGATE_DOCKER_IMAGE: docker.io/harness/delegate:latest
  DELEGATE_SERVICE_TARGET: harness-manager:9879
  DELEGATE_SERVICE_AUTHORITY: harness-manager:9879
  DISTRIBUTED_LOCK_IMPLEMENTATION: REDIS
  DEPLOY_MODE: KUBERNETES_ONPREM
  DISABLE_NEW_RELIC: "true"
  ENABLE_G1GC: "true"
  EXTERNAL_GRAPHQL_RATE_LIMIT: '500'
  FEATURES: "LDAP_SSO_PROVIDER,ASYNC_ARTIFACT_COLLECTION,JIRA_INTEGRATION,AUDIT_TRAIL_UI,GDS_TIME_SERIES_SAVE_PER_MINUTE,STACKDRIVER_SERVICEGUARD,BATCH_SECRET_DECRYPTION,TIME_SERIES_SERVICEGUARD_V2,TIME_SERIES_WORKFLOW_V2,CUSTOM_DASHBOARD,GRAPHQL,CV_FEEDBACKS,LOGS_V2_247,UPGRADE_JRE,NEXT_GEN_ENABLED,LOG_STREAMING_INTEGRATION,NG_HARNESS_APPROVAL,GIT_SYNC_NG,NG_SHOW_DELEGATE,NG_CG_TASK_ASSIGNMENT_ISOLATION,CI_OVERVIEW_PAGE,AZURE_CLOUD_PROVIDER_VALIDATION_ON_DELEGATE,TERRAFORM_AWS_CP_AUTHENTICATION,NG_TEMPLATES,NEW_DEPLOYMENT_FREEZE,HELM_CHART_AS_ARTIFACT,RESOLVE_DEPLOYMENT_TAGS_BEFORE_EXECUTION,WEBHOOK_TRIGGER_AUTHORIZATION,GITHUB_WEBHOOK_AUTHENTICATION,CUSTOM_MANIFEST,GIT_ACCOUNT_SUPPORT,AZURE_WEBAPP,PRUNE_KUBERNETES_RESOURCES,LDAP_GROUP_SYNC_JOB_ITERATOR,POLLING_INTERVAL_CONFIGURABLE,APPLICATION_DROPDOWN_MULTISELECT,USER_GROUP_AS_EXPRESSION,RESOURCE_CONSTRAINT_SCOPE_PIPELINE_ENABLED,ENABLE_DEFAULT_NG_EXPERIENCE_FOR_ONPREM"
  HAZELCAST_NAMESPACE: 'default'
  HAZELCAST_SERVICE: harness-manager
  HZ_CLUSTER_NAME: harness-manager
  LOGGING_LEVEL: 'INFO'
  MEMORY: "2048"
  REDIS_SENTINEL: "true"
  REDIS_URL: 'redis://localhost:6379'
  REDIS_MASTER_NAME: 'harness-redis'
  REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  SERVER_PORT: "9090"
  SERVICE_ACC: /opt/harness/svc/service_acc.json
  VERSION: 1.0.77117
  LOG_STREAMING_SERVICE_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  NG_MANAGER_BASE_URL: '/ng/api/'
  ACCESS_CONTROL_ENABLED: "true"
  ACCESS_CONTROL_BASE_URL: '/authz/api/'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_AVAILABLE_IN_ONPREM: "true"
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  HARNESS_ENABLE_NG_AUTH_UI_PLACEHOLDER: "true"
  VERIFICATION_SERVICE_SECRET: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8

kind: ConfigMap
metadata:
  name: harness-manager-config
  namespace: default
  annotations: {}
---
# Source: harness/charts/platform/charts/le-nextgen/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: le-nextgen
  namespace: default
data:
  https_port: "10800"
  learning_env: "on_prem"
  server_url: "http://cv-nextgen:6060"
  service_secret: 59MR5RlVARcdH7zb7pNx6GzqiglBmXR8
---
# Source: harness/charts/platform/charts/log-service/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-service
  namespace: default
data:
  LOG_SERVICE_S3_BUCKET: logs
  LOG_SERVICE_S3_REGION: us-east-1
  LOG_SERVICE_S3_ENDPOINT:
  LOG_SERVICE_S3_PATH_STYLE: "true"
  LOG_SERVICE_DISABLE_AUTH: "true"
  LOG_SERVICE_GLOBAL_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  LOG_SERVICE_SECRET: IC04LYMBf1lDP5oeY4hupxd4HJhLmN6azUku3xEbeE3SUx5G3ZYzhbiwVtK4i7AmqyU9OZkwB4v8E9qM
---
# Source: harness/charts/platform/charts/mongodb/templates/common-scripts-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-replicaset-chart-common-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
data:
  startup-probe.sh: |
    #!/bin/bash
    mongosh  $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval 'db.hello().isWritablePrimary || db.hello().secondary' | grep -q 'true$'
  readiness-probe.sh: |
    #!/bin/bash
    # Run the proper check depending on the version
    [[ $(mongod -version | grep "db version") =~ ([0-9]+\.[0-9]+\.[0-9]+) ]] && VERSION=${BASH_REMATCH[1]}
    . /opt/bitnami/scripts/libversion.sh
    VERSION_MAJOR="$(get_sematic_version "$VERSION" 1)"
    VERSION_MINOR="$(get_sematic_version "$VERSION" 2)"
    VERSION_PATCH="$(get_sematic_version "$VERSION" 3)"
    if [[ ( "$VERSION_MAJOR" -ge 5 ) || ( "$VERSION_MAJOR" -ge 4 && "$VERSION_MINOR" -ge 4 && "$VERSION_PATCH" -ge 2 ) ]]; then
        mongosh $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval 'db.hello().isWritablePrimary || db.hello().secondary' | grep -q 'true$'
    else
        mongosh  $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval 'db.isMaster().ismaster || db.isMaster().secondary' | grep -q 'true$'
    fi
  ping-mongodb.sh: |
    #!/bin/bash
    mongosh  $TLS_OPTIONS --port $MONGODB_PORT_NUMBER --eval "db.adminCommand('ping')"
---
# Source: harness/charts/platform/charts/mongodb/templates/replicaset/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mongodb-replicaset-chart-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
data:
  setup.sh: |-
    #!/bin/bash

    . /opt/bitnami/scripts/mongodb-env.sh
    . /opt/bitnami/scripts/libfs.sh
    . /opt/bitnami/scripts/liblog.sh
    . /opt/bitnami/scripts/libvalidations.sh

    if is_empty_value "$MONGODB_ADVERTISED_PORT_NUMBER"; then
      export MONGODB_ADVERTISED_PORT_NUMBER="$MONGODB_PORT_NUMBER"
    fi

    info "Advertised Hostname: $MONGODB_ADVERTISED_HOSTNAME"
    info "Advertised Port: $MONGODB_ADVERTISED_PORT_NUMBER"

    # Check for existing replica set in case there is no data in the PVC
    # This is for cases where the PVC is lost or for MongoDB caches without
    # persistence
    current_primary=""
    if is_dir_empty "${MONGODB_DATA_DIR}/db"; then
      info "Data dir empty, checking if the replica set already exists"
      current_primary=$(mongosh admin --host "mongodb-replicaset-chart-0.mongodb-replicaset-chart-headless.default.svc.cluster.local:27017,mongodb-replicaset-chart-1.mongodb-replicaset-chart-headless.default.svc.cluster.local:27017,mongodb-replicaset-chart-2.mongodb-replicaset-chart-headless.default.svc.cluster.local:27017" --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD --eval 'db.runCommand("ismaster")' | awk -F\' '/primary/ {print $2}')

      if ! is_empty_value "$current_primary"; then
        info "Detected existing primary: ${current_primary}"
      fi
    fi

    if ! is_empty_value "$current_primary" && [[ "$MONGODB_ADVERTISED_HOSTNAME:$MONGODB_ADVERTISED_PORT_NUMBER" == "$current_primary" ]]; then
        info "Advertised name matches current primary, configuring node as a primary"
        export MONGODB_REPLICA_SET_MODE="primary"
    elif ! is_empty_value "$current_primary" && [[ "$MONGODB_ADVERTISED_HOSTNAME:$MONGODB_ADVERTISED_PORT_NUMBER" != "$current_primary" ]]; then
        info "Current primary is different from this node. Configuring the node as replica of ${current_primary}"
        export MONGODB_REPLICA_SET_MODE="secondary"
        export MONGODB_INITIAL_PRIMARY_HOST="${current_primary%:*}"
        export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="${current_primary#*:}"
        export MONGODB_SET_SECONDARY_OK="yes"
    elif [[ "$MY_POD_NAME" = "mongodb-replicaset-chart-0" ]]; then
        info "Pod name matches initial primary pod name, configuring node as a primary"
        export MONGODB_REPLICA_SET_MODE="primary"
    else
        info "Pod name doesn't match initial primary pod name, configuring node as a secondary"
        export MONGODB_REPLICA_SET_MODE="secondary"
        export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="$MONGODB_PORT_NUMBER"
    fi

    if [[ "$MONGODB_REPLICA_SET_MODE" == "secondary" ]]; then
        export MONGODB_INITIAL_PRIMARY_ROOT_USER="$MONGODB_ROOT_USER"
        export MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD="$MONGODB_ROOT_PASSWORD"
        export MONGODB_ROOT_PASSWORD=""
        export MONGODB_EXTRA_USERNAMES=""
        export MONGODB_EXTRA_DATABASES=""
        export MONGODB_EXTRA_PASSWORDS=""
        export MONGODB_ROOT_PASSWORD_FILE=""
        export MONGODB_EXTRA_USERNAMES_FILE=""
        export MONGODB_EXTRA_DATABASES_FILE=""
        export MONGODB_EXTRA_PASSWORDS_FILE=""
    fi

    exec /opt/bitnami/scripts/mongodb/entrypoint.sh /opt/bitnami/scripts/mongodb/run.sh
  setup-hidden.sh: |-
    #!/bin/bash

    . /opt/bitnami/scripts/mongodb-env.sh

    echo "Advertised Hostname: $MONGODB_ADVERTISED_HOSTNAME"
    echo "Advertised Port: $MONGODB_ADVERTISED_PORT_NUMBER"
    echo "Configuring node as a hidden node"
    export MONGODB_REPLICA_SET_MODE="hidden"
    export MONGODB_INITIAL_PRIMARY_ROOT_USER="$MONGODB_ROOT_USER"
    export MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD="$MONGODB_ROOT_PASSWORD"
    export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="$MONGODB_PORT_NUMBER"
    export MONGODB_ROOT_PASSWORD=""
    export MONGODB_EXTRA_USERNAMES=""
    export MONGODB_EXTRA_DATABASES=""
    export MONGODB_EXTRA_PASSWORDS=""
    export MONGODB_ROOT_PASSWORD_FILE=""
    export MONGODB_EXTRA_USERNAMES_FILE=""
    export MONGODB_EXTRA_DATABASES_FILE=""
    export MONGODB_EXTRA_PASSWORDS_FILE=""
    exec /opt/bitnami/scripts/mongodb/entrypoint.sh /opt/bitnami/scripts/mongodb/run.sh
---
# Source: harness/charts/platform/charts/next-gen-ui/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: next-gen-ui
  namespace: default
data:
  API_URL: '/gateway'
  DEPLOYMENT_TYPE: ON_PREM
  HARNESS_ENABLE_NG_AUTH_UI_PLACEHOLDER: "true"
---
# Source: harness/charts/platform/charts/ng-auth-ui/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ng-auth-ui
  namespace: default
data:
  DEPLOYMENT_TYPE: ON_PREM
  API_URL: /gateway
---
# Source: harness/charts/platform/charts/ng-manager/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ng-manager
  namespace: default
data:
  CACHE_BACKEND: "REDIS"
  DEPLOY_MODE: KUBERNETES_ONPREM
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  NG_MANAGER_TARGET: ng-manager:13002
  NG_MANAGER_AUTHORITY: ng-manager:13002
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  GRPC_SERVER_PORT: "9979"
  SHOULD_CONFIGURE_WITH_PMS: "true"
  PMS_GITSYNC_TARGET:  pipeline-service:14002
  PMS_GITSYNC_AUTHORITY:  pipeline-service:14002
  TEMPLATE_GITSYNC_TARGET:  template-service:16002
  TEMPLATE_GITSYNC_AUTHORITY:  template-service:16002
  PMS_TARGET:  pipeline-service:12011
  PMS_AUTHORITY:  pipeline-service:12011
  MEMORY: "4096m"
  LOGGING_LEVEL: INFO
  LOCK_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  LOCK_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  LOCK_CONFIG_USE_SENTINEL: "true"
  LOG_STREAMING_SERVICE_TOKEN: c76e567a-b341-404d-a8dd-d9738714eb82
  USE_REDIS_FOR_SDK_RESPONSE_EVENTS: "true"
  MOCK_ACCESS_CONTROL_SERVICE: "false"
  ACCESS_CONTROL_ENABLED: "true"
  ENABLE_DEFAULT_RESOURCE_GROUP_CREATION: "true"
  ENABLE_DASHBOARD_TIMESCALE: "true"
  AUDIT_ENABLED: "true"
  SCM_SERVICE_URI: "scm:8091"
  MANAGER_CLIENT_BASEURL: http://harness-manager.default.svc.cluster.local:9090/api/
  NG_MANAGER_CLIENT_BASEURL: '/ng/api/'
  MANAGER_UI_URL:  ''
  NG_MANAGER_API_URL: '/ng/api/'
  NG_MANAGER_UI_URL: '/ng/#/'
  LOG_STREAMING_SERVICE_BASEURL: 'http://log-service.default.svc.cluster.local:8079/'
  ACCESS_CONTROL_BASE_URL: 'http://access-control.default.svc.cluster.local:9006/api/'
  RESOURCE_GROUP_BASE_URL: 'http://platform-service.default.svc.cluster.local:9005/api/'
  AUDIT_CLIENT_BASEURL: 'http://platform-service.default.svc.cluster.local:9005/api/'
  CURRENT_GEN_UI_URL: '/#/'
  HARNESS_ENABLE_NG_AUTH_UI_PLACEHOLDER: "true"
  MONGO_TRANSACTIONS_ALLOWED: "false"
---
# Source: harness/charts/platform/charts/pipeline-service/templates/configmap.yaml
apiVersion: v1
data:
  CACHE_BACKEND: "REDIS"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_USE_SENTINEL: "true"
  DEPLOY_MODE: KUBERNETES_ONPREM
  LOGGING_LEVEL: INFO
  LOCK_CONFIG_REDIS_SENTINELS: redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379
  LOCK_CONFIG_SENTINEL_MASTER_NAME: harness-redis
  LOCK_CONFIG_USE_SENTINEL: "true"
  MEMORY: "4096m"
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  GRPC_SERVER_PORT: "12011"
  NG_MANAGER_TARGET: ng-manager:9979
  NG_MANAGER_AUTHORITY: ng-manager:9979
  NG_MANAGER_GITSYNC_TARGET: ng-manager:13002
  NG_MANAGER_GITSYNC_AUTHORITY: ng-manager:13002
  CI_MANAGER_TARGET: ci-manager:9979
  CI_MANAGER_AUTHORITY: ci-manager:9979
  SCM_SERVICE_URI: "scm:8091"
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  PIPELINE_SERVICE_BASE_URL: '/ng/#'
  PMS_API_BASE_URL: '/pipeline/api/'
  LOG_STREAMING_SERVICE_BASEURL: 'http://log-service.default.svc.cluster.local:8079/'
  MANAGER_BASE_URL: '/api/'
  NG_MANAGER_BASE_URL: '/ng/api/'
  WEBHOOK_TRIGGER_BASEURL: '/ng/api/'
  CUSTOM_TRIGGER_BASEURL: '/pipeline/api/'
  ACCESS_CONTROL_BASE_URL: '/authz/api/'
  NOTIFICATION_BASE_URL: '/notifications/api/'
  TEMPLATE_SERVICE_ENDPOINT: '/template/api/'
  CI_MANAGER_BASE_URL: '/ci/'
  MANAGER_CLIENT_BASEURL: '/api/'
  AUTH_ENABLED: "true"
  USE_REDIS_FOR_INTERRUPTS: "true"
  USE_REDIS_FOR_ORCHESTRATION_EVENTS: "true"
  USE_REDIS_FOR_SDK_RESPONSE_EVENTS: "true"
  MOCK_ACCESS_CONTROL_SERVICE: "false"
  ACCESS_CONTROL_ENABLED: "true"
  ENABLE_DASHBOARD_TIMESCALE: "true"
  SHOULD_USE_INSTANCE_CACHE: "false"
  STO_MANAGER_BASE_URL: '/sto-manager/'
  STO_MANAGER_AUTHORITY: sto-manager:9979
  STO_MANAGER_TARGET: sto-manager:9979
  CV_MANAGER_BASE_URL: '/cv/api/'
  CVNG_MANAGER_AUTHORITY: cv-nextgen:9979
  CVNG_MANAGER_TARGET: cv-nextgen:9979

kind: ConfigMap
metadata:
  name: pipeline-service
  namespace: default
---
# Source: harness/charts/platform/charts/platform-service/templates/configmap.yaml
apiVersion: v1
data:
  DEPLOY_MODE: KUBERNETES_ONPREM
  GRPC_MANAGER_TARGET: harness-manager:9879
  GRPC_MANAGER_AUTHORITY: harness-manager:9879
  SMTP_HOST: ""
  SMTP_PORT: ""
  SMTP_PASSWORD: ""
  SMTP_USERNAME: ""
  SMTP_USE_SSL: "true"
  ENABLE_AUDIT_SERVICE: 'true'
  MOCK_ACCESS_CONTROL_SERVICE: 'false'
  AUDIT_ENABLED: 'true'
  ENABLE_RESOURCE_GROUP: 'true'
  EVENTS_FRAMEWORK_USE_SENTINEL: 'true'
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  LOCK_CONFIG_USE_SENTINEL: 'true'
  LOCK_CONFIG_SENTINEL_MASTER_NAME: 'harness-redis'
  LOCK_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  ACCESS_CONTROL_ENABLED: 'true'
  LOGGING_LEVEL: INFO
  DISTRIBUTED_LOCK_IMPLEMENTATION: REDIS
  MANAGER_CLIENT_BASEURL: '/api/'
  RBAC_URL: '/ng/api/'
  ACCESS_CONTROL_BASE_URL: 'http://access-control.default.svc.cluster.local:9006/api/'
  RESOURCE_GROUP_CLIENT_BASE_URL: 'http://platform-service.default.svc.cluster.local:9005/api/'
  NG_MANAGER_CLIENT_BASEURL: '/ng/api/'
  PIPELINE_SERVICE_CLIENT_BASEURL: '/pipeline/api/'
  TEMPLATE_SERVICE_CLIENT_BASEURL: '/template/api/'
  AUDIT_CLIENT_BASEURL: 'http://platform-service.default.svc.cluster.local:9005/api/'
  TEMPLATE_SERVICE_BASE_URL: http://template-service.default.svc.cluster.local:15002/api/
  MEMORY: "3072m"

kind: ConfigMap
metadata:
  name: platform-service
  namespace: default
---
# Source: harness/charts/platform/charts/redis/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-sentinel-harness-configmap
  namespace: default
  labels:
    app: redis-sentinel-harness
    helm.sh/chart: redis-0.2.2
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |
    dir "/data"
    port 6379
    active-defrag-cycle-max 25
    active-defrag-ignore-bytes 1mb
    activedefrag yes
    maxmemory 0
    maxmemory-policy volatile-lru
    min-replicas-max-lag 10
    min-replicas-to-write 1
    rdbchecksum yes
    rdbcompression yes
    repl-diskless-sync yes
    save 60 1
    maxclients 30000
    timeout 10

  sentinel.conf: |
    dir "/data"
        sentinel down-after-milliseconds harness-redis 10000
        sentinel failover-timeout harness-redis 180000
        maxclients 30000
        sentinel parallel-syncs harness-redis 5

  init.sh: |
    HOSTNAME="$(hostname)"
    INDEX="${HOSTNAME##*-}"
    MASTER="$(redis-cli -h redis-sentinel-harness -p 26379 sentinel get-master-addr-by-name harness-redis | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
    MASTER_GROUP="harness-redis"
    QUORUM="2"
    REDIS_CONF=/data/conf/redis.conf
    REDIS_PORT=6379
    SENTINEL_CONF=/data/conf/sentinel.conf
    SENTINEL_PORT=26379
    SERVICE=redis-sentinel-harness
    set -eu

    sentinel_update() {
        echo "Updating sentinel config with master $MASTER"
        eval MY_SENTINEL_ID="\${SENTINEL_ID_$INDEX}"
        sed -i "1s/^/sentinel myid $MY_SENTINEL_ID\\n/" "$SENTINEL_CONF"
        sed -i "2s/^/sentinel monitor $MASTER_GROUP $1 $REDIS_PORT $QUORUM \\n/" "$SENTINEL_CONF"
        echo "sentinel announce-ip $ANNOUNCE_IP" >> $SENTINEL_CONF
        echo "sentinel announce-port $SENTINEL_PORT" >> $SENTINEL_CONF
    }

    redis_update() {
        echo "Updating redis config"
        echo "slaveof $1 $REDIS_PORT" >> "$REDIS_CONF"
        echo "slave-announce-ip $ANNOUNCE_IP" >> $REDIS_CONF
        echo "slave-announce-port $REDIS_PORT" >> $REDIS_CONF
    }

    copy_config() {
        cp /readonly-config/redis.conf "$REDIS_CONF"
        cp /readonly-config/sentinel.conf "$SENTINEL_CONF"
    }

    setup_defaults() {
        echo "Setting up defaults"
        if [ "$INDEX" = "0" ]; then
            echo "Setting this pod as the default master"
            redis_update "$ANNOUNCE_IP"
            sentinel_update "$ANNOUNCE_IP"
            sed -i "s/^.*slaveof.*//" "$REDIS_CONF"
        else
            DEFAULT_MASTER="$(getent hosts "$SERVICE-announce-0" | awk '{ print $1 }')"
            if [ -z "$DEFAULT_MASTER" ]; then
                echo "Unable to resolve host"
                exit 1
            fi
            echo "Setting default slave config.."
            redis_update "$DEFAULT_MASTER"
            sentinel_update "$DEFAULT_MASTER"
        fi
    }

    find_master() {
        echo "Attempting to find master"
        if [ "$(redis-cli -h "$MASTER" ping)" != "PONG" ]; then
           echo "Can't ping master, attempting to force failover"
           if redis-cli -h "$SERVICE" -p "$SENTINEL_PORT" sentinel failover "$MASTER_GROUP" | grep -q 'NOGOODSLAVE' ; then
               setup_defaults
               return 0
           fi
           sleep 10
           MASTER="$(redis-cli -h $SERVICE -p $SENTINEL_PORT sentinel get-master-addr-by-name $MASTER_GROUP | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
           if [ "$MASTER" ]; then
               sentinel_update "$MASTER"
               redis_update "$MASTER"
           else
              echo "Could not failover, exiting..."
              exit 1
           fi
        else
            echo "Found reachable master, updating config"
            sentinel_update "$MASTER"
            redis_update "$MASTER"
        fi
    }

    mkdir -p /data/conf/

    echo "Initializing config.."
    copy_config

    ANNOUNCE_IP=$(getent hosts "$SERVICE-announce-$INDEX" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP" ]; then
        "Could not resolve the announce ip for this pod"
        exit 1
    elif [ "$MASTER" ]; then
        find_master
    else
        setup_defaults
    fi

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/replace-default-auth/${ESCAPED_AUTH}/" "$REDIS_CONF" "$SENTINEL_CONF"
    fi

    echo "Ready..."

  haproxy_init.sh: |
    HAPROXY_CONF=/data/haproxy.cfg
    cp /readonly/haproxy.cfg "$HAPROXY_CONF"
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-0 && break
      echo "Waiting for service redis-sentinel-harness-announce-0 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP0=$(getent hosts "redis-sentinel-harness-announce-0" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP0" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-0"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE0/$ANNOUNCE_IP0/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-1 && break
      echo "Waiting for service redis-sentinel-harness-announce-1 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP1=$(getent hosts "redis-sentinel-harness-announce-1" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP1" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-1"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE1/$ANNOUNCE_IP1/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
    for loop in $(seq 1 10); do
      getent hosts redis-sentinel-harness-announce-2 && break
      echo "Waiting for service redis-sentinel-harness-announce-2 to be ready ($loop) ..." && sleep 1
    done
    ANNOUNCE_IP2=$(getent hosts "redis-sentinel-harness-announce-2" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP2" ]; then
      echo "Could not resolve the announce ip for redis-sentinel-harness-announce-2"
      exit 1
    fi
    sed -i "s/REPLACE_ANNOUNCE2/$ANNOUNCE_IP2/" "$HAPROXY_CONF"

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/REPLACE_AUTH_SECRET/${ESCAPED_AUTH}/" "$HAPROXY_CONF"
    fi
---
# Source: harness/charts/platform/charts/template-service/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: template-service
  namespace: default
data:
  CACHE_BACKEND: "REDIS"
  CACHE_CONFIG_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  CACHE_CONFIG_SENTINEL_MASTER_NAME: "harness-redis"
  CACHE_CONFIG_USE_SENTINEL: "true"
  DEPLOY_MODE: KUBERNETES_ONPREM
  LOGGING_LEVEL: INFO
  MEMORY: "1024m"
  MANAGER_TARGET: harness-manager:9879
  MANAGER_AUTHORITY: harness-manager:9879
  NG_MANAGER_GITSYNC_TARGET: ng-manager:13002
  NG_MANAGER_GITSYNC_AUTHORITY: ng-manager:13002
  SCM_SERVICE_URI: "scm:8091"
  EVENTS_FRAMEWORK_REDIS_URL: 'redis://localhost:6379'
  EVENTS_FRAMEWORK_USE_SENTINEL: "true"
  EVENTS_FRAMEWORK_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINELS: 'redis://redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  ENABLE_AUTH: "true"
  ENABLE_GIT_SYNC: "true"
  ENABLE_AUDIT: "true"
  ACCESS_CONTROL_ENABLED: "true"
  ENFORCEMENT_CHECK_ENABLED: "true"
  NG_MANAGER_BASE_URL: '/ng/api/'
  ACCESS_CONTROL_BASE_URL: 'http://access-control.default.svc.cluster.local:9006/api/'
  AUDIT_SERVICE_BASE_URL: 'http://platform-service.default.svc.cluster.local:9005/api/'
  SERVER_PORT: "15002"
  PMS_GRPC_AUTHORITY: pipeline-service:12011
  PMS_GRPC_TARGET: pipeline-service:12011
  MANAGER_CLIENT_BASEURL: /api/
---
# Source: harness/charts/platform/charts/ti-service/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ti-service
  namespace: default
data:
  TI_SERVICE_TIMESCALE_HOST: timescaledb-single-chart.default
  TI_SERVICE_TIMESCALE_PORT: "5432"
  TI_SERVICE_DB_NAME: harnessti
  TI_SERVICE_MONGODB_DB_NAME: ti-harness
  TI_SERVICE_HYPER_TABLE: evaluation
  TI_SERVICE_DISABLE_AUTH: "true"
  TI_SERVICE_SELECTION_HYPER_TABLE: selection
  TI_SERVICE_COVERAGE_HYPER_TABLE: coverage
  EVENTS_FRAMEWORK_REDIS_URL: default
  EVENTS_FRAMEWORK_REDIS_USE_SENTINEL: 'true'
  EVENTS_FRAMEWORK_REDIS_SENTINEL_MASTER_NAME: 'harness-redis'
  EVENTS_FRAMEWORK_REDIS_SENTINEL_URLS: 'redis-sentinel-harness-announce-0.default:26379,redis://redis-sentinel-harness-announce-1.default:26379,redis://redis-sentinel-harness-announce-2.default:26379'
  TI_SERVICE_GLOBAL_TOKEN: 78d16b66-4b4c-11eb-8377-acde48001122
---
# Source: harness/charts/platform/charts/timescaledb/templates/configmap-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-timescaledb-patroni
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: patroni
data:
  patroni.yaml: |
    bootstrap:
      dcs:
        loop_wait: 10
        maximum_lag_on_failover: 33554432
        postgresql:
          parameters:
            archive_command: /etc/timescaledb/scripts/pgbackrest_archive.sh %p
            archive_mode: "on"
            archive_timeout: 1800s
            autovacuum_analyze_scale_factor: 0.02
            autovacuum_max_workers: 10
            autovacuum_naptime: 5s
            autovacuum_vacuum_cost_limit: 500
            autovacuum_vacuum_scale_factor: 0.05
            hot_standby: "on"
            log_autovacuum_min_duration: 1min
            log_checkpoints: "on"
            log_connections: "on"
            log_disconnections: "on"
            log_line_prefix: '%t [%p]: [%c-%l] %u@%d,app=%a [%e] '
            log_lock_waits: "on"
            log_min_duration_statement: 1s
            log_statement: ddl
            max_connections: 100
            max_prepared_transactions: 150
            shared_preload_libraries: timescaledb,pg_stat_statements
            tcp_keepalives_idle: 900
            tcp_keepalives_interval: 100
            temp_file_limit: 1GB
            timescaledb.passfile: ../.pgpass
            unix_socket_directories: /var/run/postgresql
            unix_socket_permissions: "0750"
            wal_level: hot_standby
            wal_log_hints: "on"
          use_pg_rewind: true
          use_slots: true
        retry_timeout: 10
        ttl: 30
      method: restore_or_initdb
      post_init: /etc/timescaledb/scripts/post_init.sh
      restore_or_initdb:
        command: |
          /etc/timescaledb/scripts/restore_or_initdb.sh --encoding=UTF8 --locale=C.UTF-8
        keep_existing_recovery_conf: true
    kubernetes:
      role_label: role
      scope_label: cluster-name
      use_endpoints: true
    log:
      level: WARNING
    postgresql:
      authentication:
        replication:
          username: standby
        superuser:
          username: postgres
      basebackup:
      - waldir: /var/lib/postgresql/wal/pg_wal
      callbacks:
        on_reload: /etc/timescaledb/scripts/patroni_callback.sh
        on_restart: /etc/timescaledb/scripts/patroni_callback.sh
        on_role_change: /etc/timescaledb/scripts/patroni_callback.sh
        on_start: /etc/timescaledb/scripts/patroni_callback.sh
        on_stop: /etc/timescaledb/scripts/patroni_callback.sh
      create_replica_methods:
      - pgbackrest
      - basebackup
      listen: 0.0.0.0:5432
      pg_hba:
      - local     all             postgres                              peer
      - local     all             all                                   md5
      - hostnossl all,replication all                all                md5
      - hostssl   all             all                127.0.0.1/32       md5
      - hostssl   all             all                ::1/128            md5
      - hostssl   replication     standby            all                md5
      - hostssl   all             all                all                md5
      pgbackrest:
        command: /etc/timescaledb/scripts/pgbackrest_restore.sh
        keep_data: true
        no_master: true
        no_params: true
      recovery_conf:
        restore_command: /etc/timescaledb/scripts/pgbackrest_archive_get.sh %f "%p"
      use_unix_socket: true
    restapi:
      listen: 0.0.0.0:8008
...
---
# Source: harness/charts/platform/charts/timescaledb/templates/configmap-pgbackrest.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-timescaledb-pgbackrest
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: pgbackrest
data:
  pgbackrest.conf: |
    [global]
    compress-level=3
    compress-type=lz4
    process-max=4
    repo1-cipher-type=none
    repo1-path=/default/timescaledb-single-chart/
    repo1-retention-diff=2
    repo1-retention-full=2
    repo1-s3-endpoint=s3.amazonaws.com
    repo1-s3-region=us-east-2
    repo1-type=s3
    spool-path=/var/run/postgresql
    start-fast=y

    [poddb]
    pg1-port=5432
    pg1-host-user=postgres
    pg1-path=/var/lib/postgresql/data
    pg1-socket-path=/var/run/postgresql

    link-all=y

    [global:archive-push]

    [global:archive-get]
...
---
# Source: harness/charts/platform/charts/timescaledb/templates/configmap-postinit.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-harness-post-init
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: scripts
data:
  ## initialize harness databases
  on_start.sh: |
    #!/bin/bash
    psql -d "$1" << __SQL__
    CREATE DATABASE harness;
    CREATE DATABASE harnessti;
    __SQL__
---
# Source: harness/charts/platform/charts/timescaledb/templates/configmap-scripts.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-timescaledb-scripts
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: scripts
data:
  # If no backup is configured, archive_command would normally fail. A failing archive_command on a cluster
  # is going to cause WAL to be kept around forever, meaning we'll fill up Volumes we have quite quickly.
  #
  # Therefore, if the backup is disabled, we always return exitcode 0 when archiving
  pgbackrest_archive.sh: |
    #!/bin/sh

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - archive - $1"
    }

    [ -z "$1" ] && log "Usage: $0 <WALFILE or DIRECTORY>" && exit 1

    PGBACKREST_BACKUP_ENABLED=0
    [ ${PGBACKREST_BACKUP_ENABLED} -ne 0 ] || exit 0

    . "${HOME}/.pgbackrest_environment"
    exec pgbackrest --stanza=poddb archive-push "$@"
  pgbackrest_archive_get.sh: |
    #!/bin/sh
    PGBACKREST_BACKUP_ENABLED=0
    [ ${PGBACKREST_BACKUP_ENABLED} -ne 0 ] || exit 0

    . "${HOME}/.pgbackrest_environment"
    exec pgbackrest --stanza=poddb archive-get "${1}" "${2}"
  pgbackrest_bootstrap.sh: |
    #!/bin/sh
    set -e

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }

    terminate() {
        log "Stopping"
        exit 1
    }
    # If we don't catch these signals, and we're still waiting for PostgreSQL
    # to be ready, we will not respond at all to a regular shutdown request,
    # therefore, we explicitly terminate if we receive these signals.
    trap terminate TERM QUIT

    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done

    # We'll be lazy; we wait for another while to allow the database to promote
    # to primary if it's the only one running
    sleep 10

    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" = "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
            log "Creating initial backup"
            pgbackrest --type=full backup || exit 1
        }
    fi

    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
  pgbackrest_restore.sh: |
    #!/bin/sh
    PGBACKREST_BACKUP_ENABLED=0
    [ ${PGBACKREST_BACKUP_ENABLED} -ne 0 ] || exit 1

    . "${HOME}/.pod_environment"

    PGDATA="/var/lib/postgresql/data"
    WALDIR="/var/lib/postgresql/wal/pg_wal"

    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"

    exec pgbackrest --force --delta --log-level-console=detail restore
  restore_or_initdb.sh: |
    #!/bin/sh

    . "${HOME}/.pod_environment"

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }

    PGDATA="/var/lib/postgresql/data"
    WALDIR="/var/lib/postgresql/wal/pg_wal"
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"

    if [ "${BOOTSTRAP_FROM_BACKUP}" = "1" ]; then
        log "Attempting restore from backup"
        # we want to override the environment with the environment
        # shellcheck disable=SC2046
        export $(env -i envdir /etc/pgbackrest/bootstrap env) > /dev/null
        export PGBACKREST_REPO1_PATH=

        if [ -z "${PGBACKREST_REPO1_PATH}" ]; then
            log "Unconfigured repository path"
            cat << "__EOT__"

    TimescaleDB Single Helm Chart error:

    You should configure the bootstrapFromBackup in your Helm Chart section by explicitly setting
    the repo1-path to point to the backups.

    For example, if you want to do a disaster recovery, and you want to reuse
    the backup, you could configure the path as follows:

    ```yaml
    bootstrapFromBackup:
      enabled: true
      repo1-path: "/default/timescaledb-single-chart/"
    ```

    For more information, consult the admin guide:
    https://github.com/timescale/helm-charts/blob/main/charts/timescaledb-single/admin-guide.md#bootstrap-from-backup


    __EOT__

            exit 1
        fi

        log "Listing available backup information"
        pgbackrest info
        EXITCODE=$?
        if [ ${EXITCODE} -ne 0 ]; then
            exit $EXITCODE
        fi

        pgbackrest --log-level-console=detail restore
        EXITCODE=$?
        if [ ${EXITCODE} -eq 0 ]; then
            log "pgBackRest restore finished succesfully, starting instance in recovery"
            # We want to ensure we do not overwrite a current backup repository with archives, therefore
            # we block archiving from succeeding until Patroni can takeover
            touch "${PGDATA}/recovery.signal"
            pg_ctl -D "${PGDATA}" start -o '--archive-command=/bin/false'

            while ! pg_isready -q; do
                log "Waiting for PostgreSQL to become available"
                sleep 3
            done

            # It is not trivial to figure out to what point we should restore, pgBackRest
            # should be fetching WAL segments until the WAL is exhausted. We'll ask pgBackRest
            # what the Maximum Wal is that it currently has; as soon as we see that, we can consider
            # the restore to be done
            while true; do
              MAX_BACKUP_WAL="$(pgbackrest info --output=json | python3 -c "import json,sys;obj=json.load(sys.stdin); print(obj[0]['archive'][0]['max']);")"
              log "Testing whether WAL file ${MAX_BACKUP_WAL} has been restored ..."
              [ -f "${PGDATA}/pg_wal/${MAX_BACKUP_WAL}" ] && break
              sleep 30;
            done

            # At this point we know the final WAL archive has been restored, we should be done.
            log "The WAL file ${MAX_BACKUP_WAL} has been successully restored, shutting down instance"
            pg_ctl -D "${PGDATA}" promote
            pg_ctl -D "${PGDATA}" stop -m fast
            log "Handing over control to Patroni ..."
        else
            log "Bootstrap from backup failed"
            exit 1
        fi
    else
        # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
        # initdb has no business with these parameters
        initdb_args=""
        for value in "$@"
        do
            case $value in
                "--scope"*)
                    ;;
                "--datadir"*)
                    ;;
                *)
                    initdb_args="${initdb_args} $value"
                    ;;
            esac
        done

        log "Invoking initdb"
        # shellcheck disable=SC2086
        initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    fi

    echo "include_if_exists = '/var/run/postgresql/timescaledb.conf'" >> "${PGDATA}/postgresql.conf"
  post_init.sh: |
    #!/bin/sh
    . "${HOME}/.pod_environment"

    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }

    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
      \connect template1
      -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;

      \connect postgres
      SET synchronous_commit to 'off';
      CREATE EXTENSION timescaledb;
    __SQL__

    TABLESPACES=""
    for tablespace in $TABLESPACES
    do
      log "Creating tablespace ${tablespace}"
      tablespacedir="/var/lib/postgresql/tablespaces/${tablespace}/data"
      psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done

    # This directory may contain user defined post init steps
    for file in /etc/timescaledb/post_init.d/*
    do
      [ -d "$file" ] && continue
      [ ! -r "$file" ] && continue

      case "$file" in
        *.sh)
          if [ -x "$file" ]; then
            log "Call post init script [ $file ]"
            "$file" "$@"
            EXITCODE=$?
          else
            log "Source post init script [ $file ]"
            . "$file"
            EXITCODE=$?
          fi
          ;;
        *.sql)
          log "Apply post init sql [ $file ]"
          # Disable synchronous_commit since we're initializing
          PGOPTIONS="-c synchronous_commit=local" psql -d "$URL" -f "$file"
          EXITCODE=$?
          ;;
        *.sql.gz)
          log "Decompress and apply post init sql [ $file ]"
          gunzip -c "$file" | PGOPTIONS="-c synchronous_commit=local" psql -d "$URL"
          EXITCODE=$?
          ;;
        *)
          log "Ignore unknown post init file type [ $file ]"
          EXITCODE=0
          ;;
      esac
        EXITCODE=$?
        if [ "$EXITCODE" != "0" ]
        then
            log "ERROR: post init script $file exited with exitcode $EXITCODE"
            exit $EXITCODE
        fi
    done

    # We exit 0 this script, otherwise the database initialization fails.
    exit 0
  patroni_callback.sh: |
    #!/bin/sh
    set -e

    . "${HOME}/.pod_environment"

    for suffix in "$1" all
    do
      CALLBACK="/etc/timescaledb/callbacks/${suffix}"
      if [ -f "${CALLBACK}" ]
      then
        "${CALLBACK}" "$@"
      fi
    done

  lifecycle_preStop.psql: |
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''

    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '/var/run/postgresql', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset

    \timing on
    \set ECHO queries

    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif

    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;

    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
...
---
# Source: harness/charts/platform/charts/minio/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: minio
  namespace: "default"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "8Gi"
---
# Source: harness/charts/platform/charts/harness-manager/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: harness-manager-role
  namespace: default
  annotations: {}
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
  - apiGroups: [""]
    resources: ["pods"]
    verbs:
      - get
      - list
      - watch
---
# Source: harness/charts/platform/charts/timescaledb/templates/role-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-timescaledb
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: rbac
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources:
  - endpoints
  - endpoints/restricted
  verbs:
  - create
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources: ["pods"]
  verbs:
  - get
  - list
  - patch
  - update
  - watch
- apiGroups: [""]
  resources: ["services"]
  verbs:
#  - create
  - get
  - list
#  - patch
#  - update
  - watch
---
# Source: harness/charts/platform/charts/harness-manager/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: harness-manager-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: harness-manager-role
subjects:
  - kind: ServiceAccount
    name: harness-default
    namespace: default
---
# Source: harness/charts/platform/charts/timescaledb/templates/rolebinding-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-timescaledb
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: rbac
subjects:
  - kind: ServiceAccount
    name: release-name-timescaledb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-timescaledb
---
# Source: harness/charts/ccm/charts/nextgen-ce/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nextgen-ce
  namespace: default
  labels:
    helm.sh/chart: nextgen-ce-0.1.3
    app.kubernetes.io/name: nextgen-ce
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.773.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 6340
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: nextgen-ce
    app.kubernetes.io/instance: release-name
---
# Source: harness/charts/infra/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-hl
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app: postgres
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
# Source: harness/charts/infra/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app: postgres
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
# Source: harness/charts/platform/charts/access-control/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name:  access-control
  namespace: default
  labels:
    helm.sh/chart: access-control-0.2.7
    app.kubernetes.io/name: access-control
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 9006
    protocol: TCP
    targetPort: 9006
  selector:
    app.kubernetes.io/name: access-control
    app.kubernetes.io/instance: release-name
---
# Source: harness/charts/platform/charts/cv-nextgen/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cv-nextgen
  namespace: default
  labels:
    helm.sh/chart: cv-nextgen-0.2.13
    app.kubernetes.io/name: cv-nextgen
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: cv
      port: 6060
      protocol: TCP
      targetPort: 6060
    - name: grpc-cv-nextgen
      port: 9979
      protocol: TCP
      targetPort: 9979
  selector:
    app.kubernetes.io/name: cv-nextgen
    app.kubernetes.io/instance: release-name
  type: ClusterIP
---
# Source: harness/charts/platform/charts/delegate-proxy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: delegate-proxy
  namespace: default
  labels:
    helm.sh/chart: delegate-proxy-0.2.5
    app.kubernetes.io/name: delegate-proxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: delegate-proxy
    app.kubernetes.io/instance: release-name
  ports:
    - port: 80
      targetPort: 8080
---
# Source: harness/charts/platform/charts/gateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: gateway
  namespace: default
  labels:
    helm.sh/chart: gateway-0.2.9
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort:  8080
  selector:
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: release-name
---
# Source: harness/charts/platform/charts/harness-manager/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: harness-manager-0.2.22
    app.kubernetes.io/name: harness-manager
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: harness-manager
  namespace: default
  annotations: {}
spec:
  ports:
    - name: http-manager
      port: 9090
      protocol: TCP
      targetPort: 9090
    - name: grpc-manager
      port: 9879
      protocol: TCP
      targetPort: 9879
  selector:
    app.kubernetes.io/name: harness-manager
    app.kubernetes.io/instance: release-name
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness/charts/platform/charts/log-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: log-service
  namespace: default
  labels:
    helm.sh/chart: log-service-0.2.9
    app.kubernetes.io/name: log-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: log-service
    port: 8079
    protocol: TCP
    targetPort: 8079
  selector:
    app.kubernetes.io/name: log-service
    app.kubernetes.io/instance: release-name
---
# Source: harness/charts/platform/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: "default"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: minio-api
      port: 9000
      targetPort: minio-api
      nodePort: null
    - name: minio-console
      port: 9001
      targetPort: minio-console
      nodePort: null
  selector:
    app.kubernetes.io/name: minio
    app.kubernetes.io/instance: release-name
---
# Source: harness/charts/platform/charts/mongodb/templates/arbiter/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongodb-replicaset-chart-arbiter-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: arbiter
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-mongodb
      port: 27017
      targetPort: mongodb
  selector:
    app.kubernetes.io/name: mongodb
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: arbiter
---
# Source: harness/charts/platform/charts/mongodb/templates/replicaset/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongodb-replicaset-chart
  namespace: "default"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: "mongodb"
      port: 27017
      targetPort: mongodb
  selector:
    app.kubernetes.io/name: mongodb
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: mongodb
---
# Source: harness/charts/platform/charts/next-gen-ui/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: next-gen-ui
  namespace: default
  labels:
    helm.sh/chart: next-gen-ui-0.2.6
    app.kubernetes.io/name: next-gen-ui
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app.kubernetes.io/name: next-gen-ui
    app.kubernetes.io/instance: release-name
  sessionAffinity: None
---
# Source: harness/charts/platform/charts/ng-auth-ui/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ng-auth-ui
  namespace: default
  labels:
    helm.sh/chart: ng-auth-ui-0.2.2
    app.kubernetes.io/name: ng-auth-ui
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app.kubernetes.io/name: ng-auth-ui
    app.kubernetes.io/instance: release-name
---
# Source: harness/charts/platform/charts/ng-manager/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ng-manager
  namespace: default
  labels:
    helm.sh/chart: ng-manager-0.2.17
    app.kubernetes.io/name: ng-manager
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http-ng-manager
    port: 7090
    protocol: TCP
    targetPort: 7090
  - name: grpc-ng-manager
    port: 9979
    protocol: TCP
    targetPort: 9979
  - name: grpc-git-sync
    port: 13002
    protocol: TCP
    targetPort: 13002
  selector:
    app.kubernetes.io/name: ng-manager
    app.kubernetes.io/instance: release-name
  sessionAffinity: None
---
# Source: harness/charts/platform/charts/pipeline-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: pipeline-service-0.2.11
    app.kubernetes.io/name: pipeline-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: pipeline-service
  namespace: default
spec:
  ports:
  - name: grpc-pms
    port: 12011
    protocol: TCP
    targetPort: 12011
  - name: http-pms
    port: 12001
    protocol: TCP
    targetPort: 12001
  - name: grpc-gitsync
    port: 14002
    protocol: TCP
    targetPort: 14002
  selector:
    app.kubernetes.io/name: pipeline-service
    app.kubernetes.io/instance: release-name
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness/charts/platform/charts/platform-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: platform-service-0.2.8
    app.kubernetes.io/name: platform-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: platform-service
  namespace: default
spec:
  ports:
    - name: http
      port: 9005
      protocol: TCP
      targetPort: 9005
  selector:
    app.kubernetes.io/name: platform-service
    app.kubernetes.io/instance: release-name
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness
  namespace: default
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.2
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
---
# Source: harness/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness-announce-0
  namespace: default
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.2
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
    "statefulset.kubernetes.io/pod-name": redis-sentinel-harness-server-0
---
# Source: harness/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness-announce-1
  namespace: default
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.2
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
    "statefulset.kubernetes.io/pod-name": redis-sentinel-harness-server-1
---
# Source: harness/charts/platform/charts/redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel-harness-announce-2
  namespace: default
  labels:
    app: redis-sentinel
    helm.sh/chart: redis-0.2.2
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: redis-ha
    app: redis-sentinel
    "statefulset.kubernetes.io/pod-name": redis-sentinel-harness-server-2
---
# Source: harness/charts/platform/charts/scm-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: scm-service-0.2.1
    app.kubernetes.io/name: scm-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: scm-service
  namespace: default
spec:
  ports:
    - name: scm
      port: 8091
      protocol: TCP
      targetPort: 8091
  selector:
    app.kubernetes.io/name: scm-service
    app.kubernetes.io/instance: release-name
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness/charts/platform/charts/template-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: template-service-0.2.10
    app.kubernetes.io/name: template-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  name: template-service
  namespace: default
spec:
  ports:
  - name: grpc-template
    port: 15011
    protocol: TCP
    targetPort: 15011
  - name: http-template
    port: 15002
    protocol: TCP
    targetPort: 15002
  - name: grpc-gitsync
    port: 16002
    protocol: TCP
    targetPort: 16002
  selector:
    app.kubernetes.io/name: template-service
    app.kubernetes.io/instance: release-name
  sessionAffinity: None
  type: ClusterIP
---
# Source: harness/charts/platform/charts/ti-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ti-service
  namespace: default
  labels:
    helm.sh/chart: ti-service-0.2.8
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: ti-service
    port: 8078
    protocol: TCP
    targetPort: 8078
  selector:
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: release-name
---
# Source: harness/charts/platform/charts/timescaledb/templates/svc-timescaledb-config.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescaledb-single-chart-config
  namespace: default
  labels:
    component: patroni
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: patroni
spec:
  selector:
    app: timescaledb-single-chart
    cluster-name: timescaledb-single-chart
  type: ClusterIP
  clusterIP: None
  ports:
  - name: patroni
    port: 8008
    protocol: TCP
---
# Source: harness/charts/platform/charts/timescaledb/templates/svc-timescaledb-replica.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescaledb-single-chart-replica
  namespace: default
  labels:
    component: postgres
    role: replica
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: postgres
spec:
  selector:
    app: timescaledb-single-chart
    cluster-name: timescaledb-single-chart
    role: replica
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432, even if `!replicaLoadBalancer.enabled`.
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: harness/charts/platform/charts/timescaledb/templates/svc-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescaledb-single-chart
  namespace: default
  labels:
    role: master
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: timescaledb
spec:
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432, even if `!loadBalancer.enabled`.
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: harness/charts/ccm/charts/nextgen-ce/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nextgen-ce
  namespace: default
  labels:
    helm.sh/chart: nextgen-ce-0.1.3
    app.kubernetes.io/name: nextgen-ce
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.773.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: nextgen-ce
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: nextgen-ce
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      initContainers:
      #ng-manager should take care of mongo, cg-manager, redis & timescale
      - name: wait-for-ng-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=ng-manager"
      - name: wait-for-access-control
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=access-control"
      containers:
        - name: nextgen-ce
          securityContext:
            {}
          image: docker.io/harness/nextgen-ce-signed:77300-000
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /ccm/api/health
              port: 6340
            initialDelaySeconds: 180
            failureThreshold: 10
            periodSeconds: 15
          readinessProbe:
            httpGet:
              path: /ccm/api/health
              port: 6340
            initialDelaySeconds: 180
            failureThreshold: 10
            periodSeconds: 15
          envFrom:
            - configMapRef:
                name: nextgen-ce
          env:
            - name: EVENTS_MONGO_DB_URL
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/events?replicaSet=rs0&authSource=admin'
            - name: MONGO_USER
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGO_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALEDB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name : TIMESCALEDB_USERNAME
              value: postgres
            - name: TIMESCALEDB_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.default:5432/harness'
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nextgen-ce
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/access-control/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name:  access-control
  namespace: default
  labels:
    helm.sh/chart: access-control-0.2.7
    app.kubernetes.io/name: access-control
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 720
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: access-control
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: access-control
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
        - name: wait-for-mongo
          image: docker.io/harness/helm-init-container:latest
          imagePullPolicy: IfNotPresent
          args:
            - "pod"
            - "-lapp=mongodb-replicaset"
        - name: wait-for-redis
          image: docker.io/harness/helm-init-container:latest
          imagePullPolicy: IfNotPresent
          args:
            - "pod"
            - "-lapp=redis-sentinel"
      containers:
        - name: access-control
          image: docker.io/harness/accesscontrol-service-signed:77002
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          readinessProbe:
            httpGet:
              path: /api/health
              port: 9006
            initialDelaySeconds: 200
            timeoutSeconds: 20
            periodSeconds: 10
            failureThreshold: 40
          livenessProbe:
            httpGet:
              path: /api/health
              port: 9006
            initialDelaySeconds: 200
            timeoutSeconds: 20
            periodSeconds: 10
            failureThreshold: 40
          resources:
            limits:
              cpu: 0.5
              memory: 8192Mi
            requests:
              cpu: 0.5
              memory: 512Mi
          ports:
          - name: http
            containerPort: 9006
            protocol: "TCP"
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGODB_HOSTS
              value: 'mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc:27017,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc:27017,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017'
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/accesscontrol?replicaSet=rs0&authSource=admin'
            - name: OFFSET_STORAGE_FILE_FILENAME
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/accesscontrol?replicaSet=rs0&authSource=admin'
            - name: IDENTITY_SERVICE_SECRET
              value: 'HVSKUYqD4e5Rxu12hFDdCJKGM64sxgEynvdDhaOHaTHhwwn0K4Ttr0uoOxSsEVYNrUU='
          envFrom:
          - configMapRef:
              name: access-control
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - access-control
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/change-data-capture/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: change-data-capture
  namespace: default
  labels:
    helm.sh/chart: change-data-capture-0.2.10
    app.kubernetes.io/name: change-data-capture
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: change-data-capture
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: change-data-capture
        app.kubernetes.io/name: change-data-capture
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-ng-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=ng-manager"
      containers:
        - name: change-data-capture
          image: docker.io/harness/cdcdata-signed:77117
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALEDB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/harness?replicaSet=rs0&authSource=admin'
            - name: EVENTS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/events?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
            - name: CDC_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/change-data-capture?replicaSet=rs0&authSource=admin'
            - name: NG_HARNESS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/ng-harness?replicaSet=rs0&authSource=admin'
            - name: TIMESCALEDB_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.default:5432/harness'
          envFrom:
          - configMapRef:
              name: change-data-capture
          resources:
            limits:
              cpu: 1
              memory: 2880Mi
            requests:
              cpu: 1
              memory: 2880Mi
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8190
            periodSeconds: 10
            timeoutSeconds: 2
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8190
            periodSeconds: 10
            timeoutSeconds: 2
          startupProbe:
            failureThreshold: 25
            httpGet:
              path: /health
              port: 8190
            periodSeconds: 10
            timeoutSeconds: 2
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - change-data-capture
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/cv-nextgen/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cv-nextgen
  namespace: default
  labels:
    helm.sh/chart: cv-nextgen-0.2.13
    app.kubernetes.io/name: cv-nextgen
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: cv-nextgen
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cv-nextgen
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      initContainers:
      - name: wait-for-harness-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=harness-manager"
      containers:
        - name: cv-nextgen
          image: docker.io/harness/cv-nextgen-signed:77117
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/cvng-harness?replicaSet=rs0&authSource=admin'
            - name : NOTIFICATION_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/notifications?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
          envFrom:
            - configMapRef:
                name: cv-nextgen
          resources:
            limits:
              cpu: 1
              memory: 6144Mi
            requests:
              cpu: 1
              memory: 6144Mi
          readinessProbe:
            httpGet:
              path: /cv/api/health
              port: cv
            initialDelaySeconds: 60
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /cv/api/health
              port: cv
            initialDelaySeconds: 300
            periodSeconds: 10
            failureThreshold: 2
          ports:
            - name: cv
              containerPort: 6060
              protocol: "TCP"
            - name: grpc-cv-ng
              containerPort: 9979
              protocol: "TCP"
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cv-nextgen
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/delegate-proxy/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: delegate-proxy
  namespace: default
  labels:
    helm.sh/chart: delegate-proxy-0.2.5
    app.kubernetes.io/name: delegate-proxy
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: delegate-proxy
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: delegate-proxy
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        runAsUser: 65534
      containers:
        - name: delegate-proxy
          image: docker.io/harness/delegate-proxy-signed:77021
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 200m
              memory: 100Mi
            requests:
              cpu: 200m
              memory: 100Mi
          volumeMounts:
            - mountPath: /etc/nginx/conf.d
              name: harness-nginx-conf
      volumes:
        - name: harness-nginx-conf
          configMap:
            name: delegate-proxy
            items:
              - key: proxy.conf
                path: proxy.conf
---
# Source: harness/charts/platform/charts/gateway/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway
  namespace: default
  labels:
    helm.sh/chart: gateway-0.2.9
    app.kubernetes.io/name: gateway
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: gateway
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: gateway
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      containers:
        - name: gateway
          image: docker.io/harness/gateway-signed:200091
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          readinessProbe:
            httpGet:
              path: /actuator/health
              port: gateway-port
            initialDelaySeconds: 120
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /actuator/health
              port: gateway-port
            initialDelaySeconds: 180
            periodSeconds: 20
            failureThreshold: 2
          ports:
          - name: gateway-port
            containerPort: 8080
            protocol: "TCP"
          resources:
            limits:
              cpu: 0.4
              memory: 1024Mi
            requests:
              cpu: 0.2
              memory: 512Mi
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGO_DB_URL
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/gateway?replicaSet=rs0&authSource=admin'
          envFrom:
          - configMapRef:
              name: gateway
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - gateway
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/harness-manager/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: harness-manager
  namespace: default
  labels:
    helm.sh/chart: harness-manager-0.2.22
    app.kubernetes.io/name: harness-manager
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
  annotations: {}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: harness-manager
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      annotations:
        rollme: "74PVd"
      labels:
        app: harness-manager
        app.kubernetes.io/name: harness-manager
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-redis
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=redis-sentinel"
      containers:
        - envFrom:
            - configMapRef:
                name: harness-manager-config
          env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALEDB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/harness?replicaSet=rs0&authSource=admin'
            - name: TIMESCALEDB_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.default:5432/harness'
          image: docker.io/harness/manager-signed:77117
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          lifecycle:
            preStop:
              exec:
                command:
                  - touch
                  - shutdown
          livenessProbe:
            failureThreshold: 20
            initialDelaySeconds: 180
            httpGet:
              path: /api/version
              port: 9090
              scheme: HTTP
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: manager
          ports:
            - containerPort: 9879
              protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /api/health
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 90
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 2
              memory: 8192Mi
            requests:
              cpu: 2
              memory: 3000Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - harness-manager
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/le-nextgen/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: le-nextgen
  namespace: default
  labels:
    helm.sh/chart: le-nextgen-0.2.3
    app.kubernetes.io/name: le-nextgen
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: le-nextgen
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: le-nextgen
        app.kubernetes.io/instance: release-name
    spec:
      containers:
        - envFrom:
            - configMapRef:
                name: le-nextgen
          image: docker.io/harness/le-nextgen-signed:67101
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
          name: le-nextgen
          ports:
            - containerPort: 8108
              name: learning
              protocol: TCP
          resources:
            limits:
              cpu: 1
              memory: 6144Mi
            requests:
              cpu: 1
              memory: 6144Mi
      serviceAccountName: harness-default
      securityContext:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - le-nextgen
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/log-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: log-service
  namespace: default
  labels:
    helm.sh/chart: log-service-0.2.9
    app.kubernetes.io/name: log-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 300
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: log-service
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: log-service
        app.kubernetes.io/instance: release-name
    spec:
      terminationGracePeriodSeconds: 30
      serviceAccountName: harness-default
      securityContext:
        {}
      containers:
      - name: log-service
        envFrom:
        - configMapRef:
            name: log-service
        env:
          - name: LOG_SERVICE_S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: minio
                key: root-user
          - name: LOG_SERVICE_S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: minio
                key: root-password
        livenessProbe:
          httpGet:
            path: /healthz
            port: http-log-svc
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: http-log-svc
          initialDelaySeconds: 10
          periodSeconds: 10
        image: docker.io/harness/log-service-signed:release-18
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
        - name: http-log-svc
          containerPort: 8079
          protocol: "TCP"
        resources:
            limits:
              cpu: 1
              memory: 3072Mi
            requests:
              cpu: 1
              memory: 3072Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - log-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/minio/templates/standalone/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  namespace: "default"
  labels:
    app.kubernetes.io/name: minio
    helm.sh/chart: minio-11.9.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: minio
      app.kubernetes.io/instance: release-name
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: minio
        helm.sh/chart: minio-11.9.1
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/credentials-secret: 490055196912d5acbef5d770a11c44907f7e5de082390ec4dbeb5c49ef8b5528
    spec:

      serviceAccountName: minio
      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: minio
                    app.kubernetes.io/instance: release-name
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
      containers:
        - name: minio
          image: docker.io/bitnami/minio:2022.8.22-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MINIO_SCHEME
              value: "http"
            - name: MINIO_FORCE_NEW_KEYS
              value: "no"
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: minio
                  key: root-user
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: minio
                  key: root-password
            - name: MINIO_DEFAULT_BUCKETS
              value: logs
            - name: MINIO_BROWSER
              value: "on"
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
            - name: MINIO_CONSOLE_PORT_NUMBER
              value: "9001"
          envFrom:
          ports:
            - name: minio-api
              containerPort: 9000
              protocol: TCP
            - name: minio-console
              containerPort: 9001
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: minio-api
              scheme: "HTTP"
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            tcpSocket:
              port: minio-api
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: minio
---
# Source: harness/charts/platform/charts/next-gen-ui/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: next-gen-ui
  namespace: default
  labels:
    helm.sh/chart: next-gen-ui-0.2.6
    app.kubernetes.io/name: next-gen-ui
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: next-gen-ui
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: next-gen-ui
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-change-data-capture
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=change-data-capture"
      containers:
      - name: next-gen-ui
        image: docker.io/harness/nextgenui-signed:0.323.9
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: ng-ui-port
          periodSeconds: 10
        readinessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: ng-ui-port
          periodSeconds: 10
        startupProbe:
          failureThreshold: 30
          httpGet:
            path: /health
            port: ng-ui-port
          periodSeconds: 10
        ports:
        - name: ng-ui-port
          containerPort: 8080
          protocol: "TCP"
        resources:
            limits:
              cpu: 0.2
              memory: 200Mi
            requests:
              cpu: 0.2
              memory: 200Mi
        envFrom:
        - configMapRef:
            name: next-gen-ui
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - next-gen-ui
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/ng-auth-ui/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-auth-ui
  namespace: default
  labels:
    helm.sh/chart: ng-auth-ui-0.2.2
    app.kubernetes.io/name: ng-auth-ui
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ng-auth-ui
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ng-auth-ui
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      containers:
      - name: ng-auth-ui
        image: docker.io/harness/ng-auth-ui-signed:0.42.2
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 15
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 90
          periodSeconds: 20
          failureThreshold: 2
        ports:
        - name: ng-auth-ui-port
          containerPort: 8080
          protocol: "TCP"
        envFrom:
        - configMapRef:
            name: ng-auth-ui
        resources:
            limits:
              cpu: 0.5
              memory: 512Mi
            requests:
              cpu: 0.5
              memory: 512Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ng-auth-ui
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/ng-manager/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ng-manager
  namespace: default
  labels:
    helm.sh/chart: ng-manager-0.2.17
    app.kubernetes.io/name: ng-manager
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  progressDeadlineSeconds: 800
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ng-manager
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app: ng-manager
        app.kubernetes.io/name: ng-manager
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-harness-manager
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=harness-manager"
      containers:
      - name: ng-manager
        image: docker.io/harness/ng-manager-signed:77117
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
        - name: http-ng-manager
          containerPort: 7090
          protocol: "TCP"
        - name: grpc-ng-manager
          containerPort: 9979
          protocol: "TCP"
        - name: grpc-git-sync
          containerPort: 13002
          protocol: "TCP"
        resources:
            limits:
              cpu: 2
              memory: 8192Mi
            requests:
              cpu: 2
              memory: 200Mi
        env:
            - name: MONGODB_USERNAME
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: mongodbUsername
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name : TIMESCALE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: harness-secrets
                  key: timescaledbPostgresPassword
            - name: MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/ng-harness?replicaSet=rs0&authSource=admin'
            - name : NOTIFICATION_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/notifications?replicaSet=rs0&authSource=admin'
            - name: PMS_MONGO_URI
              value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
            - name: TIMESCALEDB_USERNAME
              value: postgres
            - name: TIMESCALE_URI
              value: 'jdbc:postgresql://timescaledb-single-chart.default:5432/harness'
        envFrom:
        - configMapRef:
            name: ng-manager
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: http-ng-manager
          periodSeconds: 10
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 5
          httpGet:
            path: /health
            port: http-ng-manager
          periodSeconds: 10
          timeoutSeconds: 2
        startupProbe:
          failureThreshold: 25
          httpGet:
            path: /health
            port: http-ng-manager
          periodSeconds: 10
          timeoutSeconds: 2
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ng-manager
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/pipeline-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pipeline-service
  namespace: default
  labels:
    helm.sh/chart: pipeline-service-0.2.11
    app.kubernetes.io/name: pipeline-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  progressDeadlineSeconds: 300
  selector:
    matchLabels:
      app.kubernetes.io/name: pipeline-service
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: pipeline-service
        app.kubernetes.io/name: pipeline-service
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      containers:
      - name: pipeline-service
        image: docker.io/harness/pipeline-service-signed:1.12.3
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
          - name: grpc-pms
            containerPort: 12011
            protocol: "TCP"
          - name: http-pms
            containerPort: 12001
            protocol: "TCP"
          - name: grpc-gitsync
            containerPort: 14002
            protocol: "TCP"
        resources:
            limits:
              cpu: 1
              memory: 6144Mi
            requests:
              cpu: 1
              memory: 6144Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name : TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: timescaledbPostgresPassword
          - name: MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/pms-harness?replicaSet=rs0&authSource=admin'
          - name : NOTIFICATION_MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/notifications?replicaSet=rs0&authSource=admin'
          - name: TIMESCALEDB_USERNAME
            value: postgres
          - name: TIMESCALE_URI
            value: 'jdbc:postgresql://timescaledb-single-chart.default:5432/harness'
        envFrom:
        - configMapRef:
            name: pipeline-service
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /api/health
            port: http-pms
          periodSeconds: 10
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 5
          httpGet:
            path: /api/health
            port: http-pms
          periodSeconds: 5
          timeoutSeconds: 2
        startupProbe:
          failureThreshold: 25
          httpGet:
            path: /api/health
            port: http-pms
          periodSeconds: 10
          timeoutSeconds: 2
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - pipeline-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/platform-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: platform-service
  namespace: default
  labels:
    helm.sh/chart: platform-service-0.2.8
    app.kubernetes.io/name: platform-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  progressDeadlineSeconds: 550
  selector:
    matchLabels:
      app.kubernetes.io/name: platform-service
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: platform-service
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      containers:
      - name: platform-service
        image: docker.io/harness/platform-service-signed:77201
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
        - name: http
          containerPort: 9005
          protocol: "TCP"
        resources:
            limits:
              cpu: 0.5
              memory: 8192Mi
            requests:
              cpu: 0.5
              memory: 512Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name: MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/notifications?replicaSet=rs0&authSource=admin'
          - name: AUDIT_MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/audits?replicaSet=rs0&authSource=admin'
          - name: RESOURCE_GROUP_MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/resource-groups?replicaSet=rs0&authSource=admin'
        envFrom:
        - configMapRef:
            name: platform-service

        readinessProbe:
          httpGet:
            path: /api/health
            port: 9005
          initialDelaySeconds: 100
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 20
        livenessProbe:
          httpGet:
            path: /api/health
            port: 9005
          initialDelaySeconds: 300
          timeoutSeconds: 10
          periodSeconds: 10
          failureThreshold: 20
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - platform-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/scm-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scm
  namespace: default
  labels:
    helm.sh/chart: scm-service-0.2.1
    app.kubernetes.io/name: scm-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: scm-service
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: scm-service
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      containers:
        - name: scm
          image: docker.io/harness/ci-scm-signed:release-87-ubi
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          ports:
          - name: scm
            containerPort: 8091
            protocol: TCP
          resources:
            limits:
              cpu: 0.1
              memory: 512Mi
            requests:
              cpu: 0.1
              memory: 512Mi
          readinessProbe:
            exec:
              command: ["/grpc_health_probe", "-addr=:8091"]
            initialDelaySeconds: 5
          livenessProbe:
            exec:
              command: ["/grpc_health_probe", "-addr=:8091"]
            initialDelaySeconds: 10
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - scm-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/template-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: template-service
  namespace: default
  labels:
    helm.sh/chart: template-service-0.2.10
    app.kubernetes.io/name: template-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  progressDeadlineSeconds: 300
  selector:
    matchLabels:
      app.kubernetes.io/name: template-service
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: template-service
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      terminationGracePeriodSeconds: 30
      securityContext:
        {}
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      - name: wait-for-pipeline-service
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=pipeline-service"
      containers:
      - name: template-service
        image:  docker.io/harness/template-service-signed:77117
        imagePullPolicy: IfNotPresent
        securityContext:
            runAsNonRoot: true
            runAsUser: 65534
        ports:
          - name: grpc-template
            containerPort: 15011
            protocol: "TCP"
          - name: http-template
            containerPort: 15002
            protocol: "TCP"
          - name: grpc-gitsync
            containerPort: 16002
            protocol: "TCP"
        resources:
            limits:
              cpu: 1
              memory: 1400Mi
            requests:
              cpu: 1
              memory: 1400Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name : TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: timescaledbPostgresPassword
          - name: MONGO_URI
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/template-harness?replicaSet=rs0&authSource=admin'
        envFrom:
        - configMapRef:
            name: template-service
        readinessProbe:
          httpGet:
            path: /api/health
            port: 15002
          initialDelaySeconds: 60
          timeoutSeconds: 5
          periodSeconds: 5
          failureThreshold: 8
        livenessProbe:
          httpGet:
            path: /api/health
            port: 15002
          initialDelaySeconds: 40
          timeoutSeconds: 5
          periodSeconds: 10
          failureThreshold: 20
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - template-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/platform/charts/ti-service/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ti-service
  namespace: default
  labels:
    helm.sh/chart: ti-service-0.2.8
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: ti-service
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ti-service
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: harness-default
      securityContext:
        {}
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: wait-for-mongo
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=mongodb-replicaset"
      - name: wait-for-timescale
        image: docker.io/harness/helm-init-container:latest
        imagePullPolicy: IfNotPresent
        args:
          - "pod"
          - "-lapp=timescaledb-single-chart"
      containers:
        - name: ti-service
          image: docker.io/harness/ti-service-signed:release-87
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 65534
          livenessProbe:
            httpGet:
              path: /healthz
              port: http-ti-service
            initialDelaySeconds: 10
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /healthz
              port: http-ti-service
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            limits:
              cpu: 1
              memory: 3072Mi
            requests:
              cpu: 1
              memory: 3072Mi
          ports:
          - name: http-ti-service
            containerPort: 8078
            protocol: "TCP"
          env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name: TI_SERVICE_MONGODB_CONN_STR
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/ti-harness?replicaSet=rs0&authSource=admin'
          - name: TI_SERVICE_TIMESCALE_USERNAME
            value: "postgres"
          - name: TI_SERVICE_TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: timescaledbPostgresPassword
          - name: TSDB_URL
            value: 'postgres://postgres:$(TI_SERVICE_TIMESCALE_PASSWORD)@timescaledb-single-chart.default:5432/harnessti'
          envFrom:
          - configMapRef:
              name: ti-service
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ti-service
            topologyKey: "kubernetes.io/hostname"
---
# Source: harness/charts/infra/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.16
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
    app: postgres
  annotations:
spec:
  replicas: 1
  serviceName: postgres-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: postgres
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-11.6.16
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
        app: postgres
      annotations:
    spec:
      serviceAccountName: default

      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: primary
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      initContainers:
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:14.4.0-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres
                  key: postgres-password
            - name: POSTGRES_DB
              value: "overops"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "dbname=overops" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e

                - |
                  exec pg_isready -U "postgres" -d "dbname=overops" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: harness/charts/platform/charts/mongodb/templates/arbiter/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb-replicaset-chart-arbiter
  namespace: "default"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: arbiter
spec:
  serviceName: mongodb-replicaset-chart-arbiter-headless
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: arbiter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mongodb
        helm.sh/chart: mongodb-13.1.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: arbiter
    spec:

      serviceAccountName: mongodb-replicaset-chart
      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: mongodb
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: arbiter
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
        sysctls: []

      initContainers:
      containers:
        - name: mongodb-arbiter
          image: docker.io/bitnami/mongodb:4.2.19
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: "mongodb-replicaset-chart-arbiter-headless"
            - name: MONGODB_REPLICA_SET_MODE
              value: "arbiter"
            - name: MONGODB_INITIAL_PRIMARY_HOST
              value: mongodb-replicaset-chart-0.mongodb-replicaset-chart.$(MY_POD_NAMESPACE).svc.cluster.local
            - name: MONGODB_REPLICA_SET_NAME
              value: "rs0"
            - name: MONGODB_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: MONGODB_PORT_NUMBER
              value: "27017"
            - name: MONGODB_INITIAL_PRIMARY_ROOT_USER
              value: "admin"
            - name: MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGODB_REPLICA_SET_KEY
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-replica-set-key
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
          ports:
            - containerPort: 27017
              name: mongodb
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10
            tcpSocket:
              port: mongodb
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10
            tcpSocket:
              port: mongodb
          resources:
            limits: {}
            requests: {}
---
# Source: harness/charts/platform/charts/mongodb/templates/replicaset/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb-replicaset-chart
  namespace: "default"
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-13.1.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  serviceName: mongodb-replicaset-chart
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: mongodb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mongodb
        helm.sh/chart: mongodb-13.1.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: mongodb
        app: mongodb-replicaset
    spec:

      serviceAccountName: mongodb-replicaset-chart
      affinity:
        podAffinity:

        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: mongodb
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: mongodb
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:

      securityContext:
        fsGroup: 1001
        sysctls: []

      containers:
        - name: mongodb
          image: docker.io/bitnami/mongodb:4.2.19
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: MY_POD_HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: K8S_SERVICE_NAME
              value: "mongodb-replicaset-chart"
            - name: MONGODB_INITIAL_PRIMARY_HOST
              value: mongodb-replicaset-chart-0.$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local
            - name: MONGODB_REPLICA_SET_NAME
              value: "rs0"
            - name: MONGODB_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: MONGODB_ROOT_USER
              value: "admin"
            - name: MONGODB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-root-password
            - name: MONGODB_REPLICA_SET_KEY
              valueFrom:
                secretKeyRef:
                  name: mongodb-replicaset-chart
                  key: mongodb-replica-set-key
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: MONGODB_SYSTEM_LOG_VERBOSITY
              value: "0"
            - name: MONGODB_DISABLE_SYSTEM_LOG
              value: "no"
            - name: MONGODB_DISABLE_JAVASCRIPT
              value: "no"
            - name: MONGODB_ENABLE_JOURNAL
              value: "yes"
            - name: MONGODB_PORT_NUMBER
              value: "27017"
            - name: MONGODB_ENABLE_IPV6
              value: "no"
            - name: MONGODB_ENABLE_DIRECTORY_PER_DB
              value: "no"
          ports:
            - name: mongodb
              containerPort: 27017
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10
            exec:
              command:
                - /bitnami/scripts/ping-mongodb.sh
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bitnami/scripts/readiness-probe.sh
          resources:
            limits:
              cpu: 4
              memory: 8192Mi
            requests:
              cpu: 4
              memory: 8192Mi
          volumeMounts:
            - name: datadir
              mountPath: /bitnami/mongodb
              subPath:
            - name: common-scripts
              mountPath: /bitnami/scripts
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh

      volumes:
        - name: common-scripts
          configMap:
            name: mongodb-replicaset-chart-common-scripts
            defaultMode: 0550
        - name: scripts
          configMap:
            name: mongodb-replicaset-chart-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: datadir
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "200Gi"
---
# Source: harness/charts/platform/charts/redis/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-sentinel-harness-server
  namespace: default
  labels:
    redis-sentinel-harness: replica
    app: redis-sentinel
    helm.sh/chart: redis-0.2.2
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      release: redis-ha
      app: redis-sentinel
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: release-name
  serviceName: redis-sentinel-harness
  replicas: 3
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/init-config: 0fb17318c62ec6e7f89897284e4d3edf7b1a0fc156692f8b15db8fd976df2e48
      labels:
        release: redis-ha
        app: redis-sentinel
        redis-sentinel-harness: replica
        app.kubernetes.io/name: redis
        app.kubernetes.io/instance: release-name
    spec:
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      serviceAccountName: harness-default
      initContainers:
      - name: config-init
        image: docker.io/harness/redis:6.2.7-alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        command:
        - sh
        args:
        - /readonly-config/init.sh
        env:
        - name: SENTINEL_ID_0
          value: ed89975e57ea5a6848fe664901b11b5e6b22b537

        - name: SENTINEL_ID_1
          value: 4abd57ef009b0a1595767af80ef815e3438ae7e9

        - name: SENTINEL_ID_2
          value: e21a3c2cf7abdfc6e8d921901addb1bf86fe32a0
        volumeMounts:
        - name: config
          mountPath: /readonly-config
          readOnly: true
        - name: data
          mountPath: /data
      containers:
      - name: redis
        image: docker.io/harness/redis:6.2.7-alpine
        imagePullPolicy: IfNotPresent
        command:
        - redis-server
        args:
        - /data/conf/redis.conf
        env:
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 15
        resources:
            limits:
              cpu: 0.1
              memory: 200Mi
            requests:
              cpu: 0.1
              memory: 200Mi
        ports:
        - name: redis
          containerPort: 6379
        volumeMounts:
        - mountPath: /data
          name: data
      - name: sentinel
        image: docker.io/harness/redis:6.2.7-alpine
        imagePullPolicy: IfNotPresent
        command:
          - redis-sentinel
        args:
          - /data/conf/sentinel.conf
        livenessProbe:
          tcpSocket:
            port: 26379
          initialDelaySeconds: 15
        resources:
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
        ports:
          - name: sentinel
            containerPort: 26379
        volumeMounts:
        - mountPath: /data
          name: data
      volumes:
      - name: config
        configMap:
          name: redis-sentinel-harness-configmap
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: redis-sentinel
                  release: redis-ha
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app:  redis-sentinel
                    release: redis-ha
                    redis-sentinel-harness: replica
                topologyKey: failure-domain.beta.kubernetes.io/zone
  volumeClaimTemplates:
  - metadata:
      name: data
      annotations:
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 10Gi
---
# Source: harness/charts/platform/charts/timescaledb/templates/statefulset-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-timescaledb
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: timescaledb
spec:
  serviceName: release-name-timescaledb
  replicas: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: timescaledb-single-chart
      release: release-name
  template:
    metadata:
      name: release-name-timescaledb
      labels:
        app: timescaledb-single-chart
        chart: timescaledb-0.2.13
        release: release-name
        heritage: Helm
        cluster-name: timescaledb-single-chart
        app.kubernetes.io/name: "release-name-timescaledb"
        app.kubernetes.io/version: 0.2.13
        app.kubernetes.io/component: timescaledb
    spec:
      serviceAccountName: release-name-timescaledb
      securityContext:
        # The postgres user inside the TimescaleDB image has uid=1000.
        # This configuration ensures the permissions of the mounts are suitable
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      initContainers:
      - name: tstune
        securityContext:
          allowPrivilegeEscalation: false
        image: docker.io/timescale/timescaledb-ha:pg13-ts2.6-oss-latest
        env:
        - name: TSTUNE_FILE
          value: /var/run/postgresql/timescaledb.conf
        - name: RESOURCES_WAL_VOLUME
          value: 1Gi
        - name: RESOURCES_DATA_VOLUME
          value: 100Gi
        - name: INCLUDE_DIRECTIVE
          value: include_if_exists = '/var/run/postgresql/timescaledb.conf'
        - name: CPUS
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.cpu
              divisor: "1"
        - name: MEMORY
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.memory
              divisor: 1Mi
        - name: RESOURCES_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.cpu
              divisor: "1"
        - name: RESOURCES_MEMORY_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.memory
              divisor: 1Mi
        command:
          - sh
          - "-c"
          - |
              set -e
              [ $CPUS -eq 0 ]   && CPUS="${RESOURCES_CPU_LIMIT}"
              [ $MEMORY -eq 0 ] && MEMORY="${RESOURCES_MEMORY_LIMIT}"

              if [ -f "${PGDATA}/postgresql.base.conf" ] && ! grep "${INCLUDE_DIRECTIVE}" postgresql.base.conf -qxF; then
                echo "${INCLUDE_DIRECTIVE}" >> "${PGDATA}/postgresql.base.conf"
              fi

              touch "${TSTUNE_FILE}"
              timescaledb-tune -quiet -conf-path "${TSTUNE_FILE}" -cpus "${CPUS}" -memory "${MEMORY}MB" \
                 -yes

              # If there is a dedicated WAL Volume, we want to set max_wal_size to 60% of that volume
              # If there isn't a dedicated WAL Volume, we set it to 20% of the data volume
              if [ "${RESOURCES_WAL_VOLUME}" = "0" ]; then
                WALMAX="${RESOURCES_DATA_VOLUME}"
                WALPERCENT=20
              else
                WALMAX="${RESOURCES_WAL_VOLUME}"
                WALPERCENT=60
              fi

              WALMAX=$(numfmt --from=auto ${WALMAX})

              # Wal segments are 16MB in size, in this way we get a "nice" number of the nearest
              # 16MB
              WALMAX=$(( $WALMAX / 100 * $WALPERCENT / 16777216 * 16 ))
              WALMIN=$(( $WALMAX / 2 ))

              echo "max_wal_size=${WALMAX}MB" >> "${TSTUNE_FILE}"
              echo "min_wal_size=${WALMIN}MB" >> "${TSTUNE_FILE}"
        volumeMounts:
        - name: socket-directory
          mountPath: /var/run/postgresql
        resources:
          limits:
            cpu: 1
            memory: 2048Mi
          requests:
            cpu: 1
            memory: 2048Mi
      # Issuing the final checkpoints on a busy database may take considerable time.
      # Unfinished checkpoints will require more time during startup, so the tradeoff
      # here is time spent in shutdown/time spent in startup.
      # We choose shutdown here, especially as during the largest part of the shutdown
      # we can still serve clients.
      terminationGracePeriodSeconds: 600
      containers:
      - name: timescaledb
        securityContext:
          allowPrivilegeEscalation: false
        image: docker.io/timescale/timescaledb-ha:pg13-ts2.6-oss-latest
        imagePullPolicy: Always
        lifecycle:
          preStop:
            exec:
              command:
              - psql
              - -X
              - --file
              - "/etc/timescaledb/scripts/lifecycle_preStop.psql"
        # When reusing an already existing volume it sometimes happens that the permissions
        # of the PGDATA and/or wal directory are incorrect. To guard against this, we always correctly
        # set the permissons of these directories before we hand over to Patroni.
        # We also create all the tablespaces that are defined, to ensure a smooth restore/recovery on a
        # pristine set of Volumes.
        # As PostgreSQL requires to have full control over the permissions of the tablespace directories,
        # we create a subdirectory "data" in every tablespace mountpoint. The full path of every tablespace
        # therefore always ends on "/data".
        # By creating a .pgpass file in the $HOME directory, we expose the superuser password
        # to processes that may not have it in their environment (like the preStop lifecycle hook).
        # To ensure Patroni will not mingle with this file, we give Patroni its own pgpass file.
        # As these files are in the $HOME directory, they are only available to *this* container,
        # and they are ephemeral.
        command:
          - /bin/bash
          - "-c"
          - |

            install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/data" "/var/lib/postgresql/wal/pg_wal" || exit 1
            TABLESPACES=""
            for tablespace in ; do
              install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/tablespaces/${tablespace}/data"
            done

            # Environment variables can be read by regular users of PostgreSQL. Especially in a Kubernetes
            # context it is likely that some secrets are part of those variables.
            # To ensure we expose as little as possible to the underlying PostgreSQL instance, we have a list
            # of allowed environment variable patterns to retain.
            #
            # We need the KUBERNETES_ environment variables for the native Kubernetes support of Patroni to work.
            #
            # NB: Patroni will remove all PATRONI_.* environment variables before starting PostgreSQL

            # We store the current environment, as initscripts, callbacks, archive_commands etc. may require
            # to have the environment available to them
            set -o posix
            export -p > "${HOME}/.pod_environment"
            export -p | grep PGBACKREST > "${HOME}/.pgbackrest_environment"

            for UNKNOWNVAR in $(env | awk -F '=' '!/^(PATRONI_.*|HOME|PGDATA|PGHOST|LC_.*|LANG|PATH|KUBERNETES_SERVICE_.*|AWS_ROLE_ARN|AWS_WEB_IDENTITY_TOKEN_FILE)=/ {print $1}')
            do
                unset "${UNKNOWNVAR}"
            done

            touch /var/run/postgresql/timescaledb.conf
            touch /var/run/postgresql/wal_status

            echo "*:*:*:postgres:${PATRONI_SUPERUSER_PASSWORD}" >> ${HOME}/.pgpass
            chmod 0600 ${HOME}/.pgpass

            export PATRONI_POSTGRESQL_PGPASS="${HOME}/.pgpass.patroni"

            exec patroni /etc/timescaledb/patroni.yaml
        env:
        # We use mixed case environment variables for Patroni User management,
        # as the variable themselves are documented to be PATRONI_<username>_OPTIONS.
        # Where possible, we want to have lowercase usernames in PostgreSQL as more complex postgres usernames
        # requiring quoting to be done in certain contexts, which many tools do not do correctly, or even at all.
        # https://patroni.readthedocs.io/en/latest/ENVIRONMENT.html#bootstrap-configuration
        - name: PATRONI_admin_OPTIONS
          value: createrole,createdb
        - name: PATRONI_REPLICATION_USERNAME
          value: standby
        # To specify the PostgreSQL and Rest API connect addresses we need
        # the PATRONI_KUBERNETES_POD_IP to be available as a bash variable, so we can compose an
        # IP:PORT address later on
        - name: PATRONI_KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PATRONI_POSTGRESQL_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):5432"
        - name: PATRONI_RESTAPI_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):8008"
        - name: PATRONI_KUBERNETES_PORTS
          value: '[{"name": "postgresql", "port": 5432}]'
        - name: PATRONI_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PATRONI_POSTGRESQL_DATA_DIR
          value: "/var/lib/postgresql/data"
        - name: PATRONI_KUBERNETES_NAMESPACE
          value: default
        - name: PATRONI_KUBERNETES_LABELS
          value: "{app: timescaledb-single-chart, cluster-name: timescaledb-single-chart, release: release-name}"
        - name: PATRONI_SCOPE
          value: timescaledb-single-chart
        - name: PGBACKREST_CONFIG
          value: /etc/pgbackrest/pgbackrest.conf
        # PGDATA and PGHOST are not required to let Patroni/PostgreSQL run correctly,
        # but for interactive sessions, callbacks and PostgreSQL tools they should be correct.
        - name: PGDATA
          value: "$(PATRONI_POSTGRESQL_DATA_DIR)"
        - name: PGHOST
          value: "/var/run/postgresql"
        - name: BOOTSTRAP_FROM_BACKUP
          value: "0"
          # pgBackRest is also called using the archive_command if the backup is enabled.
          # this script will also need access to the environment variables specified for
          # the backup. This can be removed once we do not directly invoke pgBackRest
          # from inside the TimescaleDB container anymore
        envFrom:
        - secretRef:
            name: "harness-secrets"
            optional: false
        - secretRef:
            name: "timescaledb-single-chart-pgbackrest"
            optional: true
        ports:
        - containerPort: 8008
          name: patroni
        - containerPort: 5432
          name: postgresql
        readinessProbe:
          exec:
            command:
              - pg_isready
              - -h
              - /var/run/postgresql
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        volumeMounts:
        - name: storage-volume
          mountPath: "/var/lib/postgresql"
          subPath: ""
        - name: wal-volume
          mountPath: "/var/lib/postgresql/wal"
          subPath: ""
        - mountPath: /etc/timescaledb/patroni.yaml
          subPath: patroni.yaml
          name: patroni-config
          readOnly: true
        - mountPath: /etc/timescaledb/scripts
          name: timescaledb-scripts
          readOnly: true
        - mountPath: /etc/timescaledb/post_init.d
          name: post-init
          readOnly: true
        - name: socket-directory
          mountPath: /var/run/postgresql

        - mountPath: /etc/pgbackrest
          name: pgbackrest
          readOnly: true
        - mountPath: /etc/pgbackrest/bootstrap
          name: pgbackrest-bootstrap
          readOnly: true
        resources:
          limits:
            cpu: 1
            memory: 2048Mi
          requests:
            cpu: 1
            memory: 2048Mi
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchLabels:
                  app: release-name-timescaledb
                  release: "release-name"
                  cluster-name: timescaledb-single-chart
          - weight: 50
            podAffinityTerm:
              topologyKey: failure-domain.beta.kubernetes.io/zone
              labelSelector:
                matchLabels:
                  app: release-name-timescaledb
                  release: "release-name"
                  cluster-name: timescaledb-single-chart

      volumes:
      - name: socket-directory
        emptyDir: {}
      - name: patroni-config
        configMap:
          name: release-name-timescaledb-patroni
      - name: timescaledb-scripts
        configMap:
          name: release-name-timescaledb-scripts
          defaultMode: 488 # 0750 permissions

      - name: post-init
        projected:
          defaultMode: 0750
          sources:
            - configMap:
                name: timescale-harness-post-init
                optional: false
            - secret:
                name: custom-secret-scripts
                optional: true
      - name: pgbouncer
        configMap:
          name: release-name-timescaledb-pgbouncer
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: pgbackrest
        configMap:
          name: release-name-timescaledb-pgbackrest
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: certificate
        secret:
          secretName: "timescaledb-single-chart-certificate"
          defaultMode: 416 # 0640 permissions
      - name: pgbackrest-bootstrap
        secret:
          secretName: pgbackrest-bootstrap
          optional: True
  volumeClaimTemplates:
    - metadata:
        name: storage-volume
        annotations:
        labels:
          app: timescaledb-single-chart
          release: release-name
          heritage: Helm
          cluster-name: timescaledb-single-chart
          purpose: data-directory
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: "100Gi"
    - metadata:
        name: wal-volume
        annotations:
        labels:
          app: timescaledb-single-chart
          release: release-name
          heritage: Helm
          cluster-name: timescaledb-single-chart
          purpose: wal-directory
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: "1Gi"
---
# Source: harness/charts/platform/charts/ti-service/templates/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ti-migration-y4yufa
  namespace: default
  annotations:
  labels:
    helm.sh/chart: ti-service-0.2.8
    app.kubernetes.io/name: ti-service
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.0.1"
    app.kubernetes.io/managed-by: Helm
spec:
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: ti-service
        app.kubernetes.io/instance: release-name
    spec:
      containers:
      - name: migrate
        image: docker.io/harness/ti-service-signed:release-87
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c", "sleep 300 && /opt/harness/migrate.sh" ]
        resources:
          limits:
            cpu: 1
            memory: 3072Mi
          requests:
            cpu: 1
            memory: 3072Mi
        env:
          - name: MONGODB_USERNAME
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: mongodbUsername
          - name: MONGODB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-replicaset-chart
                key: mongodb-root-password
          - name: TI_SERVICE_MONGODB_CONN_STR
            value: 'mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@mongodb-replicaset-chart-0.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-1.mongodb-replicaset-chart.default.svc,mongodb-replicaset-chart-2.mongodb-replicaset-chart.default.svc:27017/ti-harness?replicaSet=rs0&authSource=admin'
          - name: TI_SERVICE_TIMESCALE_USERNAME
            value: "postgres"
          - name: TI_SERVICE_TIMESCALE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: harness-secrets
                key: "timescaledbPostgresPassword"
          - name: TSDB_URL
            value: 'postgres://postgres:$(TI_SERVICE_TIMESCALE_PASSWORD)@timescaledb-single-chart.default:5432/harnessti'
      restartPolicy: Never
---
# Source: harness/charts/platform/charts/timescaledb/templates/job-update-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: batch/v1
kind: Job
metadata:
  name: "release-name-patroni-qa"
  namespace: default
  labels:
    app: timescaledb-single-chart
    chart: timescaledb-0.2.13
    release: release-name
    heritage: Helm
    cluster-name: timescaledb-single-chart
    app.kubernetes.io/name: "release-name-timescaledb"
    app.kubernetes.io/version: 0.2.13
    app.kubernetes.io/component: patroni
spec:
  activeDeadlineSeconds: 120
  template:
    metadata:
      labels:
        app: timescaledb-single-chart
        chart: timescaledb-0.2.13
        release: release-name
        heritage: Helm
    spec:
      restartPolicy: OnFailure
      containers:
      - name: release-name-timescaledb-patch-patroni-config
        image: curlimages/curl
#        image: docker.io/curlimages/curl:latest
        command: ["/bin/sh"]
        # Patching the Patroni configuration is good, however it should not block an upgrade from going through
        # Therefore we ensure we always exit with an exitcode 0, so that Helm is satisfied with this upgrade job
        args:
        - '-c'
        - |
          /usr/bin/curl --connect-timeout 30 --include --request PATCH --data \
          "{\"loop_wait\":10,\"maximum_lag_on_failover\":33554432,\"postgresql\":{\"parameters\":{\"archive_command\":\"/etc/timescaledb/scripts/pgbackrest_archive.sh %p\",\"archive_mode\":\"on\",\"archive_timeout\":\"1800s\",\"autovacuum_analyze_scale_factor\":0.02,\"autovacuum_max_workers\":10,\"autovacuum_naptime\":\"5s\",\"autovacuum_vacuum_cost_limit\":500,\"autovacuum_vacuum_scale_factor\":0.05,\"hot_standby\":\"on\",\"log_autovacuum_min_duration\":\"1min\",\"log_checkpoints\":\"on\",\"log_connections\":\"on\",\"log_disconnections\":\"on\",\"log_line_prefix\":\"%t [%p]: [%c-%l] %u@%d,app=%a [%e] \",\"log_lock_waits\":\"on\",\"log_min_duration_statement\":\"1s\",\"log_statement\":\"ddl\",\"max_connections\":100,\"max_prepared_transactions\":150,\"shared_preload_libraries\":\"timescaledb,pg_stat_statements\",\"tcp_keepalives_idle\":900,\"tcp_keepalives_interval\":100,\"temp_file_limit\":\"1GB\",\"timescaledb.passfile\":\"../.pgpass\",\"unix_socket_directories\":\"/var/run/postgresql\",\"unix_socket_permissions\":\"0750\",\"wal_level\":\"hot_standby\",\"wal_log_hints\":\"on\"},\"use_pg_rewind\":true,\"use_slots\":true},\"retry_timeout\":10,\"ttl\":30}" \
          "http://timescaledb-single-chart-config:8008/config"
          exit 0
---
# Source: harness/charts/platform/charts/timescaledb/templates/configmap-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: harness/charts/platform/charts/timescaledb/templates/configmap-pgbouncer.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: harness/charts/platform/charts/timescaledb/templates/pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: harness/charts/platform/charts/timescaledb/templates/secret-certificate.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: harness/charts/platform/charts/timescaledb/templates/secret-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: harness/charts/platform/charts/timescaledb/templates/svc-prometheus.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
#
# This service is only created if Prometheus is enabled.
